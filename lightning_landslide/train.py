#!/usr/bin/env python3
"""
PyTorch Lightningè®­ç»ƒè„šæœ¬

è¿™æ˜¯æ•´ä¸ªæ¡†æ¶çš„å…¥å£ç‚¹ã€‚å®ƒå°†æ‰€æœ‰ç»„ä»¶ï¼ˆæ¨¡å‹ã€æ•°æ®ã€è®­ç»ƒé€»è¾‘ï¼‰
æ•´åˆåœ¨ä¸€èµ·ï¼Œæä¾›ç»Ÿä¸€çš„è®­ç»ƒæ¥å£ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. é…ç½®ç®¡ç†ï¼šä½¿ç”¨Hydraè¿›è¡Œé…ç½®ç®¡ç†
2. å®éªŒè¿½è¸ªï¼šé›†æˆå¤šç§æ—¥å¿—ç³»ç»Ÿ
3. é”™è¯¯å¤„ç†ï¼šä¼˜é›…çš„é”™è¯¯å¤„ç†å’Œæ¢å¤
4. è‡ªåŠ¨åŒ–æµç¨‹ï¼šä»è®­ç»ƒåˆ°æµ‹è¯•çš„å®Œæ•´è‡ªåŠ¨åŒ–

ä½¿ç”¨ç¤ºä¾‹ï¼š
1. åŸºç¡€è®­ç»ƒï¼špython train.py
2. ä¿®æ”¹å‚æ•°ï¼špython train.py training.max_epochs=100 data.batch_size=64
3. ä½¿ç”¨ä¸åŒé…ç½®ï¼špython train.py --config-name=optical_baseline
4. å¤šå®éªŒè¿è¡Œï¼špython train.py --multirun training.lr=1e-4,5e-5,2e-4

è®¾è®¡å“²å­¦ï¼š
"è®©å¤æ‚çš„äº‹æƒ…å˜ç®€å•ï¼Œè®©ç®€å•çš„äº‹æƒ…å˜è‡ªåŠ¨åŒ–"
"""

import os
import sys
import warnings
from pathlib import Path
from typing import Optional, Dict, Any, List
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger
import hydra
from omegaconf import DictConfig, OmegaConf

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from src.models import LandslideClassificationModule
from src.data import OpticalDataModule
from src.utils.metrics import MetricsLogger
from src.utils.logging_utils import setup_logging

# æŠ‘åˆ¶ä¸€äº›ä¸é‡è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", ".*does not have many workers.*")
warnings.filterwarnings("ignore", ".*The dataloader.*")

logger = logging.getLogger(__name__)


def create_callbacks(cfg: DictConfig) -> List[pl.Callback]:
    """
    åˆ›å»ºè®­ç»ƒå›è°ƒå‡½æ•°

    å›è°ƒå‡½æ•°æ˜¯Lightningçš„æ ¸å¿ƒåŠŸèƒ½ä¹‹ä¸€ï¼Œå®ƒä»¬åœ¨è®­ç»ƒçš„ä¸åŒé˜¶æ®µ
    è‡ªåŠ¨æ‰§è¡Œç‰¹å®šçš„æ“ä½œï¼Œå¦‚ä¿å­˜æ£€æŸ¥ç‚¹ã€æ—©åœã€å­¦ä¹ ç‡ç›‘æ§ç­‰ã€‚

    Args:
        cfg: é…ç½®å¯¹è±¡

    Returns:
        å›è°ƒå‡½æ•°åˆ—è¡¨
    """
    callbacks = []

    # æ—©åœå›è°ƒ
    if cfg.callbacks.early_stopping.enable:
        early_stop = EarlyStopping(
            monitor=cfg.callbacks.early_stopping.monitor,
            mode=cfg.callbacks.early_stopping.mode,
            patience=cfg.callbacks.early_stopping.patience,
            min_delta=cfg.callbacks.early_stopping.min_delta,
            verbose=cfg.callbacks.early_stopping.verbose,
        )
        callbacks.append(early_stop)
        logger.info(f"Added EarlyStopping: monitor={cfg.callbacks.early_stopping.monitor}")

    # æ¨¡å‹æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = ModelCheckpoint(
        monitor=cfg.callbacks.model_checkpoint.monitor,
        mode=cfg.callbacks.model_checkpoint.mode,
        save_top_k=cfg.callbacks.model_checkpoint.save_top_k,
        save_last=cfg.callbacks.model_checkpoint.save_last,
        filename=cfg.callbacks.model_checkpoint.filename,
        auto_insert_metric_name=cfg.callbacks.model_checkpoint.auto_insert_metric_name,
        verbose=True,
    )
    callbacks.append(checkpoint_callback)
    logger.info(f"Added ModelCheckpoint: monitor={cfg.callbacks.model_checkpoint.monitor}")

    # å­¦ä¹ ç‡ç›‘æ§å›è°ƒ
    lr_monitor = LearningRateMonitor(logging_interval=cfg.callbacks.lr_monitor.logging_interval)
    callbacks.append(lr_monitor)

    # è‡ªå®šä¹‰æŒ‡æ ‡æ—¥å¿—å›è°ƒ
    metrics_logger = MetricsLogger()
    callbacks.append(metrics_logger)

    logger.info(f"Created {len(callbacks)} callbacks")
    return callbacks


def create_loggers(cfg: DictConfig) -> List[pl.LightningLoggerBase]:
    """
    åˆ›å»ºæ—¥å¿—è®°å½•å™¨

    æ”¯æŒå¤šç§æ—¥å¿—åç«¯ï¼ŒåŒ…æ‹¬TensorBoardå’ŒWeights & Biasesã€‚
    è¿™äº›æ—¥å¿—ç³»ç»Ÿå¸®åŠ©æˆ‘ä»¬è¿½è¸ªå®éªŒè¿›å±•ï¼Œå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚

    Args:
        cfg: é…ç½®å¯¹è±¡

    Returns:
        æ—¥å¿—è®°å½•å™¨åˆ—è¡¨
    """
    loggers = []

    # TensorBoardæ—¥å¿—è®°å½•å™¨
    if cfg.logging.tensorboard.enable:
        tb_logger = TensorBoardLogger(
            save_dir=cfg.logging.save_dir,
            name=cfg.logging.name,
            version=cfg.experiment.name,
            log_graph=cfg.logging.tensorboard.log_graph,
        )
        loggers.append(tb_logger)
        logger.info(f"Added TensorBoard logger: {tb_logger.log_dir}")

    # Weights & Biasesæ—¥å¿—è®°å½•å™¨
    if cfg.logging.wandb.enable:
        try:
            wandb_logger = WandbLogger(
                project=cfg.logging.wandb.project,
                name=cfg.experiment.name,
                tags=cfg.logging.wandb.tags,
                notes=cfg.logging.wandb.notes,
                save_dir=cfg.logging.save_dir,
            )
            loggers.append(wandb_logger)
            logger.info("Added Weights & Biases logger")
        except ImportError:
            logger.warning("wandb not installed, skipping WandB logger")

    if not loggers:
        logger.warning("No loggers configured, using default Lightning logger")

    return loggers


def setup_environment(cfg: DictConfig) -> None:
    """
    è®¾ç½®è®­ç»ƒç¯å¢ƒ

    é…ç½®å„ç§ç¯å¢ƒå˜é‡å’ŒPyTorchè®¾ç½®ï¼Œç¡®ä¿è®­ç»ƒçš„ç¨³å®šæ€§å’Œå¯é‡ç°æ€§ã€‚

    Args:
        cfg: é…ç½®å¯¹è±¡
    """
    # è®¾ç½®éšæœºç§å­
    if cfg.reproducibility.seed is not None:
        pl.seed_everything(cfg.reproducibility.seed, workers=True)
        logger.info(f"Set random seed to {cfg.reproducibility.seed}")

    # è®¾ç½®PyTorchæ€§èƒ½é€‰é¡¹
    if cfg.reproducibility.deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        logger.info("Enabled deterministic mode")
    elif cfg.reproducibility.benchmark:
        torch.backends.cudnn.benchmark = True
        logger.info("Enabled cuDNN benchmark mode")

    # è®¾ç½®çº¿ç¨‹æ•°ï¼ˆé¿å…è¿‡åº¦å¹¶è¡ŒåŒ–ï¼‰
    if "OMP_NUM_THREADS" not in os.environ:
        torch.set_num_threads(4)
        os.environ["OMP_NUM_THREADS"] = "4"

    # åˆ›å»ºè¾“å‡ºç›®å½•
    for dir_path in [
        cfg.outputs.checkpoint_dir,
        cfg.outputs.log_dir,
        cfg.outputs.results_dir,
        cfg.outputs.predictions_dir,
    ]:
        Path(dir_path).mkdir(parents=True, exist_ok=True)

    logger.info("Environment setup completed")


def validate_config(cfg: DictConfig) -> None:
    """
    éªŒè¯é…ç½®çš„æœ‰æ•ˆæ€§

    åœ¨å¼€å§‹è®­ç»ƒå‰æ£€æŸ¥é…ç½®çš„åˆç†æ€§ï¼Œé¿å…æµªè´¹æ—¶é—´åœ¨æ— æ•ˆçš„å®éªŒä¸Šã€‚

    Args:
        cfg: é…ç½®å¯¹è±¡

    Raises:
        ValueError: å¦‚æœé…ç½®æ— æ•ˆ
    """
    # éªŒè¯å¿…éœ€çš„é…ç½®é¡¹
    required_keys = [
        "model.type",
        "model.num_classes",
        "data.train_data_dir",
        "data.train_csv",
        "training.max_epochs",
        "training.optimizer.type",
    ]

    missing_keys = []
    for key in required_keys:
        try:
            OmegaConf.select(cfg, key)
        except:
            missing_keys.append(key)

    if missing_keys:
        raise ValueError(f"Missing required configuration keys: {missing_keys}")

    # éªŒè¯æ•°æ®è·¯å¾„
    data_paths = [cfg.data.train_data_dir, cfg.data.train_csv]
    if cfg.data.get("exclude_ids_file"):
        data_paths.append(cfg.data.exclude_ids_file)

    for path in data_paths:
        if not Path(path).exists():
            raise FileNotFoundError(f"Data path not found: {path}")

    # éªŒè¯è®­ç»ƒå‚æ•°
    if cfg.training.max_epochs <= 0:
        raise ValueError("max_epochs must be positive")

    if cfg.data.batch_size <= 0:
        raise ValueError("batch_size must be positive")

    if cfg.training.optimizer.lr <= 0:
        raise ValueError("learning rate must be positive")

    # éªŒè¯GPUé…ç½®
    if cfg.compute.accelerator == "gpu" and not torch.cuda.is_available():
        logger.warning("GPU requested but not available, falling back to CPU")
        cfg.compute.accelerator = "cpu"
        cfg.compute.devices = 1

    logger.info("âœ“ Configuration validation passed")


def print_experiment_info(cfg: DictConfig) -> None:
    """
    æ‰“å°å®éªŒä¿¡æ¯

    åœ¨è®­ç»ƒå¼€å§‹å‰æ˜¾ç¤ºå…³é”®çš„å®éªŒä¿¡æ¯ï¼Œä¾¿äºè¿½è¸ªå’Œè°ƒè¯•ã€‚

    Args:
        cfg: é…ç½®å¯¹è±¡
    """
    print("\n" + "=" * 80)
    print(f"ğŸš€ Starting Experiment: {cfg.experiment.name}")
    print("=" * 80)
    print(f"ğŸ“ Description: {cfg.experiment.description}")
    print(f"ğŸ·ï¸  Tags: {cfg.experiment.tags}")
    print(f"ğŸ“… Version: {cfg.experiment.version}")
    print()
    print(f"ğŸ§  Model: {cfg.model.type}")
    print(f"ğŸ“Š Data: {cfg.data.train_data_dir}")
    print(f"âš™ï¸  Batch Size: {cfg.data.batch_size}")
    print(f"ğŸ”„ Max Epochs: {cfg.training.max_epochs}")
    print(f"ğŸ“ˆ Learning Rate: {cfg.training.optimizer.lr}")
    print(f"ğŸ’¾ Precision: {cfg.compute.precision}")
    print(f"ğŸ¯ Monitor Metric: {cfg.callbacks.model_checkpoint.monitor}")
    print()
    print(f"ğŸ’¾ Checkpoints: {cfg.outputs.checkpoint_dir}")
    print(f"ğŸ“‹ Logs: {cfg.outputs.log_dir}")
    print("=" * 80 + "\n")


def save_config(cfg: DictConfig, save_dir: str) -> None:
    """
    ä¿å­˜é…ç½®æ–‡ä»¶

    å°†å®Œæ•´çš„é…ç½®ä¿å­˜åˆ°å®éªŒç›®å½•ï¼Œç¡®ä¿å®éªŒçš„å¯é‡ç°æ€§ã€‚

    Args:
        cfg: é…ç½®å¯¹è±¡
        save_dir: ä¿å­˜ç›®å½•
    """
    config_save_path = Path(save_dir) / "config.yaml"
    config_save_path.parent.mkdir(parents=True, exist_ok=True)

    with open(config_save_path, "w") as f:
        OmegaConf.save(cfg, f)

    logger.info(f"Configuration saved to {config_save_path}")


def train_model(cfg: DictConfig) -> Dict[str, Any]:
    """
    è®­ç»ƒæ¨¡å‹çš„ä¸»è¦å‡½æ•°

    è¿™æ˜¯æ•´ä¸ªè®­ç»ƒæµç¨‹çš„æ ¸å¿ƒå‡½æ•°ï¼Œç»„ç»‡å’Œæ‰§è¡Œå®Œæ•´çš„è®­ç»ƒè¿‡ç¨‹ã€‚

    Args:
        cfg: é…ç½®å¯¹è±¡

    Returns:
        è®­ç»ƒç»“æœå­—å…¸
    """
    logger.info("Starting model training...")

    # åˆ›å»ºæ•°æ®æ¨¡å—
    logger.info("Creating data module...")
    data_module = OpticalDataModule(cfg)

    # åˆ›å»ºæ¨¡å‹
    logger.info("Creating model...")
    model = LandslideClassificationModule(cfg)

    # åˆ›å»ºå›è°ƒå‡½æ•°å’Œæ—¥å¿—è®°å½•å™¨
    callbacks = create_callbacks(cfg)
    loggers = create_loggers(cfg)

    # åˆ›å»ºè®­ç»ƒå™¨
    logger.info("Creating trainer...")
    trainer = pl.Trainer(
        # åŸºç¡€é…ç½®
        max_epochs=cfg.training.max_epochs,
        accelerator=cfg.compute.accelerator,
        devices=cfg.compute.devices,
        precision=cfg.compute.precision,
        # å›è°ƒå’Œæ—¥å¿—
        callbacks=callbacks,
        logger=loggers,
        # éªŒè¯é…ç½®
        val_check_interval=cfg.evaluation.val_check_interval,
        # æ€§èƒ½ä¼˜åŒ–
        gradient_clip_val=cfg.compute.gradient_clip_val,
        accumulate_grad_batches=cfg.compute.accumulate_grad_batches,
        # å¯é‡ç°æ€§
        deterministic=cfg.reproducibility.deterministic,
        # æ—¥å¿—é…ç½®
        log_every_n_steps=cfg.logging.log_every_n_steps,
        # å…¶ä»–è®¾ç½®
        enable_checkpointing=True,
        enable_progress_bar=True,
        enable_model_summary=True,
    )

    # æ‰“å°æ¨¡å‹å’Œæ•°æ®ä¿¡æ¯
    logger.info("Model and data information:")
    data_module.setup("fit")
    model_info = model.model.get_model_info()
    data_info = data_module.get_data_info()

    for key, value in model_info.items():
        logger.info(f"  Model {key}: {value}")

    for key, value in data_info.items():
        logger.info(f"  Data {key}: {value}")

    # å¼€å§‹è®­ç»ƒ
    logger.info("ğŸš€ Starting training...")
    try:
        trainer.fit(model, data_module)
        logger.info("âœ… Training completed successfully")

        # è‡ªåŠ¨æµ‹è¯•ï¼ˆå¦‚æœé…ç½®å¯ç”¨ï¼‰
        if cfg.evaluation.test_after_training and data_module.test_dataloader() is not None:
            logger.info("ğŸ§ª Starting automatic testing...")
            test_results = trainer.test(model, data_module, ckpt_path="best")
            logger.info("âœ… Testing completed")
        else:
            test_results = None
            logger.info("â­ï¸  Skipping automatic testing")

        # æ”¶é›†è®­ç»ƒç»“æœ
        training_results = {
            "status": "success",
            "best_model_path": trainer.checkpoint_callback.best_model_path,
            "best_model_score": trainer.checkpoint_callback.best_model_score.item(),
            "logged_metrics": trainer.logged_metrics,
            "test_results": test_results,
        }

        # æ£€æŸ¥æ€§èƒ½é˜ˆå€¼
        best_score = trainer.checkpoint_callback.best_model_score.item()
        min_threshold = cfg.evaluation.performance_thresholds.min_val_f1
        target_threshold = cfg.evaluation.performance_thresholds.target_val_f1

        if best_score >= target_threshold:
            logger.info(f"ğŸ‰ Excellent! Achieved target performance: {best_score:.4f} >= {target_threshold}")
        elif best_score >= min_threshold:
            logger.info(f"âœ… Good! Achieved minimum performance: {best_score:.4f} >= {min_threshold}")
        else:
            logger.warning(f"âš ï¸  Performance below minimum threshold: {best_score:.4f} < {min_threshold}")

        return training_results

    except Exception as e:
        logger.error(f"âŒ Training failed with error: {str(e)}")
        raise


def main(cfg: DictConfig) -> None:
    """
    ä¸»å‡½æ•°

    æ•´ä¸ªè®­ç»ƒæµç¨‹çš„å…¥å£ç‚¹ã€‚å¤„ç†é…ç½®ã€ç¯å¢ƒè®¾ç½®ã€è®­ç»ƒæ‰§è¡Œå’Œç»“æœä¿å­˜ã€‚

    Args:
        cfg: Hydraé…ç½®å¯¹è±¡
    """
    try:
        # è®¾ç½®æ—¥å¿—
        setup_logging(level=logging.INFO)

        # éªŒè¯é…ç½®
        validate_config(cfg)

        # è®¾ç½®ç¯å¢ƒ
        setup_environment(cfg)

        # æ‰“å°å®éªŒä¿¡æ¯
        print_experiment_info(cfg)

        # ä¿å­˜é…ç½®
        save_config(cfg, cfg.outputs.log_dir)

        # è®­ç»ƒæ¨¡å‹
        results = train_model(cfg)

        # ä¿å­˜ç»“æœ
        results_path = Path(cfg.outputs.results_dir) / "training_results.yaml"
        with open(results_path, "w") as f:
            OmegaConf.save(OmegaConf.create(results), f)

        logger.info(f"Results saved to {results_path}")

        # æˆåŠŸå®Œæˆ
        print("\n" + "=" * 80)
        print("ğŸ‰ TRAINING COMPLETED SUCCESSFULLY!")
        print("=" * 80)
        print(f"âœ… Best model: {results['best_model_path']}")
        print(f"ğŸ“Š Best score: {results['best_model_score']:.4f}")
        print(f"ğŸ’¾ Results: {results_path}")
        print("=" * 80 + "\n")

    except Exception as e:
        logger.error(f"Experiment failed: {str(e)}")
        print("\n" + "=" * 80)
        print("âŒ TRAINING FAILED!")
        print("=" * 80)
        print(f"Error: {str(e)}")
        print("Please check the logs for more details.")
        print("=" * 80 + "\n")
        raise


@hydra.main(version_base=None, config_path="configs/experiment", config_name="optical_baseline")
def hydra_main(cfg: DictConfig) -> None:
    """
    Hydraè£…é¥°çš„ä¸»å‡½æ•°

    Hydraæä¾›äº†å¼ºå¤§çš„é…ç½®ç®¡ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - é…ç½®æ–‡ä»¶ç»„åˆ
    - å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    - å¤šå®éªŒè¿è¡Œ
    - è‡ªåŠ¨åŒ–å®éªŒç›®å½•ç®¡ç†

    Args:
        cfg: Hydraå¤„ç†åçš„é…ç½®å¯¹è±¡
    """
    main(cfg)


if __name__ == "__main__":
    # å¯åŠ¨è®­ç»ƒ
    hydra_main()
