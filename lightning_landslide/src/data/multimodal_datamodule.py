# =============================================================================
# lightning_landslide/src/data/multimodal_datamodule.py - å¤šæ¨¡æ€æ•°æ®æ¨¡å—
# =============================================================================

"""
å¤šæ¨¡æ€æ•°æ®æ¨¡å— - Lightning DataModuleå®ç°

è¿™ä¸ªæ¨¡å—æ˜¯æ‚¨é¡¹ç›®æ•°æ®å¤„ç†çš„æ ¸å¿ƒï¼Œå®ƒç»§æ‰¿äº†PyTorch Lightningçš„
DataModuleç±»ï¼Œæä¾›äº†æ ‡å‡†åŒ–çš„æ•°æ®åŠ è½½å’Œå¤„ç†æµç¨‹ã€‚

è®¾è®¡ç†å¿µï¼š
1. é…ç½®é©±åŠ¨ï¼šæ‰€æœ‰å‚æ•°éƒ½é€šè¿‡é…ç½®æ–‡ä»¶æ§åˆ¶
2. çµæ´»æ€§ï¼šæ”¯æŒä¸åŒçš„æ•°æ®ä½¿ç”¨æ¨¡å¼ï¼ˆä»…å…‰å­¦ã€å¤šæ¨¡æ€ç­‰ï¼‰
3. å¯é‡ç°æ€§ï¼šå›ºå®šéšæœºç§å­ï¼Œç¡®ä¿æ•°æ®åˆ†å‰²ä¸€è‡´
4. é«˜æ•ˆæ€§ï¼šæ”¯æŒå¤šè¿›ç¨‹æ•°æ®åŠ è½½å’Œå†…å­˜ä¼˜åŒ–
"""

import pytorch_lightning as pl
from torch.utils.data import DataLoader, random_split
from typing import Optional, Dict, Any, List, Callable
import logging
from pathlib import Path
import torch
from sklearn.model_selection import train_test_split
import numpy as np

from .multimodal_dataset import MultiModalDataset, create_train_dataset, create_test_dataset
from .base import BaseDataModule

logger = logging.getLogger(__name__)


class MultiModalDataModule(pl.LightningDataModule):
    """
    å¤šæ¨¡æ€é¥æ„Ÿæ•°æ®çš„Lightningæ•°æ®æ¨¡å—

    è¿™ä¸ªç±»æ˜¯æ‚¨é¡¹ç›®çš„æ•°æ®å¤„ç†ä¸­å¿ƒã€‚å®ƒè´Ÿè´£ï¼š
    1. ç®¡ç†å¤šé€šé“é¥æ„Ÿæ•°æ®çš„åŠ è½½
    2. å¤„ç†è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®é›†çš„åˆ’åˆ†
    3. é…ç½®æ•°æ®å¢å¼ºå’Œé¢„å¤„ç†ç­–ç•¥
    4. æä¾›æ ‡å‡†åŒ–çš„æ•°æ®åŠ è½½å™¨æ¥å£

    ä¸latent-diffusionçš„æ•°æ®å¤„ç†æ–¹å¼å¯¹æ¯”ï¼š
    - åŒæ ·æ”¯æŒé…ç½®é©±åŠ¨çš„å‚æ•°è®¾ç½®
    - æä¾›çµæ´»çš„æ•°æ®å¢å¼ºç­–ç•¥
    - æ”¯æŒå¤šç§æ•°æ®ä½¿ç”¨æ¨¡å¼
    """

    def __init__(
        self,
        # æ•°æ®è·¯å¾„é…ç½®
        train_data_dir: str,
        test_data_dir: str,
        train_csv: str,
        test_csv: str,
        exclude_ids_file: Optional[str] = None,
        # ğŸ”§ æ–°å¢ï¼šè·¨ç›®å½•æ˜ å°„é…ç½®
        cross_directory_mapping: Optional[str] = None,
        # é€šé“é…ç½®
        channel_config: Dict[str, Any] = None,
        active_mode: str = "optical_only",
        # æ•°æ®åŠ è½½é…ç½®
        batch_size: int = 64,
        num_workers: int = 8,
        pin_memory: bool = True,
        shuffle_train: bool = True,
        # æ•°æ®åˆ†å‰²é…ç½®
        val_split: float = 0.2,
        stratify: bool = True,
        use_weighted_sampling: bool = False,
        # æ•°æ®é¢„å¤„ç†é…ç½®
        preprocessing: Optional[Dict] = None,
        augmentation: Optional[Dict] = None,
        # å…¶ä»–é…ç½®
        seed: int = 3407,
        **kwargs,
    ):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€æ•°æ®æ¨¡å—

        Args:
            train_data_dir: è®­ç»ƒæ•°æ®ç›®å½•
            test_data_dir: æµ‹è¯•æ•°æ®ç›®å½•
            train_csv: è®­ç»ƒæ•°æ®æ ‡ç­¾æ–‡ä»¶
            test_csv: æµ‹è¯•æ•°æ®æ ‡ç­¾æ–‡ä»¶
            exclude_ids_file: éœ€è¦æ’é™¤çš„æ ·æœ¬IDæ–‡ä»¶
            cross_directory_mapping: è·¨ç›®å½•æ•°æ®è·¯å¾„æ˜ å°„æ–‡ä»¶ (JSONæ ¼å¼)
            channel_config: é€šé“é…ç½®å­—å…¸
            active_mode: å½“å‰ä½¿ç”¨çš„æ•°æ®æ¨¡å¼
            batch_size: æ‰¹æ¬¡å¤§å°
            num_workers: æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°
            pin_memory: æ˜¯å¦å°†æ•°æ®å›ºå®šåœ¨å†…å­˜ä¸­
            shuffle_train: æ˜¯å¦æ‰“ä¹±è®­ç»ƒæ•°æ®
            val_split: éªŒè¯é›†åˆ’åˆ†æ¯”ä¾‹
            stratify: æ˜¯å¦è¿›è¡Œåˆ†å±‚åˆ’åˆ†
            use_weighted_sampling: æ˜¯å¦ä½¿ç”¨åŠ æƒé‡‡æ ·
            preprocessing: é¢„å¤„ç†é…ç½®
            augmentation: æ•°æ®å¢å¼ºé…ç½®
            seed: éšæœºç§å­
        """
        super().__init__()

        # ä¿å­˜æ‰€æœ‰å‚æ•°ï¼ˆç”¨äºLightningçš„è¶…å‚æ•°è®°å½•ï¼‰
        self.save_hyperparameters()

        # æ•°æ®è·¯å¾„
        self.train_data_dir = Path(train_data_dir)
        self.test_data_dir = Path(test_data_dir)
        self.train_csv = Path(train_csv)
        self.test_csv = Path(test_csv)
        self.exclude_ids_file = exclude_ids_file

        # ğŸ”§ æ–°å¢ï¼šè·¨ç›®å½•æ˜ å°„æ”¯æŒ
        self.cross_directory_mapping = cross_directory_mapping

        # é€šé“å’Œæ¨¡å¼é…ç½®
        self.channel_config = channel_config or self._get_default_channel_config()
        self.active_mode = active_mode

        # æ•°æ®åŠ è½½é…ç½®
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.pin_memory = pin_memory
        self.shuffle_train = shuffle_train

        # æ•°æ®åˆ†å‰²é…ç½®
        self.val_split = val_split
        self.stratify = stratify
        self.use_weighted_sampling = use_weighted_sampling
        self.seed = seed

        # é¢„å¤„ç†é…ç½®
        self.preprocessing = preprocessing or {}
        self.augmentation = augmentation or {}

        # éšæœºç§å­
        self.seed = seed

        # æ•°æ®é›†å¯¹è±¡ï¼ˆåœ¨setupä¸­åˆå§‹åŒ–ï¼‰
        self.train_dataset = None
        self.val_dataset = None
        self.test_dataset = None

        # æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        self._data_stats = {}

        # ğŸ”§ åŠ è½½è·¨ç›®å½•æ˜ å°„æ–‡ä»¶
        self._cross_directory_mapping_dict = self._load_cross_directory_mapping()

        logger.info("ğŸ”¢MultiModalDataModule initialized" + "-" * 100)
        logger.info(f"Active mode: {self.active_mode}")
        logger.info(f"Batch size: {self.batch_size}, Workers: {self.num_workers}")
        logger.info(f"Validation split: {self.val_split}")
        if self.cross_directory_mapping:
            logger.info(f"ğŸ© Cross-directory mapping: {len(self._cross_directory_mapping_dict)} samples")
        logger.info("-" * 100)

    def _load_cross_directory_mapping(self) -> Dict[str, str]:
        """åŠ è½½è·¨ç›®å½•æ•°æ®è·¯å¾„æ˜ å°„"""
        if not self.cross_directory_mapping:
            return {}

        mapping_file = Path(self.cross_directory_mapping)
        if not mapping_file.exists():
            logger.warning(f"Cross-directory mapping file not found: {mapping_file}")
            return {}

        try:
            import json

            with open(mapping_file, "r") as f:
                mapping = json.load(f)

            logger.info(f"ğŸ“ Loaded cross-directory mapping: {len(mapping)} samples")
            return mapping
        except Exception as e:
            logger.error(f"Failed to load cross-directory mapping: {e}")
            return {}

    def _create_transforms(self, stage: str) -> Optional[Callable]:
        """
        æ ¹æ®é˜¶æ®µåˆ›å»ºæ•°æ®å˜æ¢

        è¿™ä¸ªæ–¹æ³•å®ç°äº†ç±»ä¼¼latent-diffusionçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œ
        æ ¹æ®è®­ç»ƒ/æµ‹è¯•é˜¶æ®µåº”ç”¨ä¸åŒçš„å˜æ¢ã€‚

        Args:
            stage: æ•°æ®é˜¶æ®µ ('train', 'val', 'test')

        Returns:
            æ•°æ®å˜æ¢å‡½æ•°
        """
        if stage not in self.augmentation:
            return None

        # è¿™é‡Œå¯ä»¥æ ¹æ®é…ç½®åˆ›å»ºå…·ä½“çš„æ•°æ®å˜æ¢
        # ä¾‹å¦‚ï¼šéšæœºç¿»è½¬ã€æ—‹è½¬ã€å™ªå£°æ·»åŠ ç­‰
        # å…·ä½“å®ç°å¯ä»¥å‚è€ƒalbumentationsæˆ–torchvision

        transforms = []
        stage_config = self.augmentation.get(stage, {})

        # ç¤ºä¾‹ï¼šå‡ ä½•å˜æ¢
        if stage_config.get("geometric", {}).get("random_flip", False):
            # transforms.append(RandomHorizontalFlip())
            pass

        # ç¤ºä¾‹ï¼šå…‰è°±å¢å¼º
        if stage_config.get("spectral", {}):
            # transforms.append(SpectralNoise())
            pass

        return None  # æš‚æ—¶è¿”å›Noneï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦å®ç°å…·ä½“å˜æ¢

    def prepare_data(self) -> None:
        """
        æ•°æ®å‡†å¤‡é˜¶æ®µï¼ˆå…¨å±€æ‰§è¡Œä¸€æ¬¡ï¼‰ï¼Œæ£€æŸ¥æ•°æ®æ˜¯å¦éƒ½å­˜åœ¨

        è¿™ä¸ªæ–¹æ³•éµå¾ªLightningçš„è®¾è®¡æ¨¡å¼ï¼Œåªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œä¸€æ¬¡ã€‚
        ä¸»è¦ç”¨äºï¼š
        1. éªŒè¯æ•°æ®æ–‡ä»¶å­˜åœ¨æ€§
        2. æ‰§è¡Œä¸€æ¬¡æ€§çš„æ•°æ®é¢„å¤„ç†
        3. åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„
        """
        logger.info("ğŸ” Preparing data...")

        # éªŒè¯æ•°æ®ç›®å½•å­˜åœ¨
        if not self.train_data_dir.exists():
            raise FileNotFoundError(f"Training data directory not found: {self.train_data_dir}")
        if not self.test_data_dir.exists():
            raise FileNotFoundError(f"Test data directory not found: {self.test_data_dir}")

        # éªŒè¯CSVæ–‡ä»¶å­˜åœ¨
        if not self.train_csv.exists():
            raise FileNotFoundError(f"Training CSV not found: {self.train_csv}")
        if not self.test_csv.exists():
            raise FileNotFoundError(f"Test CSV not found: {self.test_csv}")

        logger.info("Data preparation completed")

    def setup(self, stage: Optional[str] = None) -> None:
        """
        è®¾ç½®æ•°æ®é›†ï¼ˆæ¯ä¸ªè¿›ç¨‹æ‰§è¡Œï¼‰

        è¿™æ˜¯æ•°æ®å¤„ç†çš„æ ¸å¿ƒæ–¹æ³•ã€‚å®ƒæ ¹æ®ä¸åŒçš„stageåˆ›å»ºç›¸åº”çš„æ•°æ®é›†ã€‚
        """
        logger.info(f"setup:Setting up datasets for stage: {stage}")

        if stage == "fit" or stage is None:
            # åˆ›å»ºå®Œæ•´çš„è®­ç»ƒæ•°æ®é›†
            full_dataset = create_train_dataset(
                data_dir=str(self.train_data_dir),
                csv_file=str(self.train_csv),
                exclude_ids_file=self.exclude_ids_file,
                transform=self._create_transforms("train"),
                channel_config=self.channel_config,
                usage_mode=self.active_mode,
                # ğŸ”§ ä¼ é€’è·¨ç›®å½•æ˜ å°„
                cross_directory_mapping=self._cross_directory_mapping_dict,
            )

            # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
            if self.val_split > 0:
                total_size = len(full_dataset)
                val_size = int(total_size * self.val_split)
                train_size = total_size - val_size

                if self.stratify and full_dataset.has_labels:
                    labels = full_dataset.data_index["label"].tolist()

                    # åˆ†å±‚é‡‡æ ·ï¼šä¿æŒç±»åˆ«æ¯”ä¾‹
                    train_indices, val_indices = train_test_split(
                        range(len(full_dataset)), test_size=self.val_split, stratify=labels, random_state=self.seed
                    )

                    self.train_dataset = torch.utils.data.Subset(full_dataset, train_indices)
                    self.val_dataset = torch.utils.data.Subset(full_dataset, val_indices)

                    logger.info(f"Dataset split: Train={len(self.train_dataset)}, Val={len(self.val_dataset)}")
                else:
                    self.train_dataset = full_dataset
                    self.val_dataset = None
                    logger.info(f"Using full dataset for training: {len(self.train_dataset)} samples")

        if stage == "test" or stage is None:
            # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
            self.test_dataset = create_test_dataset(
                data_dir=str(self.test_data_dir),
                csv_file=str(self.test_csv),
                transform=self._create_transforms("test"),
                channel_config=self.channel_config,
                usage_mode=self.active_mode,
            )
            logger.info(f"Test dataset: {len(self.test_dataset)} samples")

        logger.info("-" * 100)

    def train_dataloader(self) -> DataLoader:
        """åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨"""
        if self.train_dataset is None:
            raise RuntimeError("Training dataset not initialized. Call setup('fit') first.")

        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=self.shuffle_train,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=False,  # è®­ç»ƒæ—¶ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡
            persistent_workers=self.num_workers > 0,  # ä¿æŒå·¥ä½œè¿›ç¨‹æ´»è·ƒ
        )

    def val_dataloader(self) -> Optional[DataLoader]:
        """åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨"""
        if self.val_dataset is None:
            return None

        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            drop_last=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            persistent_workers=self.num_workers > 0,
        )

    def test_dataloader(self) -> Optional[DataLoader]:
        """åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨"""
        if self.test_dataset is None:
            return None

        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=False,
            persistent_workers=self.num_workers > 0,
        )

    def predict_dataloader(self) -> Optional[DataLoader]:
        """åˆ›å»ºé¢„æµ‹æ•°æ®åŠ è½½å™¨"""
        return self.test_dataloader()

    def get_data_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®æ¨¡å—ä¿¡æ¯"""
        return {
            "data_stats": self._data_stats,
            "channel_config": self.channel_config,
            "active_mode": self.active_mode,
            "batch_size": self.batch_size,
            "num_workers": self.num_workers,
        }
