# =============================================================================
# lightning_landslide/src/data/multimodal_datamodule.py - å¤šæ¨¡æ€æ•°æ®æ¨¡å—
# =============================================================================

"""
å¤šæ¨¡æ€æ•°æ®æ¨¡å— - Lightning DataModuleå®ç°

è¿™ä¸ªæ¨¡å—æ˜¯æ‚¨é¡¹ç›®æ•°æ®å¤„ç†çš„æ ¸å¿ƒï¼Œå®ƒç»§æ‰¿äº†PyTorch Lightningçš„
DataModuleç±»ï¼Œæä¾›äº†æ ‡å‡†åŒ–çš„æ•°æ®åŠ è½½å’Œå¤„ç†æµç¨‹ã€‚

è®¾è®¡ç†å¿µï¼š
1. é…ç½®é©±åŠ¨ï¼šæ‰€æœ‰å‚æ•°éƒ½é€šè¿‡é…ç½®æ–‡ä»¶æ§åˆ¶
2. çµæ´»æ€§ï¼šæ”¯æŒä¸åŒçš„æ•°æ®ä½¿ç”¨æ¨¡å¼ï¼ˆä»…å…‰å­¦ã€å¤šæ¨¡æ€ç­‰ï¼‰
3. å¯é‡ç°æ€§ï¼šå›ºå®šéšæœºç§å­ï¼Œç¡®ä¿æ•°æ®åˆ†å‰²ä¸€è‡´
4. é«˜æ•ˆæ€§ï¼šæ”¯æŒå¤šè¿›ç¨‹æ•°æ®åŠ è½½å’Œå†…å­˜ä¼˜åŒ–
"""

import pytorch_lightning as pl
from torch.utils.data import DataLoader, random_split
from typing import Optional, Dict, Any, List, Callable
import logging
from pathlib import Path
import torch
from sklearn.model_selection import train_test_split
import numpy as np
import json
import pandas as pd

from .multimodal_dataset import MultiModalDataset, create_train_dataset, create_test_dataset

logger = logging.getLogger(__name__)


class MultiModalDataModule(pl.LightningDataModule):
    """
    å¤šæ¨¡æ€é¥æ„Ÿæ•°æ®çš„Lightningæ•°æ®æ¨¡å—

    è¿™ä¸ªç±»æ˜¯æ‚¨é¡¹ç›®çš„æ•°æ®å¤„ç†ä¸­å¿ƒã€‚å®ƒè´Ÿè´£ï¼š
    1. ç®¡ç†å¤šé€šé“é¥æ„Ÿæ•°æ®çš„åŠ è½½
    2. å¤„ç†è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®é›†çš„åˆ’åˆ†
    3. é…ç½®æ•°æ®å¢å¼ºå’Œé¢„å¤„ç†ç­–ç•¥
    4. æä¾›æ ‡å‡†åŒ–çš„æ•°æ®åŠ è½½å™¨æ¥å£

    ä¸latent-diffusionçš„æ•°æ®å¤„ç†æ–¹å¼å¯¹æ¯”ï¼š
    - åŒæ ·æ”¯æŒé…ç½®é©±åŠ¨çš„å‚æ•°è®¾ç½®
    - æä¾›çµæ´»çš„æ•°æ®å¢å¼ºç­–ç•¥
    - æ”¯æŒå¤šç§æ•°æ®ä½¿ç”¨æ¨¡å¼
    """

    def __init__(
        self,
        # æ•°æ®è·¯å¾„é…ç½®
        train_data_dir: str,
        test_data_dir: str,
        train_csv: str,
        test_csv: str,
        # è·¨ç›®å½•æ˜ å°„é…ç½®
        cross_directory_mapping: Optional[str] = None,
        # é€šé“é…ç½®
        channel_config: Dict[str, Any] = None,
        active_mode: str = "optical_only",
        # æ•°æ®åŠ è½½é…ç½®
        batch_size: int = 64,
        num_workers: int = 8,
        pin_memory: bool = True,
        shuffle_train: bool = True,
        # æ•°æ®åˆ†å‰²é…ç½®
        val_split: float = 0.2,
        stratify: bool = True,
        use_weighted_sampling: bool = False,
        # æ•°æ®é¢„å¤„ç†é…ç½®
        preprocessing: Optional[Dict] = None,
        augmentation: Optional[Dict] = None,
        # å…¶ä»–é…ç½®
        seed: int = 3407,
        **kwargs,
    ):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€æ•°æ®æ¨¡å—

        Args:
            train_data_dir: è®­ç»ƒæ•°æ®ç›®å½•
            test_data_dir: æµ‹è¯•æ•°æ®ç›®å½•
            train_csv: è®­ç»ƒæ•°æ®æ ‡ç­¾æ–‡ä»¶ï¼ˆæ¸…æ´æ•°æ®ï¼‰
            test_csv: æµ‹è¯•æ•°æ®æ ‡ç­¾æ–‡ä»¶
            cross_directory_mapping: è·¨ç›®å½•æ•°æ®è·¯å¾„æ˜ å°„æ–‡ä»¶ (JSONæ ¼å¼)
            channel_config: é€šé“é…ç½®å­—å…¸
            active_mode: å½“å‰ä½¿ç”¨çš„æ•°æ®æ¨¡å¼
            batch_size: æ‰¹æ¬¡å¤§å°
            num_workers: æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°
            pin_memory: æ˜¯å¦å°†æ•°æ®å›ºå®šåœ¨å†…å­˜ä¸­
            shuffle_train: æ˜¯å¦æ‰“ä¹±è®­ç»ƒæ•°æ®
            val_split: éªŒè¯é›†åˆ’åˆ†æ¯”ä¾‹
            stratify: æ˜¯å¦è¿›è¡Œåˆ†å±‚åˆ’åˆ†
            use_weighted_sampling: æ˜¯å¦ä½¿ç”¨åŠ æƒé‡‡æ ·
            preprocessing: é¢„å¤„ç†é…ç½®
            augmentation: æ•°æ®å¢å¼ºé…ç½®
            seed: éšæœºç§å­
        """
        super().__init__()

        # ä¿å­˜æ‰€æœ‰å‚æ•°ï¼ˆç”¨äºLightningçš„è¶…å‚æ•°è®°å½•ï¼‰
        self.save_hyperparameters()

        # æ•°æ®è·¯å¾„
        self.train_data_dir = Path(train_data_dir)
        self.test_data_dir = Path(test_data_dir)
        self.train_csv = Path(train_csv)
        self.test_csv = Path(test_csv)

        # è·¨ç›®å½•æ˜ å°„æ”¯æŒ
        self.cross_directory_mapping = cross_directory_mapping

        # é€šé“å’Œæ¨¡å¼é…ç½®
        self.channel_config = channel_config
        self.active_mode = active_mode

        # æ•°æ®åŠ è½½é…ç½®
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.pin_memory = pin_memory
        self.shuffle_train = shuffle_train

        # æ•°æ®åˆ†å‰²é…ç½®
        self.val_split = val_split
        self.stratify = stratify
        self.use_weighted_sampling = use_weighted_sampling
        self.seed = seed

        # é¢„å¤„ç†é…ç½®
        self.preprocessing = preprocessing or {}
        self.augmentation = augmentation or {}

        # éšæœºç§å­
        self.seed = seed

        # æ•°æ®é›†å¯¹è±¡ï¼ˆåœ¨setupä¸­åˆå§‹åŒ–ï¼‰
        self.train_dataset = None
        self.val_dataset = None
        self.test_dataset = None

        # æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        self._data_stats = {}

        # ğŸ”§ åŠ è½½è·¨ç›®å½•æ˜ å°„æ–‡ä»¶
        self._cross_directory_mapping_dict = self._load_cross_directory_mapping()

        logger.info("ğŸ”¢MultiModalDataModule initialized" + "=" * 100)
        logger.info(f"Active mode: {self.active_mode}")
        logger.info(f"Batch size: {self.batch_size}, Workers: {self.num_workers}")
        logger.info(f"Validation split: {self.val_split}")
        if self.cross_directory_mapping:
            logger.info(f"ğŸ”— Cross-directory mapping: {len(self._cross_directory_mapping_dict)} samples")

        logger.info("=" * 80)

    def _load_cross_directory_mapping(self) -> Dict[str, str]:
        """
        åŠ è½½è·¨ç›®å½•æ˜ å°„é…ç½®ï¼ˆç®€åŒ–ç‰ˆï¼‰

        ç®€åŒ–è¯´æ˜ï¼š
        - ç§»é™¤äº†å¤æ‚çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—
        - ä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½

        Returns:
            Dict[str, str]: æ ·æœ¬IDåˆ°å®Œæ•´è·¯å¾„çš„æ˜ å°„
        """
        if not self.cross_directory_mapping:
            return {}

        mapping_file = Path(self.cross_directory_mapping)
        if not mapping_file.exists():
            logger.warning(f"âš ï¸ Cross-directory mapping file not found: {mapping_file}")
            return {}

        try:
            with open(mapping_file, "r", encoding="utf-8") as f:
                mapping_data = json.load(f)

            logger.info(f"ğŸ“‚ Loaded cross-directory mapping: {len(mapping_data)} entries")
            return mapping_data

        except Exception as e:
            logger.error(f"âŒ Error loading cross-directory mapping: {e}")
            return {}

    def setup(self, stage: Optional[str] = None) -> None:
        """
        è®¾ç½®æ•°æ®é›†ï¼ˆæ¯ä¸ªè¿›ç¨‹æ‰§è¡Œï¼‰

        è¿™ä¸ªæ–¹æ³•åœ¨åˆ†å¸ƒå¼è®­ç»ƒçš„æ¯ä¸ªè¿›ç¨‹ä¸­éƒ½ä¼šè¢«è°ƒç”¨ã€‚
        ç®€åŒ–åçš„é€»è¾‘æ›´åŠ æ¸…æ™°ç›´æ¥ã€‚

        Args:
            stage: å½“å‰é˜¶æ®µ ('fit', 'validate', 'test', 'predict')
        """
        logger.info(f"ğŸ”§ Setting up data for stage: {stage}")

        if stage == "fit" or stage is None:
            # åˆ›å»ºå®Œæ•´çš„è®­ç»ƒæ•°æ®é›†
            full_dataset = create_train_dataset(
                data_dir=str(self.train_data_dir),
                csv_file=str(self.train_csv),
                transform=self._create_transforms("train"),  # âœ… ä½¿ç”¨åŸå§‹æ–¹æ³•
                channel_config=self.channel_config,
                usage_mode=self.active_mode,
                cross_directory_mapping=self._cross_directory_mapping_dict,
            )

            # æ•°æ®åˆ†å‰²
            self.train_dataset, self.val_dataset = self._split_dataset(
                full_dataset,
                self._create_transforms("train"),  # âœ… è®­ç»ƒå˜æ¢
                self._create_transforms("val"),  # âœ… éªŒè¯å˜æ¢
            )

            logger.info(f"âœ… Train dataset: {len(self.train_dataset)} samples")
            logger.info(f"âœ… Val dataset: {len(self.val_dataset)} samples")

        if stage == "test" or stage is None:
            # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
            self.test_dataset = create_test_dataset(
                data_dir=str(self.test_data_dir),
                csv_file=str(self.test_csv),
                transform=self._create_transforms("test"),  # âœ… ä½¿ç”¨åŸå§‹æ–¹æ³•
                channel_config=self.channel_config,
                usage_mode=self.active_mode,
            )

            logger.info(f"âœ… Test dataset: {len(self.test_dataset)} samples")

        if stage == "predict":
            # é¢„æµ‹é˜¶æ®µä½¿ç”¨æµ‹è¯•æ•°æ®é›†
            if self.test_dataset is None:
                self.test_dataset = create_test_dataset(
                    data_dir=str(self.test_data_dir),
                    csv_file=str(self.test_csv),
                    transform=self._create_transforms("test"),  # âœ… ä½¿ç”¨åŸå§‹æ–¹æ³•
                    channel_config=self.channel_config,
                    usage_mode=self.active_mode,
                )

    def _create_transforms(self, stage: str) -> Optional[Callable]:
        """
        æ ¹æ®é˜¶æ®µåˆ›å»ºæ•°æ®å˜æ¢ï¼ˆä¿æŒåŸå§‹å®ç°ï¼‰

        Args:
            stage: æ•°æ®é˜¶æ®µ ('train', 'val', 'test')

        Returns:
            æ•°æ®å˜æ¢å‡½æ•°ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™è¿”å›None
        """
        if stage not in self.augmentation:
            return None

        # ğŸ”§ è¿™é‡Œä¿æŒåŸå§‹çš„ç®€å•å®ç°
        # å¦‚æœéœ€è¦å¤æ‚å˜æ¢ï¼Œå¯ä»¥åç»­æ‰©å±•
        stage_config = self.augmentation.get(stage, {})

        # ç›®å‰è¿”å›Noneï¼Œè®©æ•°æ®é›†ç±»å¤„ç†åŸå§‹æ•°æ®
        # è¿™æ˜¯æœ€ç®€å•ã€æœ€ç¨³å®šçš„æ–¹æ¡ˆ
        return None

    def _split_dataset(self, full_dataset, train_transform, val_transform):
        """
        åˆ†å‰²æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œå¹¶ç”ŸæˆCSVåˆ†å‰²æ–‡ä»¶

        ğŸ”§ å¢å¼ºåŠŸèƒ½ï¼š
        - ä¿æŒåŸæœ‰çš„åˆ†å±‚åˆ’åˆ†é€»è¾‘
        - æ–°å¢ï¼šç”Ÿæˆ train_split.csv å’Œ val_split.csv
        - æ–°å¢ï¼šæ”¯æŒä¸»åŠ¨å­¦ä¹ æ—¶ä¿®æ”¹è®­ç»ƒé›†

        Args:
            full_dataset: å®Œæ•´æ•°æ®é›†
            train_transform: è®­ç»ƒæ•°æ®å˜æ¢
            val_transform: éªŒè¯æ•°æ®å˜æ¢

        Returns:
            Tuple: (è®­ç»ƒæ•°æ®é›†, éªŒè¯æ•°æ®é›†)
        """
        # è·å–æ•°æ®ç´¢å¼•
        total_size = len(full_dataset)
        val_size = int(self.val_split * total_size)
        train_size = total_size - val_size

        # ğŸ”§ æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åˆ†å‰²æ–‡ä»¶ï¼ˆç”¨äºä¸»åŠ¨å­¦ä¹ ï¼‰
        train_split_file = self.train_csv.parent / "train_split.csv"
        val_split_file = self.train_csv.parent / "val_split.csv"

        if train_split_file.exists() and val_split_file.exists():
            logger.info("ğŸ”„ Found existing train/val split files, loading...")
            train_indices, val_indices = self._load_existing_splits(full_dataset, train_split_file, val_split_file)
        else:
            logger.info("ğŸ†• Creating new train/val split...")
            train_indices, val_indices = self._create_new_splits(full_dataset, total_size)
            # ä¿å­˜åˆ†å‰²ç»“æœ
            self._save_splits(full_dataset, train_indices, val_indices, train_split_file, val_split_file)

        # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
        train_dataset = DatasetSubset(full_dataset, train_indices, train_transform)
        val_dataset = DatasetSubset(full_dataset, val_indices, val_transform)

        # æ‰“å°åˆ†å‰²ç»Ÿè®¡ä¿¡æ¯
        self._print_split_statistics(full_dataset, train_indices, val_indices)

        return train_dataset, val_dataset

    def _create_new_splits(self, full_dataset, total_size):
        """
        åˆ›å»ºæ–°çš„æ•°æ®åˆ†å‰²

        Args:
            full_dataset: å®Œæ•´æ•°æ®é›†
            total_size: æ€»æ ·æœ¬æ•°

        Returns:
            Tuple: (è®­ç»ƒç´¢å¼•, éªŒè¯ç´¢å¼•)
        """
        val_size = int(self.val_split * total_size)
        train_size = total_size - val_size

        # åˆ†å±‚åˆ’åˆ†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.stratify and full_dataset.has_labels:
            # åŸºäºæ ‡ç­¾è¿›è¡Œåˆ†å±‚åˆ’åˆ†
            labels = [full_dataset.data_index.iloc[i]["label"] for i in range(total_size)]
            train_indices, val_indices = train_test_split(
                range(total_size),
                test_size=self.val_split,
                stratify=labels,
                random_state=self.seed,
            )
            logger.info(f"âœ… Used stratified split with seed {self.seed}")
        else:
            # éšæœºåˆ’åˆ†
            torch.manual_seed(self.seed)
            train_indices, val_indices = random_split(range(total_size), [train_size, val_size])
            train_indices = train_indices.indices
            val_indices = val_indices.indices
            logger.info(f"âœ… Used random split with seed {self.seed}")

        return train_indices, val_indices

    def _save_splits(self, full_dataset, train_indices, val_indices, train_split_file, val_split_file):
        """
        ä¿å­˜æ•°æ®åˆ†å‰²åˆ°CSVæ–‡ä»¶

        Args:
            full_dataset: å®Œæ•´æ•°æ®é›†
            train_indices: è®­ç»ƒé›†ç´¢å¼•
            val_indices: éªŒè¯é›†ç´¢å¼•
            train_split_file: è®­ç»ƒåˆ†å‰²æ–‡ä»¶è·¯å¾„
            val_split_file: éªŒè¯åˆ†å‰²æ–‡ä»¶è·¯å¾„
        """
        import pandas as pd

        # åˆ›å»ºè®­ç»ƒé›†CSV
        train_data = []
        for idx in train_indices:
            row = full_dataset.data_index.iloc[idx].copy()
            train_data.append(row)

        train_df = pd.DataFrame(train_data)
        train_df.to_csv(train_split_file, index=False)

        # åˆ›å»ºéªŒè¯é›†CSV
        val_data = []
        for idx in val_indices:
            row = full_dataset.data_index.iloc[idx].copy()
            val_data.append(row)

        val_df = pd.DataFrame(val_data)
        val_df.to_csv(val_split_file, index=False)

        logger.info(f"ğŸ’¾ Saved train split: {train_split_file} ({len(train_df)} samples)")
        logger.info(f"ğŸ’¾ Saved val split: {val_split_file} ({len(val_df)} samples)")

    def _load_existing_splits(self, full_dataset, train_split_file, val_split_file):
        """
        ä»ç°æœ‰CSVæ–‡ä»¶åŠ è½½æ•°æ®åˆ†å‰²

        Args:
            full_dataset: å®Œæ•´æ•°æ®é›†
            train_split_file: è®­ç»ƒåˆ†å‰²æ–‡ä»¶è·¯å¾„
            val_split_file: éªŒè¯åˆ†å‰²æ–‡ä»¶è·¯å¾„

        Returns:
            Tuple: (è®­ç»ƒç´¢å¼•, éªŒè¯ç´¢å¼•)
        """
        import pandas as pd

        # è¯»å–åˆ†å‰²æ–‡ä»¶
        train_split_df = pd.read_csv(train_split_file)
        val_split_df = pd.read_csv(val_split_file)

        # è·å–IDåˆ—è¡¨
        train_ids = set(train_split_df["ID"].tolist())
        val_ids = set(val_split_df["ID"].tolist())

        # åœ¨å®Œæ•´æ•°æ®é›†ä¸­æ‰¾åˆ°å¯¹åº”çš„ç´¢å¼•
        train_indices = []
        val_indices = []

        for idx in range(len(full_dataset)):
            sample_id = full_dataset.data_index.iloc[idx]["ID"]
            if sample_id in train_ids:
                train_indices.append(idx)
            elif sample_id in val_ids:
                val_indices.append(idx)

        logger.info(f"ğŸ“‚ Loaded existing splits: {len(train_indices)} train, {len(val_indices)} val")
        return train_indices, val_indices

    def _print_split_statistics(self, full_dataset, train_indices, val_indices):
        """
        æ‰“å°æ•°æ®åˆ†å‰²ç»Ÿè®¡ä¿¡æ¯

        Args:
            full_dataset: å®Œæ•´æ•°æ®é›†
            train_indices: è®­ç»ƒé›†ç´¢å¼•
            val_indices: éªŒè¯é›†ç´¢å¼•
        """
        if not full_dataset.has_labels:
            logger.info(f"ğŸ“Š Split: {len(train_indices)} train, {len(val_indices)} val (no labels)")
            return

        # ç»Ÿè®¡è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ
        train_labels = [full_dataset.data_index.iloc[idx]["label"] for idx in train_indices]
        train_class_counts = pd.Series(train_labels).value_counts().sort_index()

        # ç»Ÿè®¡éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ
        val_labels = [full_dataset.data_index.iloc[idx]["label"] for idx in val_indices]
        val_class_counts = pd.Series(val_labels).value_counts().sort_index()

        # è®¡ç®—æ¯”ä¾‹
        train_ratio = train_class_counts[1] / len(train_labels) if len(train_labels) > 0 else 0
        val_ratio = val_class_counts[1] / len(val_labels) if len(val_labels) > 0 else 0

        logger.info("ğŸ“Š Dataset split statistics:")
        logger.info(f"   Training: {len(train_indices)} samples")
        logger.info(f"     - Class 0 (Non-landslide): {train_class_counts.get(0, 0)} ({(1-train_ratio)*100:.1f}%)")
        logger.info(f"     - Class 1 (Landslide): {train_class_counts.get(1, 0)} ({train_ratio*100:.1f}%)")
        logger.info(f"   Validation: {len(val_indices)} samples")
        logger.info(f"     - Class 0 (Non-landslide): {val_class_counts.get(0, 0)} ({(1-val_ratio)*100:.1f}%)")
        logger.info(f"     - Class 1 (Landslide): {val_class_counts.get(1, 0)} ({val_ratio*100:.1f}%)")

        # æ£€æŸ¥åˆ†å±‚æ•ˆæœ
        ratio_diff = abs(train_ratio - val_ratio)
        if ratio_diff < 0.02:  # 2%ä»¥å†…è®¤ä¸ºæ˜¯å¥½çš„åˆ†å±‚æ•ˆæœ
            logger.info(f"âœ… Good stratification: ratio difference = {ratio_diff:.3f}")
        else:
            logger.warning(f"âš ï¸ Stratification may be imperfect: ratio difference = {ratio_diff:.3f}")

    def train_dataloader(self) -> DataLoader:
        """åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=self.shuffle_train,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=False,
        )

    def val_dataloader(self) -> DataLoader:
        """åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=False,
        )

    def test_dataloader(self) -> DataLoader:
        """åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=False,
        )

    def predict_dataloader(self) -> DataLoader:
        """åˆ›å»ºé¢„æµ‹æ•°æ®åŠ è½½å™¨"""
        return self.test_dataloader()

    def _get_test_transforms(self):
        """è·å–æµ‹è¯•æ•°æ®å˜æ¢"""
        from .transforms import get_test_transforms

        return get_test_transforms(self.augmentation.get("test", {}))

    # =============================================================================
    # ğŸ”§ ä¸»åŠ¨å­¦ä¹ æ”¯æŒæ–¹æ³•
    # =============================================================================

    def add_samples_to_train_split(self, new_sample_ids: List[str], new_labels: Optional[List[int]] = None):
        """
        å°†æ–°æ ·æœ¬æ·»åŠ åˆ°è®­ç»ƒåˆ†å‰²ä¸­ï¼ˆç”¨äºä¸»åŠ¨å­¦ä¹ ï¼‰

        Args:
            new_sample_ids: æ–°æ ·æœ¬IDåˆ—è¡¨
            new_labels: æ–°æ ·æœ¬æ ‡ç­¾åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        """
        train_split_file = self.train_csv.parent / "train_split.csv"

        if not train_split_file.exists():
            logger.error("âŒ train_split.csv does not exist. Please run initial training first.")
            return

        import pandas as pd

        # è¯»å–ç°æœ‰è®­ç»ƒåˆ†å‰²
        train_df = pd.read_csv(train_split_file)

        # å‡†å¤‡æ–°æ ·æœ¬æ•°æ®
        new_rows = []
        for i, sample_id in enumerate(new_sample_ids):
            new_row = {"ID": sample_id}
            if new_labels is not None and i < len(new_labels):
                new_row["label"] = new_labels[i]
            else:
                # å¦‚æœæ²¡æœ‰æä¾›æ ‡ç­¾ï¼Œå°è¯•ä»åŸå§‹æ•°æ®ä¸­æŸ¥æ‰¾
                original_df = pd.read_csv(self.train_csv)
                if sample_id in original_df["ID"].values:
                    original_row = original_df[original_df["ID"] == sample_id].iloc[0]
                    new_row.update(original_row.to_dict())
            new_rows.append(new_row)

        # æ·»åŠ æ–°æ ·æœ¬åˆ°è®­ç»ƒé›†
        new_train_df = pd.concat([train_df, pd.DataFrame(new_rows)], ignore_index=True)

        # å»é‡ï¼ˆé˜²æ­¢é‡å¤æ·»åŠ ï¼‰
        new_train_df = new_train_df.drop_duplicates(subset=["ID"], keep="last")

        # ä¿å­˜æ›´æ–°åçš„è®­ç»ƒåˆ†å‰²
        new_train_df.to_csv(train_split_file, index=False)

        added_count = len(new_train_df) - len(train_df)
        logger.info(f"âœ… Added {added_count} new samples to training set")
        logger.info(f"ğŸ“ˆ Training set size: {len(train_df)} â†’ {len(new_train_df)}")

    def get_split_info(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰æ•°æ®åˆ†å‰²ä¿¡æ¯

        Returns:
            Dict: åŒ…å«åˆ†å‰²ä¿¡æ¯çš„å­—å…¸
        """
        train_split_file = self.train_csv.parent / "train_split.csv"
        val_split_file = self.train_csv.parent / "val_split.csv"

        info = {
            "train_split_exists": train_split_file.exists(),
            "val_split_exists": val_split_file.exists(),
            "train_split_file": str(train_split_file),
            "val_split_file": str(val_split_file),
        }

        if train_split_file.exists():
            import pandas as pd

            train_df = pd.read_csv(train_split_file)
            info["train_size"] = len(train_df)
            if "label" in train_df.columns:
                info["train_class_distribution"] = train_df["label"].value_counts().to_dict()

        if val_split_file.exists():
            import pandas as pd

            val_df = pd.read_csv(val_split_file)
            info["val_size"] = len(val_df)
            if "label" in val_df.columns:
                info["val_class_distribution"] = val_df["label"].value_counts().to_dict()

        return info


class DatasetSubset:
    """
    æ•°æ®é›†å­é›†åŒ…è£…å™¨

    ç”¨äºå°†å®Œæ•´æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒ/éªŒè¯å­é›†ï¼Œ
    å¹¶ä¸ºæ¯ä¸ªå­é›†åº”ç”¨ä¸åŒçš„æ•°æ®å˜æ¢ã€‚
    """

    def __init__(self, dataset, indices, transform=None):
        """
        åˆå§‹åŒ–æ•°æ®é›†å­é›†

        Args:
            dataset: åŸå§‹æ•°æ®é›†
            indices: å­é›†ç´¢å¼•åˆ—è¡¨
            transform: æ•°æ®å˜æ¢å‡½æ•°
        """
        self.dataset = dataset
        self.indices = indices
        self.transform = transform

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        # è·å–åŸå§‹æ•°æ®
        original_idx = self.indices[idx]
        image, label = self.dataset[original_idx]

        # åº”ç”¨å˜æ¢
        if self.transform:
            image = self.transform(image)

        return image, label
