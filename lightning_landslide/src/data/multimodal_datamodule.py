# =============================================================================
# lightning_landslide/src/data/multimodal_datamodule.py - å¤šæ¨¡æ€æ•°æ®æ¨¡å—
# =============================================================================

"""
å¤šæ¨¡æ€æ•°æ®æ¨¡å— - Lightning DataModuleå®ç°

è¿™ä¸ªæ¨¡å—æ˜¯æ‚¨é¡¹ç›®æ•°æ®å¤„ç†çš„æ ¸å¿ƒï¼Œå®ƒç»§æ‰¿äº†PyTorch Lightningçš„
DataModuleç±»ï¼Œæä¾›äº†æ ‡å‡†åŒ–çš„æ•°æ®åŠ è½½å’Œå¤„ç†æµç¨‹ã€‚

è®¾è®¡ç†å¿µï¼š
1. é…ç½®é©±åŠ¨ï¼šæ‰€æœ‰å‚æ•°éƒ½é€šè¿‡é…ç½®æ–‡ä»¶æ§åˆ¶
2. çµæ´»æ€§ï¼šæ”¯æŒä¸åŒçš„æ•°æ®ä½¿ç”¨æ¨¡å¼ï¼ˆä»…å…‰å­¦ã€å¤šæ¨¡æ€ç­‰ï¼‰
3. å¯é‡ç°æ€§ï¼šå›ºå®šéšæœºç§å­ï¼Œç¡®ä¿æ•°æ®åˆ†å‰²ä¸€è‡´
4. é«˜æ•ˆæ€§ï¼šæ”¯æŒå¤šè¿›ç¨‹æ•°æ®åŠ è½½å’Œå†…å­˜ä¼˜åŒ–
"""

import pytorch_lightning as pl
from torch.utils.data import DataLoader, random_split
from typing import Optional, Dict, Any, List, Callable
import logging
from pathlib import Path
import torch
from sklearn.model_selection import train_test_split
import numpy as np
import json

from .multimodal_dataset import MultiModalDataset, create_train_dataset, create_test_dataset

logger = logging.getLogger(__name__)


class MultiModalDataModule(pl.LightningDataModule):
    """
    å¤šæ¨¡æ€é¥æ„Ÿæ•°æ®çš„Lightningæ•°æ®æ¨¡å—

    è¿™ä¸ªç±»æ˜¯æ‚¨é¡¹ç›®çš„æ•°æ®å¤„ç†ä¸­å¿ƒã€‚å®ƒè´Ÿè´£ï¼š
    1. ç®¡ç†å¤šé€šé“é¥æ„Ÿæ•°æ®çš„åŠ è½½
    2. å¤„ç†è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®é›†çš„åˆ’åˆ†
    3. é…ç½®æ•°æ®å¢å¼ºå’Œé¢„å¤„ç†ç­–ç•¥
    4. æä¾›æ ‡å‡†åŒ–çš„æ•°æ®åŠ è½½å™¨æ¥å£

    ä¸latent-diffusionçš„æ•°æ®å¤„ç†æ–¹å¼å¯¹æ¯”ï¼š
    - åŒæ ·æ”¯æŒé…ç½®é©±åŠ¨çš„å‚æ•°è®¾ç½®
    - æä¾›çµæ´»çš„æ•°æ®å¢å¼ºç­–ç•¥
    - æ”¯æŒå¤šç§æ•°æ®ä½¿ç”¨æ¨¡å¼
    """

    def __init__(
        self,
        # æ•°æ®è·¯å¾„é…ç½®
        train_data_dir: str,
        test_data_dir: str,
        train_csv: str,
        test_csv: str,
        # è·¨ç›®å½•æ˜ å°„é…ç½®
        cross_directory_mapping: Optional[str] = None,
        # é€šé“é…ç½®
        channel_config: Dict[str, Any] = None,
        active_mode: str = "optical_only",
        # æ•°æ®åŠ è½½é…ç½®
        batch_size: int = 64,
        num_workers: int = 8,
        pin_memory: bool = True,
        shuffle_train: bool = True,
        # æ•°æ®åˆ†å‰²é…ç½®
        val_split: float = 0.2,
        stratify: bool = True,
        use_weighted_sampling: bool = False,
        # æ•°æ®é¢„å¤„ç†é…ç½®
        preprocessing: Optional[Dict] = None,
        augmentation: Optional[Dict] = None,
        # å…¶ä»–é…ç½®
        seed: int = 3407,
        **kwargs,
    ):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€æ•°æ®æ¨¡å—

        Args:
            train_data_dir: è®­ç»ƒæ•°æ®ç›®å½•
            test_data_dir: æµ‹è¯•æ•°æ®ç›®å½•
            train_csv: è®­ç»ƒæ•°æ®æ ‡ç­¾æ–‡ä»¶ï¼ˆæ¸…æ´æ•°æ®ï¼‰
            test_csv: æµ‹è¯•æ•°æ®æ ‡ç­¾æ–‡ä»¶
            cross_directory_mapping: è·¨ç›®å½•æ•°æ®è·¯å¾„æ˜ å°„æ–‡ä»¶ (JSONæ ¼å¼)
            channel_config: é€šé“é…ç½®å­—å…¸
            active_mode: å½“å‰ä½¿ç”¨çš„æ•°æ®æ¨¡å¼
            batch_size: æ‰¹æ¬¡å¤§å°
            num_workers: æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°
            pin_memory: æ˜¯å¦å°†æ•°æ®å›ºå®šåœ¨å†…å­˜ä¸­
            shuffle_train: æ˜¯å¦æ‰“ä¹±è®­ç»ƒæ•°æ®
            val_split: éªŒè¯é›†åˆ’åˆ†æ¯”ä¾‹
            stratify: æ˜¯å¦è¿›è¡Œåˆ†å±‚åˆ’åˆ†
            use_weighted_sampling: æ˜¯å¦ä½¿ç”¨åŠ æƒé‡‡æ ·
            preprocessing: é¢„å¤„ç†é…ç½®
            augmentation: æ•°æ®å¢å¼ºé…ç½®
            seed: éšæœºç§å­
        """
        super().__init__()

        # ä¿å­˜æ‰€æœ‰å‚æ•°ï¼ˆç”¨äºLightningçš„è¶…å‚æ•°è®°å½•ï¼‰
        self.save_hyperparameters()

        # æ•°æ®è·¯å¾„
        self.train_data_dir = Path(train_data_dir)
        self.test_data_dir = Path(test_data_dir)
        self.train_csv = Path(train_csv)
        self.test_csv = Path(test_csv)

        # è·¨ç›®å½•æ˜ å°„æ”¯æŒ
        self.cross_directory_mapping = cross_directory_mapping

        # é€šé“å’Œæ¨¡å¼é…ç½®
        self.channel_config = channel_config
        self.active_mode = active_mode

        # æ•°æ®åŠ è½½é…ç½®
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.pin_memory = pin_memory
        self.shuffle_train = shuffle_train

        # æ•°æ®åˆ†å‰²é…ç½®
        self.val_split = val_split
        self.stratify = stratify
        self.use_weighted_sampling = use_weighted_sampling
        self.seed = seed

        # é¢„å¤„ç†é…ç½®
        self.preprocessing = preprocessing or {}
        self.augmentation = augmentation or {}

        # éšæœºç§å­
        self.seed = seed

        # æ•°æ®é›†å¯¹è±¡ï¼ˆåœ¨setupä¸­åˆå§‹åŒ–ï¼‰
        self.train_dataset = None
        self.val_dataset = None
        self.test_dataset = None

        # æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        self._data_stats = {}

        # ğŸ”§ åŠ è½½è·¨ç›®å½•æ˜ å°„æ–‡ä»¶
        self._cross_directory_mapping_dict = self._load_cross_directory_mapping()

        logger.info("ğŸ”¢MultiModalDataModule initialized" + "=" * 100)
        logger.info(f"Active mode: {self.active_mode}")
        logger.info(f"Batch size: {self.batch_size}, Workers: {self.num_workers}")
        logger.info(f"Validation split: {self.val_split}")
        if self.cross_directory_mapping:
            logger.info(f"ğŸ”— Cross-directory mapping: {len(self._cross_directory_mapping_dict)} samples")

        logger.info("=" * 80)

    def _load_cross_directory_mapping(self) -> Dict[str, str]:
        """
        åŠ è½½è·¨ç›®å½•æ˜ å°„é…ç½®ï¼ˆç®€åŒ–ç‰ˆï¼‰

        ç®€åŒ–è¯´æ˜ï¼š
        - ç§»é™¤äº†å¤æ‚çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—
        - ä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½

        Returns:
            Dict[str, str]: æ ·æœ¬IDåˆ°å®Œæ•´è·¯å¾„çš„æ˜ å°„
        """
        if not self.cross_directory_mapping:
            return {}

        mapping_file = Path(self.cross_directory_mapping)
        if not mapping_file.exists():
            logger.warning(f"âš ï¸ Cross-directory mapping file not found: {mapping_file}")
            return {}

        try:
            with open(mapping_file, "r", encoding="utf-8") as f:
                mapping_data = json.load(f)

            logger.info(f"ğŸ“‚ Loaded cross-directory mapping: {len(mapping_data)} entries")
            return mapping_data

        except Exception as e:
            logger.error(f"âŒ Error loading cross-directory mapping: {e}")
            return {}

    def setup(self, stage: Optional[str] = None) -> None:
        """
        è®¾ç½®æ•°æ®é›†ï¼ˆæ¯ä¸ªè¿›ç¨‹æ‰§è¡Œï¼‰

        è¿™ä¸ªæ–¹æ³•åœ¨åˆ†å¸ƒå¼è®­ç»ƒçš„æ¯ä¸ªè¿›ç¨‹ä¸­éƒ½ä¼šè¢«è°ƒç”¨ã€‚
        ç®€åŒ–åçš„é€»è¾‘æ›´åŠ æ¸…æ™°ç›´æ¥ã€‚

        Args:
            stage: å½“å‰é˜¶æ®µ ('fit', 'validate', 'test', 'predict')
        """
        logger.info(f"ğŸ”§ Setting up data for stage: {stage}")

        if stage == "fit" or stage is None:
            # åˆ›å»ºå®Œæ•´çš„è®­ç»ƒæ•°æ®é›†
            full_dataset = create_train_dataset(
                data_dir=str(self.train_data_dir),
                csv_file=str(self.train_csv),
                transform=self._create_transforms("train"),  # âœ… ä½¿ç”¨åŸå§‹æ–¹æ³•
                channel_config=self.channel_config,
                usage_mode=self.active_mode,
                cross_directory_mapping=self._cross_directory_mapping_dict,
            )

            # æ•°æ®åˆ†å‰²
            self.train_dataset, self.val_dataset = self._split_dataset(
                full_dataset,
                self._create_transforms("train"),  # âœ… è®­ç»ƒå˜æ¢
                self._create_transforms("val"),  # âœ… éªŒè¯å˜æ¢
            )

            logger.info(f"âœ… Train dataset: {len(self.train_dataset)} samples")
            logger.info(f"âœ… Val dataset: {len(self.val_dataset)} samples")

        if stage == "test" or stage is None:
            # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
            self.test_dataset = create_test_dataset(
                data_dir=str(self.test_data_dir),
                csv_file=str(self.test_csv),
                transform=self._create_transforms("test"),  # âœ… ä½¿ç”¨åŸå§‹æ–¹æ³•
                channel_config=self.channel_config,
                usage_mode=self.active_mode,
            )

            logger.info(f"âœ… Test dataset: {len(self.test_dataset)} samples")

        if stage == "predict":
            # é¢„æµ‹é˜¶æ®µä½¿ç”¨æµ‹è¯•æ•°æ®é›†
            if self.test_dataset is None:
                self.test_dataset = create_test_dataset(
                    data_dir=str(self.test_data_dir),
                    csv_file=str(self.test_csv),
                    transform=self._create_transforms("test"),  # âœ… ä½¿ç”¨åŸå§‹æ–¹æ³•
                    channel_config=self.channel_config,
                    usage_mode=self.active_mode,
                )

    def _create_transforms(self, stage: str) -> Optional[Callable]:
        """
        æ ¹æ®é˜¶æ®µåˆ›å»ºæ•°æ®å˜æ¢ï¼ˆä¿æŒåŸå§‹å®ç°ï¼‰

        Args:
            stage: æ•°æ®é˜¶æ®µ ('train', 'val', 'test')

        Returns:
            æ•°æ®å˜æ¢å‡½æ•°ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™è¿”å›None
        """
        if stage not in self.augmentation:
            return None

        # ğŸ”§ è¿™é‡Œä¿æŒåŸå§‹çš„ç®€å•å®ç°
        # å¦‚æœéœ€è¦å¤æ‚å˜æ¢ï¼Œå¯ä»¥åç»­æ‰©å±•
        stage_config = self.augmentation.get(stage, {})

        # ç›®å‰è¿”å›Noneï¼Œè®©æ•°æ®é›†ç±»å¤„ç†åŸå§‹æ•°æ®
        # è¿™æ˜¯æœ€ç®€å•ã€æœ€ç¨³å®šçš„æ–¹æ¡ˆ
        return None

    def _split_dataset(self, full_dataset, train_transform, val_transform):
        """
        åˆ†å‰²æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†

        Args:
            full_dataset: å®Œæ•´æ•°æ®é›†
            train_transform: è®­ç»ƒæ•°æ®å˜æ¢
            val_transform: éªŒè¯æ•°æ®å˜æ¢

        Returns:
            Tuple: (è®­ç»ƒæ•°æ®é›†, éªŒè¯æ•°æ®é›†)
        """
        # è·å–æ•°æ®ç´¢å¼•
        total_size = len(full_dataset)
        val_size = int(self.val_split * total_size)
        train_size = total_size - val_size

        # åˆ†å±‚åˆ’åˆ†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.stratify and full_dataset.has_labels:
            # åŸºäºæ ‡ç­¾è¿›è¡Œåˆ†å±‚åˆ’åˆ†
            labels = [full_dataset.data_index.iloc[i]["label"] for i in range(total_size)]
            train_indices, val_indices = train_test_split(
                range(total_size), test_size=self.val_split, stratify=labels, random_state=self.seed
            )
        else:
            # éšæœºåˆ’åˆ†
            torch.manual_seed(self.seed)
            train_indices, val_indices = random_split(range(total_size), [train_size, val_size])
            train_indices = train_indices.indices
            val_indices = val_indices.indices

        # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
        train_dataset = DatasetSubset(full_dataset, train_indices, train_transform)
        val_dataset = DatasetSubset(full_dataset, val_indices, val_transform)

        return train_dataset, val_dataset

    def train_dataloader(self) -> DataLoader:
        """åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=self.shuffle_train,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=False,
        )

    def val_dataloader(self) -> DataLoader:
        """åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=False,
        )

    def test_dataloader(self) -> DataLoader:
        """åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=False,
        )

    def predict_dataloader(self) -> DataLoader:
        """åˆ›å»ºé¢„æµ‹æ•°æ®åŠ è½½å™¨"""
        return self.test_dataloader()

    def _get_test_transforms(self):
        """è·å–æµ‹è¯•æ•°æ®å˜æ¢"""
        from .transforms import get_test_transforms

        return get_test_transforms(self.augmentation.get("test", {}))


class DatasetSubset:
    """
    æ•°æ®é›†å­é›†åŒ…è£…å™¨

    ç”¨äºå°†å®Œæ•´æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒ/éªŒè¯å­é›†ï¼Œ
    å¹¶ä¸ºæ¯ä¸ªå­é›†åº”ç”¨ä¸åŒçš„æ•°æ®å˜æ¢ã€‚
    """

    def __init__(self, dataset, indices, transform=None):
        """
        åˆå§‹åŒ–æ•°æ®é›†å­é›†

        Args:
            dataset: åŸå§‹æ•°æ®é›†
            indices: å­é›†ç´¢å¼•åˆ—è¡¨
            transform: æ•°æ®å˜æ¢å‡½æ•°
        """
        self.dataset = dataset
        self.indices = indices
        self.transform = transform

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        # è·å–åŸå§‹æ•°æ®
        original_idx = self.indices[idx]
        image, label = self.dataset[original_idx]

        # åº”ç”¨å˜æ¢
        if self.transform:
            image = self.transform(image)

        return image, label
