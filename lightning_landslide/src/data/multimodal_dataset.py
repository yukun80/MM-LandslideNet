import torch
import numpy as np
import pandas as pd
from torch.utils.data import Dataset
from pathlib import Path
import logging
from typing import Tuple, Optional, Callable, List, Dict, Any, Union

logger = logging.getLogger(__name__)


class MultiModalDataset(Dataset):
    """
    ç®€åŒ–çš„å¤šæ¨¡æ€é¥æ„Ÿæ•°æ®é›†ç±»

    æ ¸å¿ƒèŒè´£ï¼ˆç®€åŒ–åï¼‰ï¼š
    1. æ•°æ®åŠ è½½ï¼šä».npyæ–‡ä»¶åŠ è½½å¤šé€šé“æ•°æ®
    2. é€šé“å¤„ç†ï¼šæå–å¹¶ç»„åˆå…‰å­¦é€šé“
    3. NDVIè®¡ç®—ï¼šå®æ—¶è®¡ç®—å½’ä¸€åŒ–æ¤è¢«æŒ‡æ•°
    4. æ ‡ç­¾å¤„ç†ï¼šæ­£ç¡®å¤„ç†äºŒåˆ†ç±»æ ‡ç­¾
    5. æ•°æ®å˜æ¢ï¼šåº”ç”¨é¢„å¤„ç†å’Œå¢å¼º

    """

    def __init__(
        self,
        data_dir: Union[str, Path],
        csv_file: Union[str, Path],
        transform: Optional[Callable] = None,
        compute_ndvi: bool = True,
        cache_data: bool = True,
        channel_config: Optional[Dict] = None,
        usage_mode: str = "optical_only",
        # ğŸ”§ è·¨ç›®å½•æ˜ å°„æ”¯æŒï¼ˆä¿ç•™é«˜çº§åŠŸèƒ½ï¼‰
        cross_directory_mapping: Optional[Dict[str, str]] = None,
    ):
        """
        åˆå§‹åŒ–å…‰å­¦æ•°æ®é›†

        Args:
            data_dir: æ•°æ®æ–‡ä»¶ç›®å½•è·¯å¾„
            csv_file: æ ‡ç­¾æ–‡ä»¶è·¯å¾„ï¼ˆå·²æ¸…æ´çš„CSVæ–‡ä»¶ï¼‰
            transform: æ•°æ®å˜æ¢å‡½æ•°
            compute_ndvi: æ˜¯å¦è®¡ç®—NDVIé€šé“
            cache_data: æ˜¯å¦ç¼“å­˜æ•°æ®åˆ°å†…å­˜ï¼ˆå°æ•°æ®é›†æ—¶æœ‰ç”¨ï¼‰
            channel_config: è¾“å…¥æ•°æ®é€šé“é…ç½®
            usage_mode: ä½¿ç”¨æ¨¡å¼
            cross_directory_mapping: è·¨ç›®å½•æ•°æ®è·¯å¾„æ˜ å°„å­—å…¸ {sample_id: full_path}
        """
        logger.info("MultiModalDataset_init" + "-" * 100)
        # è·¯å¾„å¤„ç†
        self.data_dir = Path(data_dir)
        self.csv_file = Path(csv_file)

        # æ•°æ®å¤„ç†é…ç½®
        self.transform = transform
        self.compute_ndvi = compute_ndvi
        self.cache_data = cache_data

        # é€šé“é…ç½®
        self.channel_config = channel_config
        self.usage_mode = usage_mode
        self.active_channels = self._parse_active_channels()

        # ğŸ”§ è·¨ç›®å½•æ˜ å°„æ”¯æŒï¼ˆä¿ç•™é«˜çº§åŠŸèƒ½ï¼‰
        self.cross_directory_mapping = cross_directory_mapping or {}

        # è®¡ç®—æœ€ç»ˆé€šé“æ•°
        self.num_channels = len(self.active_channels)

        # æ•°æ®ç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.data_cache = {} if cache_data else None

        # ğŸ¯ ç®€åŒ–çš„æ•°æ®åŠ è½½æµç¨‹
        self.data_index = self._load_data_index()

        # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡ç­¾åˆ—
        self.has_labels = "label" in self.data_index.columns

        # æ—¥å¿—ä¿¡æ¯
        logger.info(f"ğŸ“Š Loaded {len(self.data_index)} samples from cleaned CSV")
        logger.info(f"ğŸ”¢ Active channels: {self.active_channels}")
        logger.info(f"ğŸ”¢ Final channel count: {self.num_channels}")
        logger.info(f"ğŸ“‹ Has labels: {self.has_labels}")

        # ğŸ”§ è·¨ç›®å½•æ˜ å°„ä¿¡æ¯
        if self.cross_directory_mapping:
            logger.info(f"ğŸ“ Cross-directory mapping: {len(self.cross_directory_mapping)} samples")

        logger.info("ğŸ”¢âœ… MultiModalDataset initialization completed!")

    def _parse_active_channels(self) -> List[int]:
        """è§£æå½“å‰ä½¿ç”¨æ¨¡å¼ä¸‹çš„æ´»è·ƒé€šé“"""
        mode_config = self.channel_config["usage_modes"][self.usage_mode]
        active_groups = mode_config["groups"]

        active_channels = []

        for group_name in active_groups:
            group_channels = self.channel_config["channel_groups"][group_name]
            active_channels.extend(group_channels)

        return active_channels

    def _load_data_index(self) -> pd.DataFrame:
        """
        åŠ è½½æ•°æ®ç´¢å¼•æ–‡ä»¶

        è¿™ä¸ªæ–¹æ³•è¯»å–CSVæ–‡ä»¶ï¼Œå»ºç«‹æ ·æœ¬IDåˆ°æ ‡ç­¾çš„æ˜ å°„ã€‚
        Returns:
            åŒ…å«IDå’Œæ ‡ç­¾çš„DataFrame
        """
        if not self.csv_file.exists():
            raise FileNotFoundError(f"CSV file not found: {self.csv_file}")

        df = pd.read_csv(self.csv_file)

        # åŸºæœ¬æ•°æ®éªŒè¯
        if "ID" not in df.columns:
            raise ValueError("CSV file must contain 'ID' column")

        # æ£€æŸ¥æ ‡ç­¾åˆ—ï¼ˆè®­ç»ƒé›†æœ‰ï¼Œæµ‹è¯•é›†å¯èƒ½æ²¡æœ‰ï¼‰
        has_labels = "label" in df.columns

        logger.info(f"ğŸ”¢ Loaded {len(df)} samples from CSV")
        return df.reset_index(drop=True)

    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.data_index)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è·å–å•ä¸ªæ•°æ®æ ·æœ¬

        è¿™æ˜¯æ•°æ®é›†ç±»çš„æ ¸å¿ƒæ–¹æ³•ã€‚å®ƒçš„èŒè´£æ˜¯ï¼š
        1. åŠ è½½åŸå§‹æ•°æ®
        2. æå–å’Œå¤„ç†æŒ‡å®šé€šé“
        3. è®¡ç®—NDVIï¼ˆå¦‚æœéœ€è¦ï¼‰
        4. åº”ç”¨æ•°æ®å˜æ¢
        5. è¿”å›tensoræ ¼å¼çš„æ•°æ®å’Œæ ‡ç­¾

        Args:
            idx: æ ·æœ¬ç´¢å¼•

        Returns:
            (data, label) å…ƒç»„ï¼Œå…¶ä¸­ï¼š
            - data: å½¢çŠ¶ä¸º (channels, height, width) çš„å¼ é‡
            - label: æ ‡ç­¾å¼ é‡
        """

        # è·å–æ ·æœ¬ä¿¡æ¯
        """
        iloc æ˜¯ pandas ä¸­ç”¨äº æŒ‰ä½ç½®ï¼ˆpositionï¼‰ç´¢å¼• è¡Œçš„æ–¹æ³•ã€‚
        iloc[idx] è¿”å›çš„æ˜¯ç¬¬ idx è¡Œçš„æ•°æ®ï¼ˆæŒ‰æ•°å­—ä½ç½®ï¼Œä¸æ˜¯æŒ‰ IDï¼‰ã€‚
        ç»“æœæ˜¯ä¸€ä¸ª pandas.Seriesï¼ŒåŒ…å«è¯¥è¡Œçš„æ‰€æœ‰åˆ—å†…å®¹ã€‚
        """
        row = self.data_index.iloc[idx]
        sample_id = row["ID"]

        try:
            # ä»ç¼“å­˜æˆ–æ–‡ä»¶åŠ è½½æ•°æ®
            if self.data_cache is not None and sample_id in self.data_cache:
                data = self.data_cache[sample_id]
            else:
                data = self._load_sample_data(sample_id)
                if self.data_cache is not None:
                    self.data_cache[sample_id] = data

            # å¤„ç†æ ‡ç­¾
            if self.has_labels:
                label = torch.tensor(row["label"], dtype=torch.long)
            else:
                label = torch.tensor(-1, dtype=torch.long)  # æµ‹è¯•é›†çš„å ä½ç¬¦æ ‡ç­¾

            # åº”ç”¨å˜æ¢
            if self.transform is not None:
                data = self.transform(data)

            return data, label

        except Exception as e:
            logger.error(f"Error loading sample {sample_id}: {e}")
            raise e

    def _load_sample_data(self, sample_id: str) -> torch.Tensor:
        """
        åŠ è½½å¹¶å¤„ç†å•ä¸ªæ ·æœ¬å¤šæ¨¡æ€çš„æ•°æ® - æ”¯æŒè·¨ç›®å½•è®¿é—®

        è¿™ä¸ªæ–¹æ³•å®ç°äº†æ‚¨åŸæœ‰æ•°æ®åŠ è½½é€»è¾‘çš„æ ¸å¿ƒéƒ¨åˆ†ï¼š
        1. æ£€æŸ¥è·¨ç›®å½•æ˜ å°„ï¼Œä¼˜å…ˆä½¿ç”¨æ˜ å°„è·¯å¾„
        2. åŠ è½½å¤šé€šé“.npyæ–‡ä»¶
        3. æå–æŒ‡å®šçš„å…‰å­¦é€šé“
        4. è®¡ç®—NDVIé€šé“
        5. ç»„åˆæˆæœ€ç»ˆçš„å¤šé€šé“æ•°æ®

        Args:
            sample_id: æ ·æœ¬ID

        Returns:
            å¤„ç†åçš„æ•°æ®å¼ é‡ï¼Œå½¢çŠ¶ä¸º (channels, height, width)
        """

        # ğŸ”§ æ ¸å¿ƒä¿®æ”¹ï¼šä¼˜å…ˆä½¿ç”¨è·¨ç›®å½•æ˜ å°„è·¯å¾„
        if sample_id in self.cross_directory_mapping:
            data_path = Path(self.cross_directory_mapping[sample_id])
            logger.debug(f"ğŸ”— Using cross-directory path for {sample_id}: {data_path}")
        else:
            # é»˜è®¤è·¯å¾„ï¼šåœ¨æ•°æ®ç›®å½•ä¸­æŸ¥æ‰¾
            data_path = self.data_dir / f"{sample_id}.npy"
            logger.debug(f"ğŸ“ Using default path for {sample_id}: {data_path}")

        if not data_path.exists():
            raise FileNotFoundError(f"Data file not found: {data_path}")

        # åŠ è½½åŸå§‹æ•°æ®
        raw_data = np.load(data_path)  # å½¢çŠ¶é€šå¸¸æ˜¯ (12, 64, 64)

        # ç¡®ä¿æ•°æ®å½¢çŠ¶ä¸º (channels, height, width)
        if raw_data.shape[-1] == 12:  # (64, 64, 12) â†’ (12, 64, 64)
            raw_data = np.transpose(raw_data, (2, 0, 1))

        # æ ¹æ®é…ç½®é€‰æ‹©é€šé“
        selected_channels = []

        for channel in self.active_channels:
            if channel == "ndvi" and self.compute_ndvi:
                # è®¡ç®—NDVI
                optical_channels = self.channel_config["channel_groups"]["optical"]
                red_idx, nir_idx = optical_channels[0], optical_channels[3]
                ndvi = self._compute_ndvi(raw_data[red_idx], raw_data[nir_idx])
                selected_channels.append(ndvi)
            else:
                selected_channels.append(raw_data[channel])

        # å †å æ‰€æœ‰é€šé“
        final_data = np.stack(selected_channels, axis=0)  # (channels, height, width)

        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        data_tensor = torch.from_numpy(final_data).float()

        return data_tensor

    def _compute_ndvi(self, red: np.ndarray, nir: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—å½’ä¸€åŒ–æ¤è¢«æŒ‡æ•° (NDVI)

        NDVIæ˜¯é¥æ„Ÿä¸­æœ€é‡è¦çš„æ¤è¢«æŒ‡æ•°ä¹‹ä¸€ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼š
        NDVI = (NIR - Red) / (NIR + Red)

        NDVIåœ¨æ»‘å¡æ£€æµ‹ä¸­çš„é‡è¦æ€§ï¼š
        1. æ»‘å¡åŒºåŸŸé€šå¸¸æ¤è¢«è¦†ç›–è¾ƒå°‘
        2. NDVIèƒ½å¤Ÿçªå‡ºæ¤è¢«ä¸è£¸åœŸçš„å·®å¼‚
        3. æ—¶é—´åºåˆ—NDVIå˜åŒ–èƒ½æŒ‡ç¤ºåœ°è¡¨æ‰°åŠ¨

        Args:
            red: çº¢å…‰é€šé“
            nir: è¿‘çº¢å¤–é€šé“

        Returns:
            NDVIæ•°ç»„ï¼Œå½¢çŠ¶ä¸è¾“å…¥é€šé“ç›¸åŒ
        """
        # è½¬æ¢æ•°æ®ç±»å‹
        red = red.astype(np.float32)
        nir = nir.astype(np.float32)

        # è®¡ç®—NDVIï¼Œæ·»åŠ å°å¸¸æ•°é¿å…é™¤é›¶
        epsilon = 1e-8
        ndvi = (nir - red) / (nir + red + epsilon)

        # å°†NDVIé™åˆ¶åœ¨åˆç†èŒƒå›´å†… [-1, 1]
        ndvi = np.clip(ndvi, -1.0, 1.0)

        return ndvi


# ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºä¸åŒé…ç½®çš„æ•°æ®é›†
def create_train_dataset(
    data_dir: str,
    csv_file: str,
    transform: Callable = None,
    channel_config: Optional[Dict] = None,
    usage_mode: str = "full_multimodal",
    cross_directory_mapping: Optional[Dict[str, str]] = None,
) -> MultiModalDataset:
    """
    åˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼ˆç®€åŒ–ç‰ˆï¼‰

    ç®€åŒ–è¯´æ˜ï¼š
    - ç§»é™¤äº† exclude_ids_file å‚æ•°
    - å‡è®¾ csv_file å·²ç»æ˜¯æ¸…æ´çš„æ•°æ®

    Args:
        data_dir: æ•°æ®ç›®å½•
        csv_file: æ¸…æ´çš„CSVæ–‡ä»¶è·¯å¾„
        transform: æ•°æ®å˜æ¢å‡½æ•°
        channel_config: é€šé“é…ç½®
        usage_mode: ä½¿ç”¨æ¨¡å¼
        cross_directory_mapping: è·¨ç›®å½•æ˜ å°„

    Returns:
        MultiModalDataset: è®­ç»ƒæ•°æ®é›†å®ä¾‹
    """
    # éªŒè¯usage_modeçš„æœ‰æ•ˆæ€§
    valid_modes = channel_config.get("usage_modes", {}).keys()
    if usage_mode not in valid_modes:
        raise ValueError(f"Invalid usage_mode: {usage_mode}. Valid options: {list(valid_modes)}")

    """åˆ›å»ºè®­ç»ƒæ•°æ®é›†"""
    return MultiModalDataset(
        data_dir=data_dir,
        csv_file=csv_file,
        transform=transform,
        compute_ndvi=True,
        cache_data=True,
        channel_config=channel_config,
        usage_mode=usage_mode,
        cross_directory_mapping=cross_directory_mapping,
    )


def create_test_dataset(
    data_dir: str,
    csv_file: str,
    transform: Callable = None,
    channel_config: Optional[Dict] = None,
    usage_mode: str = "optical_only",
) -> MultiModalDataset:
    """
    åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆç®€åŒ–ç‰ˆï¼‰

    Args:
        data_dir: æ•°æ®ç›®å½•
        csv_file: CSVæ–‡ä»¶è·¯å¾„
        transform: æ•°æ®å˜æ¢å‡½æ•°
        channel_config: é€šé“é…ç½®
        usage_mode: ä½¿ç”¨æ¨¡å¼

    Returns:
        MultiModalDataset: æµ‹è¯•æ•°æ®é›†å®ä¾‹
    """
    return MultiModalDataset(
        data_dir=data_dir,
        csv_file=csv_file,
        transform=transform,
        compute_ndvi=True,
        cache_data=True,
        channel_config=channel_config,
        usage_mode=usage_mode,
    )
