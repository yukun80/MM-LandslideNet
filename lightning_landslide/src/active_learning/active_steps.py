# =============================================================================
# lightning_landslide/src/active_learning/active_steps.py
# ä¸»åŠ¨å­¦ä¹ æ­¥éª¤æ¨¡å— - é‡æ„ç‰ˆ
# =============================================================================

"""
ä¸»åŠ¨å­¦ä¹ æ­¥éª¤åˆ†è§£å®ç°

âš ï¸  é‡è¦è¯´æ˜ï¼š
æœ¬æ¨¡å—åªå®ç°ä¸»åŠ¨å­¦ä¹ çš„æ­¥éª¤2-5ï¼Œä¸åŒ…å«ä»å¤´å¼€å§‹çš„æ¨¡å‹è®­ç»ƒã€‚
å¿…é¡»å…ˆé€šè¿‡ `python main.py train config.yaml` å®ŒæˆåŸºç¡€è®­ç»ƒï¼ˆæ­¥éª¤1ï¼‰ã€‚

éµå¾ªä¸‰ä¸ªåŸåˆ™ï¼š
1. æœ€å°æ”¹åŠ¨åŸåˆ™ï¼šé‡ç”¨ç°æœ‰è®­ç»ƒæµç¨‹å’Œæ•°æ®æ¨¡å—
2. å•ä¸€èŒè´£åŸåˆ™ï¼šæ¯ä¸ªç±»åªè´Ÿè´£ä¸€ä¸ªç‰¹å®šæ­¥éª¤
3. æ¸è¿›å¢å¼ºåŸåˆ™ï¼šåœ¨ç°æœ‰ä»£ç åŸºç¡€ä¸Šæ·»åŠ åŠŸèƒ½

æ–°çš„å‘½ä»¤ä½“ç³»ï¼š
- æ­¥éª¤1ï¼špython main.py train config.yaml                    # åŸºç¡€è®­ç»ƒï¼ˆå¿…é¡»å…ˆæ‰§è¡Œï¼‰
- æ­¥éª¤2ï¼špython main.py uncertainty_estimation config.yaml   # ä¸ç¡®å®šæ€§ä¼°è®¡
- æ­¥éª¤3ï¼špython main.py sample_selection config.yaml         # æ ·æœ¬é€‰æ‹©
- æ­¥éª¤4ï¼šäººå·¥æ ‡æ³¨ï¼ˆç¦»çº¿å®Œæˆï¼‰
- æ­¥éª¤5ï¼špython main.py retrain config.yaml                  # æ¨¡å‹fine-tuning

æ ¸å¿ƒè®¾è®¡ï¼š
- æ‰€æœ‰æ­¥éª¤éƒ½åŸºäºæ­¥éª¤1äº§ç”Ÿçš„æ£€æŸ¥ç‚¹æ–‡ä»¶
- æ­¥éª¤5æ˜¯fine-tuningï¼Œä¸æ˜¯ä»å¤´è®­ç»ƒ
- æ¯ä¸ªæ­¥éª¤éƒ½æœ‰å®Œæ•´çš„çŠ¶æ€ç®¡ç†å’Œé”™è¯¯æ¢å¤
"""

import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
import logging
import json
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import time
from dataclasses import dataclass
import pickle
from abc import ABC, abstractmethod

import pytorch_lightning as pl
from torch.utils.data import DataLoader

from ..utils.instantiate import instantiate_from_config

logger = logging.getLogger(__name__)


@dataclass
class ActiveLearningState:
    """ä¸»åŠ¨å­¦ä¹ çŠ¶æ€ç®¡ç†"""

    # åŸºæœ¬ä¿¡æ¯
    experiment_name: str
    checkpoint_path: str
    iteration: int = 0

    # æ•°æ®ä¿¡æ¯
    unlabeled_pool: List[str] = None  # æœªæ ‡æ³¨æ ·æœ¬IDåˆ—è¡¨
    labeled_samples: List[str] = None  # å·²æ ‡æ³¨æ ·æœ¬IDåˆ—è¡¨
    annotation_history: List[Dict] = None  # æ ‡æ³¨å†å²

    # ç»“æœä¿¡æ¯
    uncertainty_scores: Dict[str, float] = None  # ä¸ç¡®å®šæ€§åˆ†æ•°
    selected_samples: List[str] = None  # å½“å‰é€‰ä¸­çš„æ ·æœ¬

    def __post_init__(self):
        if self.unlabeled_pool is None:
            self.unlabeled_pool = []
        if self.labeled_samples is None:
            self.labeled_samples = []
        if self.annotation_history is None:
            self.annotation_history = []

    def save(self, save_path: Path):
        """ä¿å­˜çŠ¶æ€åˆ°æ–‡ä»¶"""
        save_path.parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, "wb") as f:
            pickle.dump(self, f)
        logger.info(f"Active learning state saved to: {save_path}")

    @classmethod
    def load(cls, load_path: Path):
        """ä»æ–‡ä»¶åŠ è½½çŠ¶æ€"""
        if not load_path.exists():
            raise FileNotFoundError(f"State file not found: {load_path}")

        with open(load_path, "rb") as f:
            state = pickle.load(f)
        logger.info(f"Active learning state loaded from: {load_path}")
        return state

    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, experiment_name: str):
        """ä»æ£€æŸ¥ç‚¹åˆ›å»ºåˆå§‹çŠ¶æ€"""
        return cls(experiment_name=experiment_name, checkpoint_path=checkpoint_path, iteration=0)


class BaseActiveStep(ABC):
    """ä¸»åŠ¨å­¦ä¹ æ­¥éª¤åŸºç±»"""

    def __init__(self, config: Dict[str, Any], state_path: Optional[str] = None):
        self.config = config
        self.state_path = Path(state_path) if state_path else None
        self.state = self._load_or_create_state()

        # è®¾ç½®è¾“å‡ºç›®å½•
        self.output_dir = Path(config.get("outputs", {}).get("experiment_dir", "outputs"))
        self.active_dir = self.output_dir / "active_learning"
        self.active_dir.mkdir(parents=True, exist_ok=True)

    def _load_or_create_state(self) -> ActiveLearningState:
        """åŠ è½½æˆ–åˆ›å»ºçŠ¶æ€"""
        if self.state_path and self.state_path.exists():
            return ActiveLearningState.load(self.state_path)
        else:
            # ä»é…ç½®åˆ›å»ºåˆå§‹çŠ¶æ€
            experiment_name = self.config.get("experiment_name", "active_learning")
            checkpoint_path = self._find_best_checkpoint()
            return ActiveLearningState.from_checkpoint(checkpoint_path, experiment_name)

    def _find_best_checkpoint(self) -> str:
        """å¯»æ‰¾æœ€ä½³æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        # ä»é…ç½®æˆ–é»˜è®¤ä½ç½®å¯»æ‰¾æ£€æŸ¥ç‚¹
        checkpoint_path = self.config.get("checkpoint_path")
        if checkpoint_path and Path(checkpoint_path).exists():
            return checkpoint_path

        # åœ¨å®éªŒç›®å½•ä¸‹å¯»æ‰¾æœ€ä½³æ£€æŸ¥ç‚¹
        exp_dir = Path(self.config.get("outputs", {}).get("experiment_dir", "outputs"))
        checkpoint_dir = exp_dir / "checkpoints"

        if checkpoint_dir.exists():
            # å¯»æ‰¾bestå¼€å¤´çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
            best_ckpts = list(checkpoint_dir.glob("baseline_epoch*.ckpt"))
            if best_ckpts:
                # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
                latest_ckpt = max(best_ckpts, key=lambda x: x.stat().st_mtime)
                logger.info(f"Found checkpoint: {latest_ckpt}")
                return str(latest_ckpt)

        raise FileNotFoundError("No valid checkpoint found. Please run baseline training first.")

    def save_state(self):
        """ä¿å­˜å½“å‰çŠ¶æ€"""
        state_file = self.active_dir / f"state_iter_{self.state.iteration}.pkl"
        self.state.save(state_file)

    @abstractmethod
    def run(self) -> Dict[str, Any]:
        """è¿è¡Œå½“å‰æ­¥éª¤"""
        pass


class UncertaintyEstimator(BaseActiveStep):
    """æ­¥éª¤2ï¼šä¸ç¡®å®šæ€§ä¼°è®¡"""

    def __init__(self, config: Dict[str, Any], state_path: Optional[str] = None):
        super().__init__(config, state_path)

        # ä¸ç¡®å®šæ€§ä¼°è®¡é…ç½®
        uncertainty_config = config.get("active_pseudo_learning", {}).get("uncertainty_estimation", {})
        self.method = uncertainty_config.get("method", "mc_dropout")
        self.n_forward_passes = uncertainty_config.get("params", {}).get("n_forward_passes", 10)

        logger.info(f"ğŸ” UncertaintyEstimator initialized")
        logger.info(f"ğŸ“Š Method: {self.method}, Forward passes: {self.n_forward_passes}")

    def run(self) -> Dict[str, Any]:
        """è¿è¡Œä¸ç¡®å®šæ€§ä¼°è®¡"""
        logger.info("ğŸ” Starting uncertainty estimation...")

        # 1. åŠ è½½æ¨¡å‹
        model = self._load_model()

        # 2. è·å–æœªæ ‡æ³¨æ•°æ®
        datamodule = self._setup_datamodule()

        # 3. ä¼°è®¡ä¸ç¡®å®šæ€§
        uncertainty_scores = self._estimate_uncertainty(model, datamodule)

        # 4. æ›´æ–°çŠ¶æ€
        self.state.uncertainty_scores = uncertainty_scores
        self.save_state()

        # 5. ä¿å­˜ç»“æœ
        results_path = self.active_dir / f"uncertainty_scores_iter_{self.state.iteration}.json"
        with open(results_path, "w") as f:
            json.dump(uncertainty_scores, f, indent=2)

        logger.info(f"âœ… Uncertainty estimation completed")
        logger.info(f"ğŸ“ Results saved to: {results_path}")
        logger.info(f"ğŸ“Š Estimated uncertainty for {len(uncertainty_scores)} samples")

        return {
            "uncertainty_scores": uncertainty_scores,
            "results_path": str(results_path),
            "num_samples": len(uncertainty_scores),
        }

    def _load_model(self) -> pl.LightningModule:
        """åŠ è½½æ¨¡å‹"""
        logger.info(f"ğŸ“¥ Loading model from: {self.state.checkpoint_path}")

        # é‡ç”¨ç°æœ‰çš„æ¨¡å‹å®ä¾‹åŒ–é€»è¾‘
        model = instantiate_from_config(self.config["model"])

        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(self.state.checkpoint_path, map_location="cpu")
        model.load_state_dict(checkpoint["state_dict"])
        model.eval()

        # è®¾ç½®dropoutä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆç”¨äºMC Dropoutï¼‰
        if self.method == "mc_dropout":
            for module in model.modules():
                if isinstance(module, torch.nn.Dropout):
                    module.train()

        return model

    def _setup_datamodule(self):
        """è®¾ç½®æ•°æ®æ¨¡å—"""
        # é‡ç”¨ç°æœ‰çš„æ•°æ®æ¨¡å—
        datamodule = instantiate_from_config(self.config["data"])
        datamodule.setup("test")  # ä½¿ç”¨æµ‹è¯•é›†ä½œä¸ºæœªæ ‡æ³¨æ± 
        return datamodule

    def _estimate_uncertainty(self, model: pl.LightningModule, datamodule) -> Dict[str, float]:
        """
        ğŸ”¥ ä¿®å¤ç‰ˆæœ¬ï¼šä½¿ç”¨çœŸå®sample IDçš„ä¸ç¡®å®šæ€§ä¼°è®¡

        å…³é”®ä¿®å¤ï¼š
        1. ç›´æ¥ä»æ•°æ®é›†è·å–çœŸå®sample ID
        2. æ›¿æ¢è™šæ‹ŸIDç”Ÿæˆé€»è¾‘
        3. ç¡®ä¿ç»“æœå¯è¿½æº¯åˆ°åŸå§‹æ•°æ®
        """
        logger.info(f"ğŸ”„ Running {self.method} with {self.n_forward_passes} forward passes...")
        logger.info("ğŸ” FIXED VERSION: Using real sample IDs from dataset")

        model.eval()
        device = next(model.parameters()).device
        test_loader = datamodule.test_dataloader()
        uncertainty_scores = {}

        # æ¿€æ´»dropoutç”¨äºMC Dropout
        if self.method == "mc_dropout":
            for module in model.modules():
                if isinstance(module, torch.nn.Dropout):
                    module.train()

        total_batches = len(test_loader)
        logger.info(f"ğŸ“Š Total batches to process: {total_batches}")

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šè·å–æ•°æ®é›†ä»¥è®¿é—®çœŸå®ID
        test_dataset = datamodule.test_dataset

        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                try:
                    # è§£æbatchæ•°æ®
                    if isinstance(batch, list) and len(batch) >= 2:
                        images, labels = batch[0], batch[1]
                        batch_size = images.size(0)
                    else:
                        logger.warning(f"âš ï¸ Unexpected batch format at {batch_idx}")
                        continue

                    # ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šè®¡ç®—çœŸå®çš„æ•°æ®é›†ç´¢å¼•å¹¶è·å–çœŸå®ID
                    batch_start_idx = batch_idx * test_loader.batch_size
                    real_sample_ids = []

                    for i in range(batch_size):
                        dataset_idx = batch_start_idx + i
                        if dataset_idx < len(test_dataset):
                            # ç›´æ¥ä»æ•°æ®é›†è·å–çœŸå®ID
                            real_id = test_dataset.data_index.iloc[dataset_idx]["ID"]
                            real_sample_ids.append(real_id)
                        else:
                            # å®‰å…¨å›é€€
                            real_sample_ids.append(f"sample_{batch_idx}_{i}")

                    # éªŒè¯è·å–çš„çœŸå®ID
                    if batch_idx == 0:  # åªåœ¨ç¬¬ä¸€æ‰¹æ˜¾ç¤ºç¤ºä¾‹
                        logger.info(f"âœ… Real IDs example: {real_sample_ids[:3]}")
                        logger.info(f"ğŸ” ID format validation: {real_sample_ids[0].startswith('ID_')}")

                    images = images.to(device)
                    batch_predictions = []

                    # MC Dropoutå‰å‘ä¼ æ’­
                    for pass_idx in range(self.n_forward_passes):
                        output = model(images)
                        if output.dim() == 2 and output.size(1) == 1:
                            output = output.squeeze(1)

                        probs = torch.sigmoid(output)
                        batch_predictions.append(probs.cpu().numpy())

                    # è®¡ç®—ä¸ç¡®å®šæ€§
                    batch_predictions = np.array(batch_predictions)  # [n_passes, batch_size]

                    for i, real_id in enumerate(real_sample_ids):
                        try:
                            sample_pred = batch_predictions[:, i]  # [n_passes]

                            # è®¡ç®—æ–¹å·®å’Œç†µ
                            mean_pred = np.mean(sample_pred)
                            variance = np.var(sample_pred)

                            # äºŒåˆ†ç±»ç†µ
                            p = np.clip(mean_pred, 1e-8, 1 - 1e-8)
                            entropy = -(p * np.log(p) + (1 - p) * np.log(1 - p))

                            # ç»„åˆä¸ç¡®å®šæ€§
                            uncertainty = variance + 0.1 * entropy
                            uncertainty_scores[str(real_id)] = float(uncertainty)

                        except Exception as e:
                            logger.error(f"âŒ Error computing uncertainty for sample {real_id}: {e}")
                            continue

                    # ğŸ”¥ æ”¹è¿›çš„è¿›åº¦æŠ¥å‘Š
                    if (batch_idx + 1) % 10 == 0:
                        progress_pct = (batch_idx + 1) / total_batches * 100
                        samples_processed = len(uncertainty_scores)

                        # ç»Ÿè®¡çœŸå®IDæ•°é‡
                        real_id_count = sum(1 for k in uncertainty_scores.keys() if k.startswith("ID_"))
                        fake_id_count = samples_processed - real_id_count

                        logger.info(f"ğŸ”„ Progress: {progress_pct:.1f}% ({batch_idx+1}/{total_batches} batches)")
                        logger.info(
                            f"ğŸ“Š Processed: {samples_processed} samples ({real_id_count} real IDs, {fake_id_count} fallback IDs)"
                        )

                    # å†…å­˜æ¸…ç†
                    if batch_idx % 50 == 0:
                        torch.cuda.empty_cache() if torch.cuda.is_available() else None

                except Exception as e:
                    logger.error(f"âŒ Error processing batch {batch_idx}: {e}")
                    continue

        # æœ€ç»ˆéªŒè¯
        if len(uncertainty_scores) == 0:
            raise ValueError("No samples processed successfully")

        # ğŸ”¥ ç»“æœéªŒè¯å’Œç»Ÿè®¡
        total_samples = len(uncertainty_scores)
        real_id_samples = sum(1 for k in uncertainty_scores.keys() if k.startswith("ID_"))
        fallback_samples = total_samples - real_id_samples

        logger.info(f"ğŸ“Š Final statistics:")
        logger.info(f"ğŸ“Š - Total samples processed: {total_samples}")
        logger.info(f"ğŸ“Š - Real IDs: {real_id_samples} ({real_id_samples/total_samples*100:.1f}%)")
        logger.info(f"ğŸ“Š - Fallback IDs: {fallback_samples} ({fallback_samples/total_samples*100:.1f}%)")

        # æ˜¾ç¤ºçœŸå®IDç¤ºä¾‹
        real_ids = [k for k in uncertainty_scores.keys() if k.startswith("ID_")][:5]
        if real_ids:
            logger.info(f"ğŸ“ Real ID examples: {real_ids}")
        else:
            logger.warning("âš ï¸ No real IDs found! All samples using fallback IDs.")

        logger.info(f"ğŸ“Š Successfully processed {len(uncertainty_scores)} samples")
        return uncertainty_scores


class SampleSelector(BaseActiveStep):
    """æ­¥éª¤3ï¼šæ ·æœ¬é€‰æ‹©"""

    def __init__(self, config: Dict[str, Any], state_path: Optional[str] = None):
        super().__init__(config, state_path)

        # æ ·æœ¬é€‰æ‹©é…ç½®
        active_config = config.get("active_pseudo_learning", {})
        self.annotation_budget = active_config.get("annotation_budget", 50)

        selection_config = active_config.get("active_learning", {})
        self.strategies = selection_config.get(
            "strategies", {"uncertainty": 0.6, "diversity": 0.3, "cluster_based": 0.1}
        )

        logger.info(f"ğŸ¯ SampleSelector initialized")
        logger.info(f"ğŸ“ Budget: {self.annotation_budget}")
        logger.info(f"ğŸ² Strategies: {self.strategies}")

    def run(self) -> Dict[str, Any]:
        """è¿è¡Œæ ·æœ¬é€‰æ‹©"""
        logger.info("ğŸ¯ Starting sample selection...")

        # 1. æ£€æŸ¥æ˜¯å¦æœ‰ä¸ç¡®å®šæ€§åˆ†æ•°
        if not self.state.uncertainty_scores:
            # å°è¯•ä»æ–‡ä»¶åŠ è½½
            uncertainty_file = self.active_dir / f"uncertainty_scores_iter_{self.state.iteration}.json"
            if uncertainty_file.exists():
                with open(uncertainty_file, "r") as f:
                    self.state.uncertainty_scores = json.load(f)
                logger.info(f"ğŸ“¥ Loaded uncertainty scores from: {uncertainty_file}")
            else:
                raise ValueError("No uncertainty scores found. Please run uncertainty estimation first.")

        # 2. é€‰æ‹©æ ·æœ¬
        selected_samples = self._select_samples()

        # 3. æ›´æ–°çŠ¶æ€
        self.state.selected_samples = selected_samples
        self.save_state()

        # 4. ç”Ÿæˆæ ‡æ³¨è¯·æ±‚æ–‡ä»¶
        annotation_file = self._generate_annotation_request(selected_samples)

        logger.info(f"âœ… Sample selection completed")
        logger.info(f"ğŸ“ Selected {len(selected_samples)} samples for annotation")
        logger.info(f"ğŸ“ Annotation request saved to: {annotation_file}")

        return {
            "selected_samples": selected_samples,
            "annotation_file": str(annotation_file),
            "num_selected": len(selected_samples),
        }

    def _select_samples(self) -> List[str]:
        """é€‰æ‹©æ ·æœ¬è¿›è¡Œæ ‡æ³¨"""
        uncertainty_scores = self.state.uncertainty_scores

        # è¿‡æ»¤å·²æ ‡æ³¨çš„æ ·æœ¬
        available_samples = {
            sample_id: score
            for sample_id, score in uncertainty_scores.items()
            if sample_id not in self.state.labeled_samples
        }

        if len(available_samples) < self.annotation_budget:
            logger.warning(f"Available samples ({len(available_samples)}) < budget ({self.annotation_budget})")
            return list(available_samples.keys())

        # åŸºäºä¸ç¡®å®šæ€§é€‰æ‹©
        uncertainty_weight = self.strategies.get("uncertainty", 1.0)
        if uncertainty_weight > 0:
            # é€‰æ‹©ä¸ç¡®å®šæ€§æœ€é«˜çš„æ ·æœ¬
            sorted_samples = sorted(available_samples.items(), key=lambda x: x[1], reverse=True)
            n_uncertainty = int(self.annotation_budget * uncertainty_weight)
            selected_by_uncertainty = [sample_id for sample_id, _ in sorted_samples[:n_uncertainty]]
        else:
            selected_by_uncertainty = []

        # éšæœºé€‰æ‹©å‰©ä½™æ ·æœ¬ï¼ˆç®€åŒ–çš„å¤šæ ·æ€§ç­–ç•¥ï¼‰
        remaining_budget = self.annotation_budget - len(selected_by_uncertainty)
        if remaining_budget > 0:
            remaining_samples = [
                sample_id for sample_id in available_samples.keys() if sample_id not in selected_by_uncertainty
            ]

            if len(remaining_samples) <= remaining_budget:
                selected_random = remaining_samples
            else:
                import random

                random.seed(42)
                selected_random = random.sample(remaining_samples, remaining_budget)
        else:
            selected_random = []

        selected_samples = selected_by_uncertainty + selected_random

        logger.info(f"ğŸ“Š Selection breakdown:")
        logger.info(f"  ğŸ¯ By uncertainty: {len(selected_by_uncertainty)}")
        logger.info(f"  ğŸ² Random/diversity: {len(selected_random)}")

        return selected_samples

    def _generate_annotation_request(self, selected_samples: List[str]) -> Path:
        """ç”Ÿæˆæ ‡æ³¨è¯·æ±‚æ–‡ä»¶"""
        annotation_request = {
            "iteration": self.state.iteration,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "experiment_name": self.state.experiment_name,
            "selected_samples": selected_samples,
            "annotation_budget": self.annotation_budget,
            "instructions": {
                "task": "è¯·ä¸ºé€‰ä¸­çš„æ ·æœ¬æ ‡æ³¨æ˜¯å¦ä¸ºæ»‘å¡åŒºåŸŸ",
                "labels": {"0": "éæ»‘å¡åŒºåŸŸ", "1": "æ»‘å¡åŒºåŸŸ"},
                "format": "è¯·åœ¨annotation_results.jsonä¸­æä¾›æ ‡æ³¨ç»“æœ",
            },
            "sample_details": [],
        }

        # æ·»åŠ æ ·æœ¬è¯¦ç»†ä¿¡æ¯
        for sample_id in selected_samples:
            uncertainty = self.state.uncertainty_scores.get(sample_id, 0.0)
            sample_info = {
                "sample_id": sample_id,
                "uncertainty_score": uncertainty,
                "image_path": f"dataset/test_data/{sample_id}",  # æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´
                "label": None,  # å¾…æ ‡æ³¨
                "confidence": None,  # å¯é€‰ï¼šæ ‡æ³¨è€…ä¿¡å¿ƒ
            }
            annotation_request["sample_details"].append(sample_info)

        # ä¿å­˜æ ‡æ³¨è¯·æ±‚
        request_file = self.active_dir / f"annotation_request_iter_{self.state.iteration}.json"
        with open(request_file, "w", encoding="utf-8") as f:
            json.dump(annotation_request, f, indent=2, ensure_ascii=False)

        return request_file


class ActiveRetrainer(BaseActiveStep):
    """æ­¥éª¤5ï¼šæ¨¡å‹fine-tuningï¼ˆåŸºäºå·²æœ‰æ£€æŸ¥ç‚¹ï¼Œä¸æ˜¯ä»å¤´è®­ç»ƒï¼‰"""

    def __init__(self, config: Dict[str, Any], state_path: Optional[str] = None, annotation_file: Optional[str] = None):
        super().__init__(config, state_path)
        self.annotation_file = annotation_file

        # é‡è®­ç»ƒé…ç½®
        active_config = config.get("active_pseudo_learning", {})
        self.pseudo_config = active_config.get("pseudo_labeling", {})
        self.confidence_threshold = self.pseudo_config.get("confidence_threshold", 0.85)

        logger.info(f"ğŸ”„ ActiveRetrainer initialized (Fine-tuning mode)")
        logger.info(f"ğŸ“¥ Will load from checkpoint: {self.state.checkpoint_path}")
        logger.info(f"ğŸ·ï¸ Pseudo label threshold: {self.confidence_threshold}")
        logger.info(f"âš ï¸  Note: This will fine-tune existing model, NOT train from scratch")

    def run(self) -> Dict[str, Any]:
        """è¿è¡Œæ¨¡å‹é‡è®­ç»ƒï¼ˆæ­¥éª¤5ï¼šåŸºäºå·²æœ‰æ£€æŸ¥ç‚¹è¿›è¡Œfine-tuningï¼‰"""
        logger.info("ğŸ”„ Starting model fine-tuning with new annotations...")
        logger.info("âš ï¸  Note: This is fine-tuning from existing checkpoint, NOT training from scratch")

        # 1. åŠ è½½æ ‡æ³¨ç»“æœ
        annotations = self._load_annotations()

        # 2. ç”Ÿæˆä¼ªæ ‡ç­¾ï¼ˆåŸºäºå½“å‰æ¨¡å‹ï¼‰
        pseudo_labels = self._generate_pseudo_labels()

        # 3. æ›´æ–°è®­ç»ƒæ•°æ®
        updated_datamodule = self._update_training_data(annotations, pseudo_labels)

        # 4. Fine-tuneæ¨¡å‹ï¼ˆåŸºäºå·²æœ‰æ£€æŸ¥ç‚¹ï¼‰
        new_model, training_results = self._retrain_model(updated_datamodule)

        # 5. æ›´æ–°çŠ¶æ€
        self.state.annotation_history.append(
            {
                "iteration": self.state.iteration,
                "annotations": annotations,
                "num_pseudo_labels": len(pseudo_labels),
                "training_results": training_results,
                "fine_tuning": True,  # æ ‡è®°è¿™æ˜¯fine-tuning
            }
        )

        # æ›´æ–°è¿­ä»£è®¡æ•°
        old_iteration = self.state.iteration
        self.state.iteration += 1
        self.save_state()

        logger.info(f"âœ… Model fine-tuning completed (iteration {old_iteration} -> {self.state.iteration})")
        logger.info(f"ğŸ“Š Added {len(annotations)} human annotations")
        logger.info(f"ğŸ·ï¸ Generated {len(pseudo_labels)} pseudo labels")

        return {
            "num_annotations": len(annotations),
            "num_pseudo_labels": len(pseudo_labels),
            "training_results": training_results,
            "new_checkpoint": training_results.get("best_checkpoint"),
            "iteration": self.state.iteration,
            "fine_tuning": True,
        }

    def _load_annotations(self) -> List[Dict]:
        """åŠ è½½äººå·¥æ ‡æ³¨ç»“æœ"""
        if self.annotation_file:
            annotation_path = Path(self.annotation_file)
        else:
            # å¯»æ‰¾æœ€æ–°çš„æ ‡æ³¨æ–‡ä»¶
            annotation_path = self.active_dir / f"annotation_results_iter_{self.state.iteration}.json"

        if not annotation_path.exists():
            raise FileNotFoundError(f"Annotation file not found: {annotation_path}")

        with open(annotation_path, "r", encoding="utf-8") as f:
            annotation_data = json.load(f)

        # è§£ææ ‡æ³¨ç»“æœ
        annotations = []
        for sample_info in annotation_data.get("sample_details", []):
            if sample_info.get("label") is not None:
                annotations.append(
                    {
                        "sample_id": sample_info["sample_id"],
                        "label": sample_info["label"],
                        "confidence": sample_info.get("confidence", 1.0),
                    }
                )

        logger.info(f"ğŸ“¥ Loaded {len(annotations)} annotations from: {annotation_path}")
        return annotations

    def _generate_pseudo_labels(self) -> List[Dict]:
        """ç”Ÿæˆä¼ªæ ‡ç­¾"""
        logger.info("ğŸ·ï¸ Generating pseudo labels...")

        # ç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºç½®ä¿¡åº¦é˜ˆå€¼ç”Ÿæˆä¼ªæ ‡ç­¾
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æœªæ ‡æ³¨æ•°æ®è¿›è¡Œé¢„æµ‹

        # è¿™é‡Œè¿”å›ç©ºåˆ—è¡¨ï¼Œå®é™…å®ç°æ—¶éœ€è¦ï¼š
        # 1. åŠ è½½å½“å‰æœ€ä½³æ¨¡å‹
        # 2. å¯¹æœªæ ‡æ³¨æ•°æ®è¿›è¡Œé¢„æµ‹
        # 3. é€‰æ‹©é«˜ç½®ä¿¡åº¦çš„é¢„æµ‹ä½œä¸ºä¼ªæ ‡ç­¾

        pseudo_labels = []
        logger.info(f"ğŸ·ï¸ Generated {len(pseudo_labels)} pseudo labels")
        return pseudo_labels

    def _update_training_data(self, annotations: List[Dict], pseudo_labels: List[Dict]):
        """æ›´æ–°è®­ç»ƒæ•°æ®"""
        logger.info("ğŸ“Š Updating training data...")

        # é‡ç”¨ç°æœ‰çš„æ•°æ®æ¨¡å—
        datamodule = instantiate_from_config(self.config["data"])

        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œéœ€è¦ï¼š
        # 1. å°†æ–°æ ‡æ³¨çš„æ•°æ®æ·»åŠ åˆ°è®­ç»ƒé›†
        # 2. å°†ä¼ªæ ‡ç­¾æ·»åŠ åˆ°è®­ç»ƒé›†
        # 3. æ›´æ–°æ•°æ®æ¨¡å—çš„è®­ç»ƒé›†

        logger.info(
            f"ğŸ“Š Training data updated with {len(annotations)} annotations and {len(pseudo_labels)} pseudo labels"
        )
        return datamodule

    def _retrain_model(self, datamodule) -> Tuple[pl.LightningModule, Dict[str, Any]]:
        """é‡è®­ç»ƒæ¨¡å‹ï¼ˆåŸºäºå·²æœ‰æ£€æŸ¥ç‚¹è¿›è¡Œfine-tuningï¼‰"""
        logger.info("ğŸ”„ Fine-tuning model from existing checkpoint...")

        # é‡ç”¨ç°æœ‰çš„æ¨¡å‹é…ç½®
        model = instantiate_from_config(self.config["model"])

        # é…ç½®é‡è®­ç»ƒçš„è®­ç»ƒå™¨ï¼ˆä½¿ç”¨è¾ƒå°‘epochï¼‰
        trainer_config = self.config.get("trainer", {}).get("params", {}).copy()
        trainer_config["max_epochs"] = 20  # é‡è®­ç»ƒä½¿ç”¨è¾ƒå°‘epoch
        trainer_config["enable_model_summary"] = False  # é‡è®­ç»ƒæ—¶ä¸æ˜¾ç¤ºæ¨¡å‹æ‘˜è¦

        # æ·»åŠ é‡è®­ç»ƒä¸“ç”¨çš„å›è°ƒ
        from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping

        callbacks = [
            ModelCheckpoint(
                dirpath=str(self.active_dir / "retrain_checkpoints"),
                filename=f"retrain_iter_{self.state.iteration}_{{epoch:02d}}_{{val_f1:.4f}}",
                monitor="val_f1",
                mode="max",
                save_top_k=1,
                verbose=False,
            ),
            EarlyStopping(monitor="val_f1", patience=5, mode="max", verbose=False),  # é‡è®­ç»ƒæ—¶æ›´å¿«æ—©åœ
        ]

        trainer_config["callbacks"] = callbacks
        trainer = pl.Trainer(**trainer_config)

        # ğŸ”¥ å…³é”®ï¼šä»å·²æœ‰æ£€æŸ¥ç‚¹å¼€å§‹fine-tuningï¼Œè€Œä¸æ˜¯ä»å¤´è®­ç»ƒ
        logger.info(f"ğŸ“¥ Loading from checkpoint: {self.state.checkpoint_path}")
        trainer.fit(model, datamodule, ckpt_path=self.state.checkpoint_path)

        # æ›´æ–°çŠ¶æ€ä¸­çš„æ£€æŸ¥ç‚¹è·¯å¾„ä¸ºæ–°çš„æœ€ä½³æ¨¡å‹
        new_checkpoint = callbacks[0].best_model_path
        if new_checkpoint:
            self.state.checkpoint_path = new_checkpoint
            logger.info(f"ğŸ“¤ Updated checkpoint path: {new_checkpoint}")

        # è¿”å›è®­ç»ƒç»“æœ
        training_results = {
            "best_checkpoint": new_checkpoint,
            "previous_checkpoint": self.state.checkpoint_path,
            "retrain_epochs": trainer_config["max_epochs"],
            "final_metrics": {},  # å¯ä»¥æ·»åŠ æœ€ç»ˆéªŒè¯æŒ‡æ ‡
        }

        logger.info("âœ… Model fine-tuning completed")
        return model, training_results


# =============================================================================
# æ­¥éª¤ç®¡ç†å™¨
# =============================================================================


class ActiveLearningStepManager:
    """ä¸»åŠ¨å­¦ä¹ æ­¥éª¤ç®¡ç†å™¨"""

    @staticmethod
    def run_uncertainty_estimation(config: Dict[str, Any], state_path: Optional[str] = None) -> Dict[str, Any]:
        """è¿è¡Œä¸ç¡®å®šæ€§ä¼°è®¡æ­¥éª¤"""
        estimator = UncertaintyEstimator(config, state_path)
        return estimator.run()

    @staticmethod
    def run_sample_selection(config: Dict[str, Any], state_path: Optional[str] = None) -> Dict[str, Any]:
        """è¿è¡Œæ ·æœ¬é€‰æ‹©æ­¥éª¤"""
        selector = SampleSelector(config, state_path)
        return selector.run()

    @staticmethod
    def run_retraining(
        config: Dict[str, Any], state_path: Optional[str] = None, annotation_file: Optional[str] = None
    ) -> Dict[str, Any]:
        """è¿è¡Œæ¨¡å‹é‡è®­ç»ƒæ­¥éª¤"""
        retrainer = ActiveRetrainer(config, state_path, annotation_file)
        return retrainer.run()
