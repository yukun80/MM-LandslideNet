# =============================================================================
# lightning_landslide/src/active_learning/active_steps.py
# ä¸»åŠ¨å­¦ä¹ æ­¥éª¤æ¨¡å— - é‡æ„ç‰ˆ
# =============================================================================

"""
ä¸»åŠ¨å­¦ä¹ æ­¥éª¤åˆ†è§£å®ç°

âš ï¸  é‡è¦è¯´æ˜ï¼š
æœ¬æ¨¡å—åªå®ç°ä¸»åŠ¨å­¦ä¹ çš„æ­¥éª¤2-5ï¼Œä¸åŒ…å«ä»å¤´å¼€å§‹çš„æ¨¡å‹è®­ç»ƒã€‚
å¿…é¡»å…ˆé€šè¿‡ `python main.py train config.yaml` å®ŒæˆåŸºç¡€è®­ç»ƒï¼ˆæ­¥éª¤1ï¼‰ã€‚

éµå¾ªä¸‰ä¸ªåŸåˆ™ï¼š
1. æœ€å°æ”¹åŠ¨åŸåˆ™ï¼šé‡ç”¨ç°æœ‰è®­ç»ƒæµç¨‹å’Œæ•°æ®æ¨¡å—
2. å•ä¸€èŒè´£åŸåˆ™ï¼šæ¯ä¸ªç±»åªè´Ÿè´£ä¸€ä¸ªç‰¹å®šæ­¥éª¤
3. æ¸è¿›å¢å¼ºåŸåˆ™ï¼šåœ¨ç°æœ‰ä»£ç åŸºç¡€ä¸Šæ·»åŠ åŠŸèƒ½

æ–°çš„å‘½ä»¤ä½“ç³»ï¼š
- æ­¥éª¤1ï¼špython main.py train config.yaml                    # åŸºç¡€è®­ç»ƒï¼ˆå¿…é¡»å…ˆæ‰§è¡Œï¼‰
- æ­¥éª¤2ï¼špython main.py uncertainty_estimation config.yaml   # ä¸ç¡®å®šæ€§ä¼°è®¡
- æ­¥éª¤3ï¼špython main.py sample_selection config.yaml         # æ ·æœ¬é€‰æ‹©
- æ­¥éª¤4ï¼šäººå·¥æ ‡æ³¨ï¼ˆç¦»çº¿å®Œæˆï¼‰
- æ­¥éª¤5ï¼špython main.py retrain config.yaml                  # æ¨¡å‹fine-tuning

æ ¸å¿ƒè®¾è®¡ï¼š
- æ‰€æœ‰æ­¥éª¤éƒ½åŸºäºæ­¥éª¤1äº§ç”Ÿçš„æ£€æŸ¥ç‚¹æ–‡ä»¶
- æ­¥éª¤5æ˜¯fine-tuningï¼Œä¸æ˜¯ä»å¤´è®­ç»ƒ
- æ¯ä¸ªæ­¥éª¤éƒ½æœ‰å®Œæ•´çš„çŠ¶æ€ç®¡ç†å’Œé”™è¯¯æ¢å¤
"""

import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
import logging
import json
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import time
from dataclasses import dataclass
import pickle
from abc import ABC, abstractmethod

import pytorch_lightning as pl
from torch.utils.data import DataLoader

from ..utils.instantiate import instantiate_from_config

logger = logging.getLogger(__name__)


@dataclass
class ActiveLearningState:
    """ä¸»åŠ¨å­¦ä¹ çŠ¶æ€ç®¡ç†"""

    # åŸºæœ¬ä¿¡æ¯
    experiment_name: str
    checkpoint_path: str
    iteration: int = 0

    # æ•°æ®ä¿¡æ¯
    unlabeled_pool: List[str] = None  # æœªæ ‡æ³¨æ ·æœ¬IDåˆ—è¡¨
    labeled_samples: List[str] = None  # å·²æ ‡æ³¨æ ·æœ¬IDåˆ—è¡¨
    annotation_history: List[Dict] = None  # æ ‡æ³¨å†å²

    # ç»“æœä¿¡æ¯
    uncertainty_scores: Dict[str, float] = None  # ä¸ç¡®å®šæ€§åˆ†æ•°
    selected_samples: List[str] = None  # å½“å‰é€‰ä¸­çš„æ ·æœ¬
    pseudo_labels: List[Dict] = None  # ä¼ªæ ‡ç­¾ç»“æœ

    def __post_init__(self):
        if self.unlabeled_pool is None:
            self.unlabeled_pool = []
        if self.labeled_samples is None:
            self.labeled_samples = []
        if self.annotation_history is None:
            self.annotation_history = []
        # ğŸ”§ æ–°å¢ï¼šåˆå§‹åŒ–ç¼ºå¤±çš„å­—æ®µ
        if self.selected_samples is None:
            self.selected_samples = []
        if self.pseudo_labels is None:
            self.pseudo_labels = []
        # ğŸ”§ æ–°å¢ï¼šåˆå§‹åŒ– uncertainty_scoresï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.uncertainty_scores is None:
            self.uncertainty_scores = {}

    def save(self, save_path: Path):
        """ä¿å­˜çŠ¶æ€åˆ°æ–‡ä»¶"""
        save_path.parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, "wb") as f:
            pickle.dump(self, f)
        logger.info(f"Active learning state saved to: {save_path}")

    @classmethod
    def load(cls, load_path: Path):
        """ä»æ–‡ä»¶åŠ è½½çŠ¶æ€"""
        if not load_path.exists():
            raise FileNotFoundError(f"State file not found: {load_path}")

        with open(load_path, "rb") as f:
            state = pickle.load(f)

        # ğŸ”§ æ–°å¢ï¼šç¡®ä¿åŠ è½½çš„çŠ¶æ€ä¹Ÿæœ‰æ­£ç¡®çš„å­—æ®µåˆå§‹åŒ–
        if not hasattr(state, "selected_samples") or state.selected_samples is None:
            state.selected_samples = []
        if not hasattr(state, "pseudo_labels") or state.pseudo_labels is None:
            state.pseudo_labels = []
        if not hasattr(state, "uncertainty_scores") or state.uncertainty_scores is None:
            state.uncertainty_scores = {}

        logger.info(f"Active learning state loaded from: {load_path}")
        return state

    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, experiment_name: str):
        """ä»æ£€æŸ¥ç‚¹åˆ›å»ºåˆå§‹çŠ¶æ€"""
        return cls(experiment_name=experiment_name, checkpoint_path=checkpoint_path, iteration=0)


class BaseActiveStep(ABC):
    """ä¸»åŠ¨å­¦ä¹ æ­¥éª¤åŸºç±»"""

    def __init__(self, config: Dict[str, Any], state_path: Optional[str] = None):
        self.config = config
        self.state_path = Path(state_path) if state_path else None
        self.state = self._load_or_create_state()

        # è®¾ç½®è¾“å‡ºç›®å½•
        self.output_dir = Path(config.get("outputs", {}).get("experiment_dir", "outputs"))
        self.active_dir = self.output_dir / "active_learning"
        self.active_dir.mkdir(parents=True, exist_ok=True)

    def _load_or_create_state(self) -> ActiveLearningState:
        """åŠ è½½æˆ–åˆ›å»ºçŠ¶æ€"""
        if self.state_path and self.state_path.exists():
            return ActiveLearningState.load(self.state_path)
        else:
            # ä»é…ç½®åˆ›å»ºåˆå§‹çŠ¶æ€
            experiment_name = self.config.get("experiment_name", "active_learning")
            checkpoint_path = self._find_best_checkpoint()
            return ActiveLearningState.from_checkpoint(checkpoint_path, experiment_name)

    def _find_best_checkpoint(self) -> str:
        """å¯»æ‰¾æœ€ä½³æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        # ä»é…ç½®æˆ–é»˜è®¤ä½ç½®å¯»æ‰¾æ£€æŸ¥ç‚¹
        checkpoint_path = self.config.get("checkpoint_path")
        if checkpoint_path and Path(checkpoint_path).exists():
            return checkpoint_path

        # åœ¨å®éªŒç›®å½•ä¸‹å¯»æ‰¾æœ€ä½³æ£€æŸ¥ç‚¹
        exp_dir = Path(self.config.get("outputs", {}).get("experiment_dir", "outputs"))
        checkpoint_dir = exp_dir / "checkpoints"

        if checkpoint_dir.exists():
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®çš„æ£€æŸ¥ç‚¹æœç´¢æ¨¡å¼
            # å°è¯•å¤šç§å¸¸è§çš„æ£€æŸ¥ç‚¹å‘½åæ¨¡å¼
            patterns = "best-*.ckpt"  # é€šç”¨bestæ¨¡å¼

            best_ckpts = list(checkpoint_dir.glob(patterns))
            if best_ckpts:
                logger.info(f"Found {len(best_ckpts)} checkpoints with pattern: {patterns}")

                # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
                latest_ckpt = max(best_ckpts, key=lambda x: x.stat().st_mtime)
                logger.info(f"Found checkpoint: {latest_ckpt}")
                return str(latest_ckpt)

        raise FileNotFoundError("No valid checkpoint found. Please run baseline training first.")

    def save_state(self):
        """ä¿å­˜å½“å‰çŠ¶æ€"""
        state_file = self.active_dir / f"state_iter_{self.state.iteration}.pkl"
        self.state.save(state_file)

    @abstractmethod
    def run(self) -> Dict[str, Any]:
        """è¿è¡Œå½“å‰æ­¥éª¤"""
        pass


class UncertaintyEstimator(BaseActiveStep):
    """æ­¥éª¤2ï¼šä¸ç¡®å®šæ€§ä¼°è®¡"""

    def __init__(self, config: Dict[str, Any], state_path: Optional[str] = None):
        super().__init__(config, state_path)

        # ä¸ç¡®å®šæ€§ä¼°è®¡é…ç½®
        uncertainty_config = config.get("active_pseudo_learning", {}).get("uncertainty_estimation", {})
        self.method = uncertainty_config.get("method", "mc_dropout")
        self.n_forward_passes = uncertainty_config.get("params", {}).get("n_forward_passes", 10)

        logger.info(f"ğŸ” UncertaintyEstimator initialized")
        logger.info(f"ğŸ“Š Method: {self.method}, Forward passes: {self.n_forward_passes}")

    def run(self) -> Dict[str, Any]:
        """è¿è¡Œä¸ç¡®å®šæ€§ä¼°è®¡"""
        logger.info("ğŸ” Starting uncertainty estimation...")

        # 1. åŠ è½½æ¨¡å‹
        model = self._load_model()

        # 2. è·å–æœªæ ‡æ³¨æ•°æ®
        datamodule = self._setup_datamodule()

        # 3. ä¼°è®¡ä¸ç¡®å®šæ€§
        uncertainty_scores = self._estimate_uncertainty(model, datamodule)

        # 4. æ›´æ–°çŠ¶æ€
        self.state.uncertainty_scores = uncertainty_scores
        self.save_state()

        # 5. ä¿å­˜ç»“æœ
        results_path = self.active_dir / f"uncertainty_scores_iter_{self.state.iteration}.json"
        with open(results_path, "w") as f:
            json.dump(uncertainty_scores, f, indent=2)

        logger.info(f"âœ… Uncertainty estimation completed")
        logger.info(f"ğŸ“ Results saved to: {results_path}")
        logger.info(f"ğŸ“Š Estimated uncertainty for {len(uncertainty_scores)} samples")

        return {
            "uncertainty_scores": uncertainty_scores,
            "results_path": str(results_path),
            "num_samples": len(uncertainty_scores),
        }

    def _load_model(self) -> pl.LightningModule:
        """åŠ è½½æ¨¡å‹ - GPUä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info(f"ğŸ“¥ Loading model from: {self.state.checkpoint_path}")

        # é‡ç”¨ç°æœ‰çš„æ¨¡å‹å®ä¾‹åŒ–é€»è¾‘
        model = instantiate_from_config(self.config["model"])

        # ğŸ”§ ä¿®å¤1ï¼šæ£€æŸ¥GPUå¯ç”¨æ€§å¹¶è®¾ç½®æ­£ç¡®çš„è®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸ¯ Using device: {device}")

        # ğŸ”§ ä¿®å¤2ï¼šæ ¹æ®è®¾å¤‡ç±»å‹åŠ è½½æ£€æŸ¥ç‚¹
        if device == "cuda":
            checkpoint = torch.load(self.state.checkpoint_path, map_location="cuda")
        else:
            checkpoint = torch.load(self.state.checkpoint_path, map_location="cpu")

        model.load_state_dict(checkpoint["state_dict"])

        # ğŸ”§ ä¿®å¤3ï¼šæ˜ç¡®ç§»åŠ¨æ¨¡å‹åˆ°GPU
        model = model.to(device)
        model.eval()

        # è®¾ç½®dropoutä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆç”¨äºMC Dropoutï¼‰
        if self.method == "mc_dropout":
            for module in model.modules():
                if isinstance(module, torch.nn.Dropout):
                    module.train()

        return model

    def _setup_datamodule(self):
        """è®¾ç½®æ•°æ®æ¨¡å—"""
        # é‡ç”¨ç°æœ‰çš„æ•°æ®æ¨¡å—
        datamodule = instantiate_from_config(self.config["data"])
        datamodule.setup("test")  # ä½¿ç”¨æµ‹è¯•é›†ä½œä¸ºæœªæ ‡æ³¨æ± 
        return datamodule

    def _estimate_uncertainty(self, model: pl.LightningModule, datamodule) -> Dict[str, float]:
        """
        ğŸ”¥ ä¿®å¤ç‰ˆæœ¬ï¼šä½¿ç”¨çœŸå®sample IDçš„ä¸ç¡®å®šæ€§ä¼°è®¡

        å…³é”®ä¿®å¤ï¼š
        1. ç›´æ¥ä»æ•°æ®é›†è·å–çœŸå®sample ID
        2. æ›¿æ¢è™šæ‹ŸIDç”Ÿæˆé€»è¾‘
        3. ç¡®ä¿ç»“æœå¯è¿½æº¯åˆ°åŸå§‹æ•°æ®
        """
        logger.info(f"ğŸ”„ Running {self.method} with {self.n_forward_passes} forward passes...")
        logger.info("ğŸ” FIXED VERSION: Using real sample IDs from dataset")

        # ğŸ”§ ä¿®å¤1ï¼šç¡®ä¿è®¾å¤‡æ­£ç¡®
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = model.to(device)
        model.eval()
        device = next(model.parameters()).device
        test_loader = datamodule.test_dataloader()
        uncertainty_scores = {}

        # æ¿€æ´»dropoutç”¨äºMC Dropout
        if self.method == "mc_dropout":
            for module in model.modules():
                if isinstance(module, torch.nn.Dropout):
                    module.train()

        total_batches = len(test_loader)
        logger.info(f"ğŸ“Š Total batches to process: {total_batches}")

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šè·å–æ•°æ®é›†ä»¥è®¿é—®çœŸå®ID
        test_dataset = datamodule.test_dataset

        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                try:
                    # è§£æbatchæ•°æ®
                    if isinstance(batch, list) and len(batch) >= 2:
                        images, labels = batch[0], batch[1]
                        batch_size = images.size(0)
                    else:
                        logger.warning(f"âš ï¸ Unexpected batch format at {batch_idx}")
                        continue

                    # ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šè®¡ç®—çœŸå®çš„æ•°æ®é›†ç´¢å¼•å¹¶è·å–çœŸå®ID
                    batch_start_idx = batch_idx * test_loader.batch_size
                    real_sample_ids = []

                    for i in range(batch_size):
                        dataset_idx = batch_start_idx + i
                        if dataset_idx < len(test_dataset):
                            # ç›´æ¥ä»æ•°æ®é›†è·å–çœŸå®ID
                            real_id = test_dataset.data_index.iloc[dataset_idx]["ID"]
                            real_sample_ids.append(real_id)
                        else:
                            # å®‰å…¨å›é€€
                            real_sample_ids.append(f"sample_{batch_idx}_{i}")

                    # éªŒè¯è·å–çš„çœŸå®ID
                    if batch_idx == 0:  # åªåœ¨ç¬¬ä¸€æ‰¹æ˜¾ç¤ºç¤ºä¾‹
                        logger.info(f"âœ… Real IDs example: {real_sample_ids[:3]}")
                        logger.info(f"ğŸ” ID format validation: {real_sample_ids[0].startswith('ID_')}")

                    images = images.to(device)
                    batch_predictions = []

                    # MC Dropoutå‰å‘ä¼ æ’­
                    for pass_idx in range(self.n_forward_passes):
                        output = model(images)
                        if output.dim() == 2 and output.size(1) == 1:
                            output = output.squeeze(1)

                        probs = torch.sigmoid(output)
                        batch_predictions.append(probs.cpu().numpy())

                    # è®¡ç®—ä¸ç¡®å®šæ€§
                    batch_predictions = np.array(batch_predictions)  # [n_passes, batch_size]

                    for i, real_id in enumerate(real_sample_ids):
                        try:
                            sample_pred = batch_predictions[:, i]  # [n_passes]

                            # è®¡ç®—æ–¹å·®å’Œç†µ
                            mean_pred = np.mean(sample_pred)
                            variance = np.var(sample_pred)

                            # äºŒåˆ†ç±»ç†µ
                            p = np.clip(mean_pred, 1e-8, 1 - 1e-8)
                            entropy = -(p * np.log(p) + (1 - p) * np.log(1 - p))

                            # ç»„åˆä¸ç¡®å®šæ€§
                            uncertainty = variance + 0.1 * entropy
                            uncertainty_scores[str(real_id)] = float(uncertainty)

                        except Exception as e:
                            logger.error(f"âŒ Error computing uncertainty for sample {real_id}: {e}")
                            continue

                    # ğŸ”¥ æ”¹è¿›çš„è¿›åº¦æŠ¥å‘Š
                    if (batch_idx + 1) % 10 == 0:
                        progress_pct = (batch_idx + 1) / total_batches * 100
                        samples_processed = len(uncertainty_scores)

                        # ç»Ÿè®¡çœŸå®IDæ•°é‡
                        real_id_count = sum(1 for k in uncertainty_scores.keys() if k.startswith("ID_"))
                        fake_id_count = samples_processed - real_id_count

                        logger.info(f"ğŸ”„ Progress: {progress_pct:.1f}% ({batch_idx+1}/{total_batches} batches)")
                        logger.info(
                            f"ğŸ“Š Processed: {samples_processed} samples ({real_id_count} real IDs, {fake_id_count} fallback IDs)"
                        )

                    # å†…å­˜æ¸…ç†
                    if batch_idx % 50 == 0:
                        torch.cuda.empty_cache() if torch.cuda.is_available() else None

                except Exception as e:
                    logger.error(f"âŒ Error processing batch {batch_idx}: {e}")
                    continue

        # æœ€ç»ˆéªŒè¯
        if len(uncertainty_scores) == 0:
            raise ValueError("No samples processed successfully")

        # ğŸ”¥ ç»“æœéªŒè¯å’Œç»Ÿè®¡
        total_samples = len(uncertainty_scores)
        real_id_samples = sum(1 for k in uncertainty_scores.keys() if k.startswith("ID_"))
        fallback_samples = total_samples - real_id_samples

        logger.info(f"ğŸ“Š Final statistics:")
        logger.info(f"ğŸ“Š - Total samples processed: {total_samples}")
        logger.info(f"ğŸ“Š - Real IDs: {real_id_samples} ({real_id_samples/total_samples*100:.1f}%)")
        logger.info(f"ğŸ“Š - Fallback IDs: {fallback_samples} ({fallback_samples/total_samples*100:.1f}%)")

        # æ˜¾ç¤ºçœŸå®IDç¤ºä¾‹
        real_ids = [k for k in uncertainty_scores.keys() if k.startswith("ID_")][:5]
        if real_ids:
            logger.info(f"ğŸ“ Real ID examples: {real_ids}")
        else:
            logger.warning("âš ï¸ No real IDs found! All samples using fallback IDs.")

        logger.info(f"ğŸ“Š Successfully processed {len(uncertainty_scores)} samples")
        return uncertainty_scores


class SampleSelector(BaseActiveStep):
    """æ­¥éª¤3ï¼šæ ·æœ¬é€‰æ‹©"""

    def __init__(self, config: Dict[str, Any], state_path: Optional[str] = None):
        super().__init__(config, state_path)

        # æ ·æœ¬é€‰æ‹©é…ç½®
        active_config = config.get("active_pseudo_learning", {})
        self.annotation_budget = active_config.get("annotation_budget", 50)

        selection_config = active_config.get("active_learning", {})
        self.strategies = selection_config.get(
            "strategies", {"uncertainty": 0.6, "diversity": 0.3, "cluster_based": 0.1}
        )

        logger.info(f"ğŸ¯ SampleSelector initialized")
        logger.info(f"ğŸ“ Budget: {self.annotation_budget}")
        logger.info(f"ğŸ² Strategies: {self.strategies}")

    def run(self) -> Dict[str, Any]:
        """è¿è¡Œæ ·æœ¬é€‰æ‹©"""
        logger.info("ğŸ¯ Starting sample selection...")

        # 1. æ£€æŸ¥æ˜¯å¦æœ‰ä¸ç¡®å®šæ€§åˆ†æ•°
        if not self.state.uncertainty_scores:
            # å°è¯•ä»æ–‡ä»¶åŠ è½½
            uncertainty_file = self.active_dir / f"uncertainty_scores_iter_{self.state.iteration}.json"
            if uncertainty_file.exists():
                with open(uncertainty_file, "r") as f:
                    self.state.uncertainty_scores = json.load(f)
                logger.info(f"ğŸ“¥ Loaded uncertainty scores from: {uncertainty_file}")
            else:
                raise ValueError("No uncertainty scores found. Please run uncertainty estimation first.")

        # 2. é€‰æ‹©æ ·æœ¬
        selected_samples = self._select_samples()

        # 3. æ›´æ–°çŠ¶æ€
        self.state.selected_samples = selected_samples
        self.save_state()

        # 4. ç”Ÿæˆæ ‡æ³¨è¯·æ±‚æ–‡ä»¶
        annotation_file = self._generate_annotation_request(selected_samples)

        logger.info(f"âœ… Sample selection completed")
        logger.info(f"ğŸ“ Selected {len(selected_samples)} samples for annotation")
        logger.info(f"ğŸ“ Annotation request saved to: {annotation_file}")

        return {
            "selected_samples": selected_samples,
            "annotation_file": str(annotation_file),
            "num_selected": len(selected_samples),
        }

    def _select_samples(self) -> List[str]:
        """é€‰æ‹©æ ·æœ¬è¿›è¡Œæ ‡æ³¨"""
        uncertainty_scores = self.state.uncertainty_scores

        # è¿‡æ»¤å·²æ ‡æ³¨çš„æ ·æœ¬
        available_samples = {
            sample_id: score
            for sample_id, score in uncertainty_scores.items()
            if sample_id not in self.state.labeled_samples
        }

        if len(available_samples) < self.annotation_budget:
            logger.warning(f"Available samples ({len(available_samples)}) < budget ({self.annotation_budget})")
            return list(available_samples.keys())

        # åŸºäºä¸ç¡®å®šæ€§é€‰æ‹©
        uncertainty_weight = self.strategies.get("uncertainty", 1.0)
        if uncertainty_weight > 0:
            # é€‰æ‹©ä¸ç¡®å®šæ€§æœ€é«˜çš„æ ·æœ¬
            sorted_samples = sorted(available_samples.items(), key=lambda x: x[1], reverse=True)
            n_uncertainty = int(self.annotation_budget * uncertainty_weight)
            selected_by_uncertainty = [sample_id for sample_id, _ in sorted_samples[:n_uncertainty]]
        else:
            selected_by_uncertainty = []

        # éšæœºé€‰æ‹©å‰©ä½™æ ·æœ¬ï¼ˆç®€åŒ–çš„å¤šæ ·æ€§ç­–ç•¥ï¼‰
        remaining_budget = self.annotation_budget - len(selected_by_uncertainty)
        if remaining_budget > 0:
            remaining_samples = [
                sample_id for sample_id in available_samples.keys() if sample_id not in selected_by_uncertainty
            ]

            if len(remaining_samples) <= remaining_budget:
                selected_random = remaining_samples
            else:
                import random

                random.seed(42)
                selected_random = random.sample(remaining_samples, remaining_budget)
        else:
            selected_random = []

        selected_samples = selected_by_uncertainty + selected_random

        logger.info(f"ğŸ“Š Selection breakdown:")
        logger.info(f"  ğŸ¯ By uncertainty: {len(selected_by_uncertainty)}")
        logger.info(f"  ğŸ² Random/diversity: {len(selected_random)}")

        return selected_samples

    def _generate_annotation_request(self, selected_samples: List[str]) -> Path:
        """ç”Ÿæˆæ ‡æ³¨è¯·æ±‚æ–‡ä»¶"""
        annotation_request = {
            "iteration": self.state.iteration,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "experiment_name": self.state.experiment_name,
            "selected_samples": selected_samples,
            "annotation_budget": self.annotation_budget,
            "instructions": {
                "task": "è¯·ä¸ºé€‰ä¸­çš„æ ·æœ¬æ ‡æ³¨æ˜¯å¦ä¸ºæ»‘å¡åŒºåŸŸ",
                "labels": {"0": "éæ»‘å¡åŒºåŸŸ", "1": "æ»‘å¡åŒºåŸŸ"},
                "format": "è¯·åœ¨annotation_results.jsonä¸­æä¾›æ ‡æ³¨ç»“æœ",
            },
            "sample_details": [],
        }

        # æ·»åŠ æ ·æœ¬è¯¦ç»†ä¿¡æ¯
        for sample_id in selected_samples:
            uncertainty = self.state.uncertainty_scores.get(sample_id, 0.0)
            sample_info = {
                "sample_id": sample_id,
                "uncertainty_score": uncertainty,
                "image_path": f"dataset/test_data/{sample_id}",  # æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´
                "label": None,  # å¾…æ ‡æ³¨
                "confidence": None,  # å¯é€‰ï¼šæ ‡æ³¨è€…ä¿¡å¿ƒ
            }
            annotation_request["sample_details"].append(sample_info)

        # ä¿å­˜æ ‡æ³¨è¯·æ±‚
        request_file = self.active_dir / f"annotation_request_iter_{self.state.iteration}.json"
        with open(request_file, "w", encoding="utf-8") as f:
            json.dump(annotation_request, f, indent=2, ensure_ascii=False)

        return request_file


class ActiveRetrainer(BaseActiveStep):
    """æ­¥éª¤5ï¼šæ¨¡å‹fine-tuningï¼ˆåŸºäºå·²æœ‰æ£€æŸ¥ç‚¹ï¼Œä¸æ˜¯ä»å¤´è®­ç»ƒï¼‰"""

    def __init__(self, config: Dict[str, Any], state_path: Optional[str] = None, annotation_file: Optional[str] = None):
        super().__init__(config, state_path)
        self.annotation_file = annotation_file

        # é‡è®­ç»ƒé…ç½®
        active_config = config.get("active_pseudo_learning", {})
        self.pseudo_config = active_config.get("pseudo_labeling", {})

        logger.info(f"ğŸ”„ ActiveRetrainer initialized (Fine-tuning mode)")
        logger.info(f"ğŸ“¥ Will load from checkpoint: {self.state.checkpoint_path}")
        logger.info(f"âš ï¸  Note: This will fine-tune existing model, NOT train from scratch")

    def run(self) -> Dict[str, Any]:
        """è¿è¡Œæ¨¡å‹é‡è®­ç»ƒï¼ˆæ­¥éª¤5ï¼šåŸºäºå·²æœ‰æ£€æŸ¥ç‚¹è¿›è¡Œfine-tuningï¼‰"""
        logger.info("ğŸ”„ Starting model fine-tuning with new annotations and pseudo labels...")
        logger.info("âš ï¸  Note: This is fine-tuning from existing checkpoint, NOT training from scratch")

        # 1. åŠ è½½æ ‡æ³¨ç»“æœ
        annotations = self._load_annotations()

        # 2. åŠ è½½ä¼ªæ ‡ç­¾ï¼ˆä¸å†ç”Ÿæˆï¼Œè€Œæ˜¯ä»æ–‡ä»¶åŠ è½½ï¼‰
        pseudo_labels = self._generate_pseudo_labels()  # æ–¹æ³•åä¿æŒä¸å˜ï¼Œä½†å†…éƒ¨é€»è¾‘æ”¹ä¸ºåŠ è½½

        # 3. æ›´æ–°è®­ç»ƒæ•°æ®
        updated_datamodule = self._update_training_data(annotations, pseudo_labels)

        # 4. Fine-tuneæ¨¡å‹ï¼ˆåŸºäºå·²æœ‰æ£€æŸ¥ç‚¹ï¼‰
        new_model, training_results = self._retrain_model(updated_datamodule)

        # 5. æ›´æ–°çŠ¶æ€
        self.state.annotation_history.append(
            {
                "iteration": self.state.iteration,
                "annotations": annotations,
                "num_pseudo_labels": len(pseudo_labels),
                "training_results": training_results,
                "fine_tuning": True,
                "pseudo_labels_source": "loaded_from_file",  # æ ‡è®°æ¥æº
            }
        )

        # æ›´æ–°è¿­ä»£è®¡æ•°
        old_iteration = self.state.iteration
        self.state.iteration += 1
        self.save_state()

        logger.info(f"âœ… Model fine-tuning completed (iteration {old_iteration} -> {self.state.iteration})")
        logger.info(f"ğŸ“Š Used {len(annotations)} human annotations")
        logger.info(f"ğŸ·ï¸ Used {len(pseudo_labels)} pseudo labels (loaded from file)")

        return {
            "num_annotations": len(annotations),
            "num_pseudo_labels": len(pseudo_labels),
            "training_results": training_results,
            "new_checkpoint": training_results.get("best_checkpoint"),
            "iteration": self.state.iteration,
            "fine_tuning": True,
            "pseudo_labels_source": "loaded_from_file",
        }

    def _load_annotations(self) -> List[Dict]:
        """åŠ è½½äººå·¥æ ‡æ³¨ç»“æœ"""
        if self.annotation_file:
            annotation_path = Path(self.annotation_file)
        else:
            # å¯»æ‰¾æœ€æ–°çš„æ ‡æ³¨æ–‡ä»¶
            annotation_path = self.active_dir / f"annotation_results_iter_{self.state.iteration}.json"

        if not annotation_path.exists():
            raise FileNotFoundError(f"Annotation file not found: {annotation_path}")

        with open(annotation_path, "r", encoding="utf-8") as f:
            annotation_data = json.load(f)

        # è§£ææ ‡æ³¨ç»“æœ
        annotations = []

        if "annotations" in annotation_data:
            for sample_id, label in annotation_data["annotations"].items():
                if label is not None:  # è·³è¿‡ None å€¼ï¼ˆæœªæ ‡æ³¨çš„æ ·æœ¬ï¼‰
                    annotations.append(
                        {
                            "sample_id": sample_id,
                            "label": label,
                            "confidence": 0.9,  # äººå·¥æ ‡æ³¨é»˜è®¤ç½®ä¿¡åº¦ä¸º0.9
                        }
                    )
        else:
            raise ValueError(
                f"Unsupported annotation format in {annotation_path}. "
                f"Expected 'sample_details', 'annotations' dict, or direct list format."
            )

        logger.info(f"ğŸ“¥ Loaded {len(annotations)} annotations from: {annotation_path}")
        return annotations

    def _generate_pseudo_labels(self) -> List[Dict]:
        """
        ç”Ÿæˆä¼ªæ ‡ç­¾ - å®Œæ•´å®ç°ç‰ˆæœ¬

        æ ¸å¿ƒæ€è·¯ï¼š
        1. åˆ©ç”¨æ­¥éª¤2ä¸­çš„ä¸ç¡®å®šæ€§è¯„ä¼°ç»“æœ
        2. é€‰æ‹©ç¡®å®šæ€§å¼ºçš„æ ·æœ¬ï¼ˆä½ä¸ç¡®å®šæ€§ï¼‰
        3. ä½¿ç”¨å½“å‰æœ€ä½³æ¨¡å‹è¿›è¡Œæ¨ç†
        4. æ ¹æ®ç½®ä¿¡åº¦é˜ˆå€¼ç”Ÿæˆä¼ªæ ‡ç­¾

        éµå¾ªä¸‰ä¸ªåŸåˆ™ï¼š
        - æœ€å°æ”¹åŠ¨ï¼šé‡ç”¨ç°æœ‰çš„æ¨¡å‹åŠ è½½å’Œæ•°æ®å¤„ç†é€»è¾‘
        - å•ä¸€èŒè´£ï¼šåªè´Ÿè´£ä¼ªæ ‡ç­¾ç”Ÿæˆ
        - æ¸è¿›å¢å¼ºï¼šåœ¨ç°æœ‰uncertainty_scoresåŸºç¡€ä¸Šå®ç°
        """
        logger.info("ğŸ·ï¸ Loading pseudo labels from previous step...")

        # 1. å¯»æ‰¾ä¼ªæ ‡ç­¾æ–‡ä»¶
        pseudo_labels_file = self._find_pseudo_labels_file()

        if not pseudo_labels_file:
            logger.warning("âš ï¸ No pseudo labels file found, proceeding without pseudo labels")
            return []

        # 2. åŠ è½½ä¼ªæ ‡ç­¾æ•°æ®
        try:
            with open(pseudo_labels_file, "r", encoding="utf-8") as f:
                pseudo_labels_data = json.load(f)

            # æå–ä¼ªæ ‡ç­¾åˆ—è¡¨
            if "pseudo_labels" in pseudo_labels_data:
                pseudo_labels = pseudo_labels_data["pseudo_labels"]
            else:
                # å…¼å®¹ç›´æ¥ä¸ºåˆ—è¡¨æ ¼å¼çš„æ–‡ä»¶
                pseudo_labels = pseudo_labels_data if isinstance(pseudo_labels_data, list) else []

            logger.info(f"ğŸ“¥ Loaded {len(pseudo_labels)} pseudo labels from: {pseudo_labels_file}")

            # 3. è®°å½•ç»Ÿè®¡ä¿¡æ¯
            if "statistics" in pseudo_labels_data:
                stats = pseudo_labels_data["statistics"]
                logger.info(f"ğŸ“Š Pseudo label statistics:")
                logger.info(f"  - Class distribution: {stats.get('class_distribution', {})}")
                logger.info(f"  - Average confidence: {stats.get('overall_avg_confidence', 0):.3f}")
                logger.info(f"  - Average uncertainty: {stats.get('overall_avg_uncertainty', 0):.3f}")

            return pseudo_labels

        except Exception as e:
            logger.error(f"âŒ Error loading pseudo labels from {pseudo_labels_file}: {e}")
            logger.warning("âš ï¸ Proceeding without pseudo labels")
            return []

    def _find_pseudo_labels_file(self) -> Optional[Path]:
        """
        å¯»æ‰¾ä¼ªæ ‡ç­¾æ–‡ä»¶

        æœç´¢ä¼˜å…ˆçº§ï¼š
        1. å½“å‰è¿­ä»£çš„ä¼ªæ ‡ç­¾æ–‡ä»¶
        2. æœ€æ–°çš„ä¼ªæ ‡ç­¾æ–‡ä»¶
        3. æŒ‡å®šè·¯å¾„çš„ä¼ªæ ‡ç­¾æ–‡ä»¶ï¼ˆå¦‚æœé…ç½®ä¸­æœ‰ï¼‰
        """
        # 1. ä¼˜å…ˆæŸ¥æ‰¾å½“å‰è¿­ä»£çš„æ–‡ä»¶
        current_iter_file = self.active_dir / f"pseudo_labels_iter_{self.state.iteration}.json"
        if current_iter_file.exists():
            logger.info(f"ğŸ“ Found current iteration pseudo labels: {current_iter_file}")
            return current_iter_file

        # 2. æŸ¥æ‰¾æœ€æ–°çš„ä¼ªæ ‡ç­¾æ–‡ä»¶
        pseudo_files = list(self.active_dir.glob("pseudo_labels_iter_*.json"))
        if pseudo_files:
            # æŒ‰è¿­ä»£å·æ’åºï¼Œå–æœ€æ–°çš„
            latest_file = sorted(pseudo_files, key=lambda x: int(x.stem.split("_")[-1]))[-1]
            logger.info(f"ğŸ“ Found latest pseudo labels: {latest_file}")
            return latest_file

        # 3. æ£€æŸ¥é…ç½®ä¸­æ˜¯å¦æŒ‡å®šäº†ä¼ªæ ‡ç­¾æ–‡ä»¶è·¯å¾„
        pseudo_file_path = self.pseudo_config.get("pseudo_labels_file")
        if pseudo_file_path:
            pseudo_file = Path(pseudo_file_path)
            if pseudo_file.exists():
                logger.info(f"ğŸ“ Found configured pseudo labels: {pseudo_file}")
                return pseudo_file
            else:
                logger.warning(f"âš ï¸ Configured pseudo labels file not found: {pseudo_file}")

        # 4. æœªæ‰¾åˆ°ä»»ä½•ä¼ªæ ‡ç­¾æ–‡ä»¶
        logger.warning("âš ï¸ No pseudo labels file found")
        logger.info("ğŸ’¡ Tip: Run 'python main.py pseudo_labeling config.yaml' first to generate pseudo labels")
        return None

    def _update_training_data(self, annotations: List[Dict], pseudo_labels: List[Dict]):
        """
        æ›´æ–°è®­ç»ƒæ•°æ® - æ­£ç¡®ç‰ˆæœ¬ï¼Œç¡®ä¿æ— æ•°æ®æ³„éœ²

        æ ¸å¿ƒåŸåˆ™ï¼š
        1. åŸéªŒè¯é›†æ ·æœ¬ç»å¯¹ä¸èƒ½è¿›å…¥è®­ç»ƒé›†ï¼ˆæ•°æ®æ³„éœ²ï¼‰
        2. åŸè®­ç»ƒé›†æ ·æœ¬ä¿æŒåœ¨è®­ç»ƒé›†ä¸­
        3. æ–°æ ‡æ³¨æ ·æœ¬å¯ä»¥æŒ‰ç­–ç•¥åˆ†é…åˆ°è®­ç»ƒé›†å’ŒéªŒè¯é›†
        """
        logger.info("ğŸ“Š Updating training data...")

        # 1. è¯»å–åŸå§‹åˆ†å‰²æ–‡ä»¶ï¼ˆä»datasetç›®å½•ï¼‰

        original_train_split = Path("dataset/train_split.csv")
        original_val_split = Path("dataset/val_split.csv")

        if not original_train_split.exists() or not original_val_split.exists():
            raise FileNotFoundError(
                f"âŒ Original split files not found! " f"Expected: {original_train_split} and {original_val_split}"
            )

        # 2. åŠ è½½åŸå§‹åˆ†å‰²
        original_train_df = pd.read_csv(original_train_split)
        original_val_df = pd.read_csv(original_val_split)

        logger.info(f"ğŸ“‚ Original train split: {len(original_train_df)} samples")
        logger.info(f"ğŸ“‚ Original val split: {len(original_val_df)} samples")

        # 4. åˆå¹¶æ–°æ ·æœ¬ï¼ˆæ ‡æ³¨ + ä¼ªæ ‡ç­¾ï¼‰
        new_samples = []

        # æ·»åŠ äººå·¥æ ‡æ³¨æ ·æœ¬
        for ann in annotations:
            new_samples.append({"sample_id": ann["sample_id"], "label": ann["label"], "source": "annotation"})

        # æ·»åŠ ä¼ªæ ‡ç­¾æ ·æœ¬
        for pseudo in pseudo_labels:
            new_samples.append({"sample_id": pseudo["sample_id"], "label": pseudo["label"], "source": "pseudo_label"})

        logger.info(
            f"ğŸ“ Processing {len(new_samples)} new samples ({len(annotations)} annotations + {len(pseudo_labels)} pseudo labels)"
        )

        # 5. åˆ†é…æ–°æ ·æœ¬åˆ°è®­ç»ƒé›†å’ŒéªŒè¯é›†
        # ç­–ç•¥ï¼šæŒ‰80/20æ¯”ä¾‹åˆ†é…æ–°æ ·æœ¬ï¼ˆå¯ä»¥è°ƒæ•´ï¼‰
        np.random.seed(self.config.get("seed", 3407))  # ç¡®ä¿å¯é‡ç°
        new_sample_indices = np.random.permutation(len(new_samples))

        # è®¡ç®—åˆ†é…æ•°é‡
        val_ratio = 0.2  # 20%æ–°æ ·æœ¬è¿›å…¥éªŒè¯é›†
        n_new_val = int(len(new_samples) * val_ratio)
        n_new_train = len(new_samples) - n_new_val

        new_val_indices = new_sample_indices[:n_new_val]
        new_train_indices = new_sample_indices[n_new_val:]

        logger.info(f"ğŸ“Š New sample allocation: {n_new_train} â†’ train, {n_new_val} â†’ val")

        # 6. æ„å»ºå¢å¼ºçš„è®­ç»ƒé›†å’ŒéªŒè¯é›†
        enhanced_train_rows = []
        enhanced_val_rows = []

        # ä¿ç•™æ‰€æœ‰åŸå§‹è®­ç»ƒæ ·æœ¬
        for _, row in original_train_df.iterrows():
            enhanced_train_rows.append(row.to_dict())

        # ä¿ç•™æ‰€æœ‰åŸå§‹éªŒè¯æ ·æœ¬
        for _, row in original_val_df.iterrows():
            enhanced_val_rows.append(row.to_dict())

        # æ·»åŠ æ–°æ ·æœ¬åˆ°è®­ç»ƒé›†
        for idx in new_train_indices:
            sample = new_samples[idx]
            enhanced_train_rows.append({"ID": sample["sample_id"], "label": sample["label"]})

        # æ·»åŠ æ–°æ ·æœ¬åˆ°éªŒè¯é›†
        for idx in new_val_indices:
            sample = new_samples[idx]
            enhanced_val_rows.append({"ID": sample["sample_id"], "label": sample["label"]})

        # 7. åˆ›å»ºDataFrames
        enhanced_train_df = pd.DataFrame(enhanced_train_rows)
        enhanced_val_df = pd.DataFrame(enhanced_val_rows)

        # 8. ğŸ”§ å…³é”®ä¿®å¤ï¼šåˆ›å»ºåŒ…å«æ‰€æœ‰æ•°æ®çš„å®Œæ•´CSVæ–‡ä»¶
        all_enhanced_data = []
        all_enhanced_data.extend(enhanced_train_rows)
        all_enhanced_data.extend(enhanced_val_rows)
        all_enhanced_df = pd.DataFrame(all_enhanced_data)

        # å»é‡ï¼ˆé˜²æ­¢é‡å¤ï¼‰
        all_enhanced_df = all_enhanced_df.drop_duplicates(subset=["ID"], keep="first")

        # 10. ä¿å­˜æ–‡ä»¶åˆ°active_learningç›®å½•
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¿å­˜å®Œæ•´æ•°æ®é›†ä½œä¸ºè®­ç»ƒCSV
        complete_enhanced_csv = self.active_dir / f"complete_enhanced_iter_{self.state.iteration}.csv"
        all_enhanced_df.to_csv(complete_enhanced_csv, index=False)

        # ä¿å­˜åˆ†å‰²æ–‡ä»¶
        active_train_split = self.active_dir / "train_split.csv"
        active_val_split = self.active_dir / "val_split.csv"

        enhanced_train_df.to_csv(active_train_split, index=False)
        enhanced_val_df.to_csv(active_val_split, index=False)

        # 11. åˆ›å»ºæ•°æ®è·¯å¾„æ˜ å°„æ–‡ä»¶
        mapping_file = self._create_data_path_mapping(annotations, pseudo_labels)

        # 12. ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨å®Œæ•´æ•°æ®é›†CSVä½œä¸ºtrain_csv
        enhanced_config = self.config["data"].copy()
        enhanced_config["params"]["train_csv"] = str(complete_enhanced_csv)  # å®Œæ•´æ•°æ®é›†
        enhanced_config["params"]["cross_directory_mapping"] = str(mapping_file)

        # åˆ›å»ºæ•°æ®æ¨¡å—
        datamodule = instantiate_from_config(enhanced_config)

        # 13. ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"ğŸ“Š Enhanced dataset statistics:")
        logger.info(f"  - Original train: {len(original_train_df)} samples")
        logger.info(f"  - Original val: {len(original_val_df)} samples")
        logger.info(f"  - New samples: {len(new_samples)} samples")
        logger.info(f"    â””â”€ Added to train: {n_new_train} samples")
        logger.info(f"    â””â”€ Added to val: {n_new_val} samples")
        logger.info(f"  - Enhanced train: {len(enhanced_train_df)} samples (+{n_new_train})")
        logger.info(f"  - Enhanced val: {len(enhanced_val_df)} samples (+{n_new_val})")
        logger.info(f"  - Complete dataset: {len(all_enhanced_df)} samples")

        return datamodule

    def _create_data_path_mapping(self, annotations: List[Dict], pseudo_labels: List[Dict]) -> Path:
        """åˆ›å»ºæ•°æ®è·¯å¾„æ˜ å°„æ–‡ä»¶ï¼Œå‘Šè¯‰æ•°æ®åŠ è½½å™¨æ–°æ ·æœ¬çš„ä½ç½®"""
        import json

        # åˆ›å»ºè·¯å¾„æ˜ å°„
        path_mapping = {}
        test_data_dir = Path(self.config["data"]["params"]["test_data_dir"])

        # æ·»åŠ äººå·¥æ ‡æ³¨æ ·æœ¬çš„è·¯å¾„æ˜ å°„
        for ann in annotations:
            sample_id = ann["sample_id"]
            source_file = test_data_dir / f"{sample_id}.npy"

            if source_file.exists():
                path_mapping[sample_id] = str(source_file)
                logger.info(f"  ğŸ“ Mapped {sample_id} -> {source_file}")
            else:
                logger.warning(f"  âš ï¸ Source file not found for {sample_id}: {source_file}")

        # æ·»åŠ ä¼ªæ ‡ç­¾æ ·æœ¬çš„è·¯å¾„æ˜ å°„
        for pseudo in pseudo_labels:
            sample_id = pseudo["sample_id"]
            source_file = test_data_dir / f"{sample_id}.npy"

            if source_file.exists():
                path_mapping[sample_id] = str(source_file)
                logger.info(f"  ğŸ“ Mapped pseudo {sample_id} -> {source_file}")
            else:
                logger.warning(f"  âš ï¸ Source file not found for pseudo {sample_id}: {source_file}")

        # ä¿å­˜æ˜ å°„æ–‡ä»¶
        mapping_file = self.active_dir / f"data_path_mapping_iter_{self.state.iteration}.json"
        with open(mapping_file, "w") as f:
            json.dump(path_mapping, f, indent=2)

        logger.info(f"ğŸ“ Data path mapping saved: {mapping_file}")
        logger.info(f"ğŸ“ Successfully mapped {len(path_mapping)} samples from test directory")

        return mapping_file

    def _create_sample_link(self, sample_id: str, source_dir: Path, target_dir: Path):
        """ä¸ºå•ä¸ªæ ·æœ¬åˆ›å»ºç¬¦å·é“¾æ¥æˆ–å¤åˆ¶æ–‡ä»¶"""
        source_file = source_dir / f"{sample_id}.npy"
        target_file = target_dir / f"{sample_id}.npy"

        if not source_file.exists():
            logger.warning(f"  Source file not found: {source_file}")
            return False

        if target_file.exists():
            logger.debug(f"  Target already exists: {target_file}")
            return True

        # å°è¯•åˆ›å»ºç¬¦å·é“¾æ¥ï¼ˆLinux/Macï¼‰
        target_file.symlink_to(source_file)
        logger.debug(f"  Created symlink: {sample_id}")
        return True

    def _create_enhanced_training_csv(self, annotations: List[Dict], pseudo_labels: List[Dict]) -> Path:
        """åˆ›å»ºåŒ…å«æ–°æ ‡æ³¨æ•°æ®çš„å¢å¼ºè®­ç»ƒCSVæ–‡ä»¶"""
        # åŠ è½½åŸå§‹è®­ç»ƒCSV
        original_csv = self.config["data"]["params"]["train_csv"]
        original_df = pd.read_csv(original_csv)

        logger.info(f"ğŸ“Š Original training data: {len(original_df)} samples")

        # å‡†å¤‡æ–°æ ‡æ³¨æ•°æ®
        new_rows = []

        # 1. æ·»åŠ äººå·¥æ ‡æ³¨æ•°æ®
        for ann in annotations:
            sample_id = ann["sample_id"]
            label = ann["label"]

            new_row = {
                "ID": sample_id,  # åŒ¹é…CSVæ ¼å¼ä¸­çš„"ID"åˆ—
                "label": label,
            }
            new_rows.append(new_row)
            logger.debug(f"  Added annotation: {sample_id} -> {label}")

        # 2. æ·»åŠ ä¼ªæ ‡ç­¾æ•°æ®
        for pseudo in pseudo_labels:
            sample_id = pseudo["sample_id"]
            label = pseudo["label"]
            confidence = pseudo.get("confidence", 0.0)

            new_row = {
                "ID": sample_id,
                "label": label,
            }
            new_rows.append(new_row)
            logger.debug(f"  Added pseudo label: {sample_id} -> {label} (conf: {confidence:.3f})")

        # 3. åˆå¹¶æ•°æ®
        if new_rows:
            new_df = pd.DataFrame(new_rows)
            enhanced_df = pd.concat([original_df, new_df], ignore_index=True)

            # å»é‡ï¼ˆä»¥é˜²é‡å¤æ·»åŠ ï¼‰
            before_dedup = len(enhanced_df)
            enhanced_df = enhanced_df.drop_duplicates(subset=["ID"], keep="last")
            after_dedup = len(enhanced_df)

            if before_dedup != after_dedup:
                logger.info(f"ğŸ“Š Removed {before_dedup - after_dedup} duplicate samples")
        else:
            enhanced_df = original_df.copy()
            logger.warning("âš ï¸ No new samples to add!")

        # 4. ä¿å­˜å¢å¼ºçš„CSVæ–‡ä»¶
        enhanced_csv_path = self.active_dir / f"enhanced_train_iter_{self.state.iteration}.csv"
        enhanced_df.to_csv(enhanced_csv_path, index=False)

        # 5. ç»Ÿè®¡ä¿¡æ¯
        added_annotations = len([ann for ann in annotations])
        added_pseudo = len([pl for pl in pseudo_labels])

        logger.info(f"ğŸ“Š Enhanced training data: {len(enhanced_df)} samples (+{len(new_rows)} new)")
        logger.info(f"  - Human annotations: {added_annotations}")
        logger.info(f"  - Pseudo labels: {added_pseudo}")

        # 6. æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
        if "label" in enhanced_df.columns:
            class_counts = enhanced_df["label"].value_counts().sort_index()
            logger.info(f"ğŸ“Š Class distribution: {dict(class_counts)}")

        return enhanced_csv_path

    def _retrain_model(self, datamodule) -> Tuple[pl.LightningModule, Dict[str, Any]]:
        """é‡è®­ç»ƒæ¨¡å‹ï¼ˆåŸºäºå·²æœ‰æ£€æŸ¥ç‚¹è¿›è¡Œfine-tuningï¼‰"""
        logger.info("ğŸ”„ Fine-tuning model from existing checkpoint...")

        # é‡ç”¨ç°æœ‰çš„æ¨¡å‹é…ç½®
        model = instantiate_from_config(self.config["model"])

        # ğŸ”§ ä¿®å¤ï¼šæ‰‹åŠ¨åŠ è½½æ£€æŸ¥ç‚¹æƒé‡ï¼Œè€Œä¸æ˜¯æ¢å¤å®Œæ•´è®­ç»ƒçŠ¶æ€
        logger.info(f"ğŸ“¥ Loading weights from checkpoint: {self.state.checkpoint_path}")
        checkpoint = torch.load(self.state.checkpoint_path, map_location="cpu")

        # åªåŠ è½½æ¨¡å‹æƒé‡ï¼Œä¸æ¢å¤è®­ç»ƒçŠ¶æ€
        if "state_dict" in checkpoint:
            model.load_state_dict(checkpoint["state_dict"])
            logger.info("âœ… Successfully loaded model weights")
        else:
            logger.warning("âš ï¸ No 'state_dict' found in checkpoint, loading as direct state dict")
            model.load_state_dict(checkpoint)

        # é…ç½®é‡è®­ç»ƒçš„è®­ç»ƒå™¨ï¼ˆä½¿ç”¨è¾ƒå°‘epochï¼‰
        trainer_config = self.config.get("trainer", {}).get("params", {}).copy()
        trainer_config["max_epochs"] = 50  # é‡è®­ç»ƒä½¿ç”¨è¾ƒå°‘epoch
        trainer_config["enable_model_summary"] = False  # é‡è®­ç»ƒæ—¶ä¸æ˜¾ç¤ºæ¨¡å‹æ‘˜è¦

        # ğŸ”§ ä¿®å¤ï¼šå…ˆåˆ›å»ºè®­ç»ƒå™¨ï¼ˆä¸åŒ…å«callbacksï¼‰
        trainer = pl.Trainer(**trainer_config)

        # ğŸ”§ ä¿®å¤ï¼šå•ç‹¬åˆ›å»ºé‡è®­ç»ƒä¸“ç”¨çš„å›è°ƒ
        from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping

        callbacks = [
            ModelCheckpoint(
                dirpath=str(self.active_dir / "retrain_checkpoints"),
                filename=f"retrain_iter_{self.state.iteration}_{{epoch:02d}}_{{val_f1:.4f}}",
                monitor="val_f1",
                mode="max",
                save_top_k=1,
                verbose=False,
            ),
            EarlyStopping(monitor="val_f1", patience=10, mode="max", verbose=False),  # é‡è®­ç»ƒæ—¶æ›´å¿«æ—©åœ
        ]

        # ğŸ”§ ä¿®å¤ï¼šç›´æ¥å°†å›è°ƒå¯¹è±¡èµ‹å€¼ç»™è®­ç»ƒå™¨å®ä¾‹ï¼Œè€Œä¸æ˜¯é…ç½®å¯¹è±¡
        trainer.callbacks = callbacks

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸ä½¿ç”¨ckpt_pathå‚æ•°ï¼Œä»å…¨æ–°çš„è®­ç»ƒçŠ¶æ€å¼€å§‹ï¼ˆä½†ä½¿ç”¨é¢„åŠ è½½çš„æƒé‡ï¼‰
        logger.info("ğŸš€ Starting fine-tuning from epoch 0 (with pre-loaded weights)")
        trainer.fit(model, datamodule)  # ä¸ä¼ é€’ckpt_pathï¼Œé‡æ–°å¼€å§‹è®­ç»ƒè®¡æ•°

        # æ›´æ–°çŠ¶æ€ä¸­çš„æ£€æŸ¥ç‚¹è·¯å¾„ä¸ºæ–°çš„æœ€ä½³æ¨¡å‹
        new_checkpoint = callbacks[0].best_model_path
        if new_checkpoint:
            self.state.checkpoint_path = new_checkpoint
            logger.info(f"ğŸ“¤ Updated checkpoint path: {new_checkpoint}")

        # è¿”å›è®­ç»ƒç»“æœ
        training_results = {
            "best_checkpoint": new_checkpoint,
            "previous_checkpoint": self.state.checkpoint_path,
            "retrain_epochs": trainer_config["max_epochs"],
            "final_metrics": {},  # å¯ä»¥æ·»åŠ æœ€ç»ˆéªŒè¯æŒ‡æ ‡
        }

        logger.info("âœ… Model fine-tuning completed")
        return model, training_results


# =============================================================================
# åœ¨ active_steps.py ä¸­æ·»åŠ ç‹¬ç«‹çš„ä¼ªæ ‡ç­¾ç”Ÿæˆç±»
# =============================================================================


class PseudoLabelGenerator(BaseActiveStep):
    """
    æ­¥éª¤3.5ï¼šä¼ªæ ‡ç­¾ç”Ÿæˆï¼ˆå¯é€‰çš„ç‹¬ç«‹æ­¥éª¤ï¼‰

    è®¾è®¡æ€è·¯ï¼š
    - åœ¨uncertainty_estimationä¹‹åï¼Œsample_selectionä¹‹å‰æ‰§è¡Œ
    - ç”Ÿæˆé«˜è´¨é‡çš„ä¼ªæ ‡ç­¾ï¼Œä¸ºæ¨¡å‹æä¾›æ›´å¤šè®­ç»ƒæ•°æ®
    - ä¸ä¸»åŠ¨å­¦ä¹ æµç¨‹å®Œç¾é›†æˆ
    """

    def __init__(self, config: Dict[str, Any], state_path: Optional[str] = None):
        super().__init__(config, state_path)

        # ä¼ªæ ‡ç­¾é…ç½®
        self.pseudo_config = config.get("active_pseudo_learning", {}).get("pseudo_labeling", {})
        self.confidence_threshold = self.pseudo_config.get("confidence_threshold", 0.85)
        self.uncertainty_threshold = self.pseudo_config.get("uncertainty_threshold", 0.1)
        self.max_pseudo_samples = self.pseudo_config.get("max_pseudo_samples", 500)

        logger.info(f"ğŸ·ï¸ PseudoLabelGenerator initialized")
        logger.info(f"ğŸ“Š Confidence threshold: {self.confidence_threshold}")
        logger.info(f"ğŸ“Š Uncertainty threshold: {self.uncertainty_threshold}")
        logger.info(f"ğŸ“Š Max pseudo samples: {self.max_pseudo_samples}")

    def run(self) -> Dict[str, Any]:
        """è¿è¡Œä¼ªæ ‡ç­¾ç”Ÿæˆ"""
        logger.info("ğŸ·ï¸ Starting pseudo label generation...")

        # 1. æ£€æŸ¥å‰ç½®æ¡ä»¶
        if not self.state.uncertainty_scores:
            # å°è¯•ä»æ–‡ä»¶åŠ è½½
            uncertainty_file = self.active_dir / f"uncertainty_scores_iter_{self.state.iteration}.json"
            if uncertainty_file.exists():
                with open(uncertainty_file, "r") as f:
                    self.state.uncertainty_scores = json.load(f)
                logger.info(f"ğŸ“¥ Loaded uncertainty scores from: {uncertainty_file}")
            else:
                raise ValueError("No uncertainty scores found. Please run uncertainty estimation first.")

        # 2. ç”Ÿæˆä¼ªæ ‡ç­¾
        pseudo_labels = self._generate_pseudo_labels()

        # 3. æ›´æ–°çŠ¶æ€
        self.state.pseudo_labels = pseudo_labels  # æ–°å¢çŠ¶æ€å­—æ®µ
        self.save_state()

        # 4. ä¿å­˜ä¼ªæ ‡ç­¾æ–‡ä»¶
        pseudo_labels_file = self._save_pseudo_labels(pseudo_labels)

        logger.info(f"âœ… Pseudo label generation completed")
        logger.info(f"ğŸ·ï¸ Generated {len(pseudo_labels)} pseudo labels")
        logger.info(f"ğŸ“ Pseudo labels saved to: {pseudo_labels_file}")

        return {
            "pseudo_labels": pseudo_labels,
            "pseudo_labels_file": str(pseudo_labels_file),
            "num_pseudo_labels": len(pseudo_labels),
        }

    def _generate_pseudo_labels(self) -> List[Dict]:
        """
        ç”Ÿæˆä¼ªæ ‡ç­¾ - å®Œæ•´å®ç°ç‰ˆæœ¬

        æ ¸å¿ƒæ€è·¯ï¼š
        1. åˆ©ç”¨æ­¥éª¤2ä¸­çš„ä¸ç¡®å®šæ€§è¯„ä¼°ç»“æœ
        2. é€‰æ‹©ç¡®å®šæ€§å¼ºçš„æ ·æœ¬ï¼ˆä½ä¸ç¡®å®šæ€§ï¼‰
        3. ä½¿ç”¨å½“å‰æœ€ä½³æ¨¡å‹è¿›è¡Œæ¨ç†
        4. æ ¹æ®ç½®ä¿¡åº¦é˜ˆå€¼ç”Ÿæˆä¼ªæ ‡ç­¾

        éµå¾ªä¸‰ä¸ªåŸåˆ™ï¼š
        - æœ€å°æ”¹åŠ¨ï¼šé‡ç”¨ç°æœ‰çš„æ¨¡å‹åŠ è½½å’Œæ•°æ®å¤„ç†é€»è¾‘
        - å•ä¸€èŒè´£ï¼šåªè´Ÿè´£ä¼ªæ ‡ç­¾ç”Ÿæˆ
        - æ¸è¿›å¢å¼ºï¼šåœ¨ç°æœ‰uncertainty_scoresåŸºç¡€ä¸Šå®ç°
        """
        logger.info("ğŸ·ï¸ Generating pseudo labels...")

        # 1. æ£€æŸ¥æ˜¯å¦æœ‰ä¸ç¡®å®šæ€§åˆ†æ•°
        if not self.state.uncertainty_scores:
            logger.warning("âš ï¸ No uncertainty scores found, skipping pseudo label generation")
            return []

        # 2. ç­›é€‰ç¡®å®šæ€§å¼ºçš„æ ·æœ¬ï¼ˆä¸é€‰æ‹©çš„é«˜ä¸ç¡®å®šæ€§æ ·æœ¬äº’è¡¥ï¼‰
        candidate_samples = self._select_high_confidence_samples()

        if not candidate_samples:
            logger.info("ğŸ“Š No suitable samples for pseudo labeling")
            return []

        # 3. åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†
        model = self._load_model()
        datamodule = self._setup_datamodule()

        # 4. å¯¹å€™é€‰æ ·æœ¬è¿›è¡Œæ¨ç†
        pseudo_labels = self._inference_pseudo_labels(model, datamodule, candidate_samples)

        logger.info(f"ğŸ·ï¸ Generated {len(pseudo_labels)} pseudo labels from {len(candidate_samples)} candidates")

        return pseudo_labels

    def _select_high_confidence_samples(self) -> List[str]:
        """
        é€‰æ‹©é«˜ç½®ä¿¡åº¦æ ·æœ¬ä½œä¸ºä¼ªæ ‡ç­¾å€™é€‰

        ç­–ç•¥ï¼š
        1. é€‰æ‹©ä¸ç¡®å®šæ€§ä½çš„æ ·æœ¬ï¼ˆä¸ä¸»åŠ¨å­¦ä¹ é€‰æ‹©çš„é«˜ä¸ç¡®å®šæ€§æ ·æœ¬å½¢æˆäº’è¡¥ï¼‰
        2. æ’é™¤å·²ç»è¢«é€‰ä¸­è¿›è¡Œäººå·¥æ ‡æ³¨çš„æ ·æœ¬
        3. åº”ç”¨ç±»åˆ«å¹³è¡¡ç­–ç•¥
        """
        uncertainty_scores = self.state.uncertainty_scores
        uncertainty_threshold = self.pseudo_config.get("uncertainty_threshold", 0.1)

        # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ é˜²æŠ¤æ€§æ£€æŸ¥ï¼Œç¡®ä¿å­—æ®µä¸ä¸º None
        labeled_samples = self.state.labeled_samples if self.state.labeled_samples is not None else []
        selected_samples = self.state.selected_samples if self.state.selected_samples is not None else []

        # æ’é™¤å·²æ ‡æ³¨å’Œå·²é€‰æ‹©çš„æ ·æœ¬
        excluded_samples = set(labeled_samples + selected_samples)

        # ç­›é€‰ä½ä¸ç¡®å®šæ€§æ ·æœ¬
        candidate_samples = [
            sample_id
            for sample_id, uncertainty in uncertainty_scores.items()
            if uncertainty <= uncertainty_threshold and sample_id not in excluded_samples
        ]

        logger.info(
            f"ğŸ“Š Found {len(candidate_samples)} low-uncertainty candidates (threshold: {uncertainty_threshold})"
        )
        logger.info(
            f"ğŸ“Š Excluded {len(excluded_samples)} samples (labeled: {len(labeled_samples)}, selected: {len(selected_samples)})"
        )

        # å¦‚æœæ ·æœ¬è¿‡å¤šï¼ŒæŒ‰ä¸ç¡®å®šæ€§æ’åºé€‰æ‹©æœ€ç¡®å®šçš„
        max_pseudo_samples = self.pseudo_config.get("max_pseudo_samples", 500)
        if len(candidate_samples) > max_pseudo_samples:
            # æŒ‰ä¸ç¡®å®šæ€§ä»ä½åˆ°é«˜æ’åºï¼Œé€‰æ‹©æœ€ç¡®å®šçš„æ ·æœ¬
            sorted_candidates = sorted(candidate_samples, key=lambda x: uncertainty_scores[x])
            candidate_samples = sorted_candidates[:max_pseudo_samples]
            logger.info(f"ğŸ“Š Limited to top {max_pseudo_samples} most certain samples")

        return candidate_samples

    def _load_model(self) -> pl.LightningModule:
        """åŠ è½½æ¨¡å‹ - å¤ç”¨ UncertaintyEstimator çš„å®ç°"""
        logger.info(f"ğŸ“¥ Loading model from: {self.state.checkpoint_path}")

        # é‡ç”¨ç°æœ‰çš„æ¨¡å‹å®ä¾‹åŒ–é€»è¾‘
        model = instantiate_from_config(self.config["model"])

        # æ£€æŸ¥GPUå¯ç”¨æ€§å¹¶è®¾ç½®æ­£ç¡®çš„è®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸ¯ Using device: {device}")

        # æ ¹æ®è®¾å¤‡ç±»å‹åŠ è½½æ£€æŸ¥ç‚¹
        if device == "cuda":
            checkpoint = torch.load(self.state.checkpoint_path, map_location="cuda")
        else:
            checkpoint = torch.load(self.state.checkpoint_path, map_location="cpu")

        model.load_state_dict(checkpoint["state_dict"])

        # æ˜ç¡®ç§»åŠ¨æ¨¡å‹åˆ°GPU
        model = model.to(device)
        model.eval()

        return model

    def _setup_datamodule(self):
        """è®¾ç½®æ•°æ®æ¨¡å— - å¤ç”¨ UncertaintyEstimator çš„å®ç°"""
        # é‡ç”¨ç°æœ‰çš„æ•°æ®æ¨¡å—
        datamodule = instantiate_from_config(self.config["data"])
        datamodule.setup("test")  # ä½¿ç”¨æµ‹è¯•é›†ä½œä¸ºæœªæ ‡æ³¨æ± 
        return datamodule

    def _inference_pseudo_labels(
        self, model: pl.LightningModule, datamodule, candidate_samples: List[str]
    ) -> List[Dict]:
        """
        å¯¹å€™é€‰æ ·æœ¬è¿›è¡Œæ¨ç†ç”Ÿæˆä¼ªæ ‡ç­¾

        Args:
            model: å·²åŠ è½½çš„æ¨¡å‹
            datamodule: æ•°æ®æ¨¡å—
            candidate_samples: å€™é€‰æ ·æœ¬IDåˆ—è¡¨

        Returns:
            åŒ…å«ä¼ªæ ‡ç­¾ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        logger.info(f"ğŸ”® Running inference on {len(candidate_samples)} candidate samples...")

        pseudo_labels = []
        confidence_threshold = self.pseudo_config.get("confidence_threshold", 0.85)
        device = next(model.parameters()).device

        # åˆ›å»ºå€™é€‰æ ·æœ¬çš„æ•°æ®åŠ è½½å™¨
        candidate_dataloader = self._create_candidate_dataloader(datamodule, candidate_samples)

        model.eval()
        with torch.no_grad():
            for batch_idx, batch in enumerate(candidate_dataloader):
                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†ä¸åŒçš„batchæ ¼å¼
                try:
                    if isinstance(batch, dict):
                        # Dictæ ¼å¼ï¼š{key: tensor, ...}
                        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
                        images = batch.get("image") or batch.get("images") or batch.get("data")
                    elif isinstance(batch, (list, tuple)) and len(batch) >= 1:
                        # List/Tupleæ ¼å¼ï¼š[images, labels] æˆ– [images]
                        images = batch[0].to(device)
                        if len(batch) > 1:
                            # å¦‚æœæœ‰æ ‡ç­¾ï¼Œä¹Ÿç§»åŠ¨åˆ°è®¾å¤‡ä¸Šï¼ˆè™½ç„¶åœ¨æ¨ç†ä¸­ä¸éœ€è¦ï¼‰
                            labels = batch[1].to(device) if isinstance(batch[1], torch.Tensor) else batch[1]
                    else:
                        # ç›´æ¥æ˜¯tensoræ ¼å¼
                        images = batch.to(device)

                    if images is None:
                        logger.warning(f"âš ï¸ Could not extract images from batch at index {batch_idx}")
                        continue

                    # ğŸ”§ éªŒè¯å›¾åƒtensorçš„æ ¼å¼
                    if len(images.shape) != 4:  # åº”è¯¥æ˜¯ [batch_size, channels, height, width]
                        logger.warning(f"âš ï¸ Unexpected image shape: {images.shape} at batch {batch_idx}")
                        continue

                    batch_size = images.shape[0]
                    logger.debug(f"ğŸ”„ Processing batch {batch_idx}: {batch_size} samples, shape: {images.shape}")

                except Exception as e:
                    logger.error(f"âŒ Error processing batch {batch_idx}: {e}")
                    continue

                try:
                    # æ¨¡å‹æ¨ç†
                    outputs = model(images)

                    # ğŸ”§ ä¿®å¤ï¼šæå–é¢„æµ‹æ¦‚ç‡çš„é€»è¾‘
                    if hasattr(outputs, "logits"):
                        logits = outputs.logits
                    elif isinstance(outputs, dict) and "logits" in outputs:
                        logits = outputs["logits"]
                    elif isinstance(outputs, dict) and "predictions" in outputs:
                        logits = outputs["predictions"]
                    else:
                        # ç›´æ¥æ˜¯logits tensor
                        logits = outputs

                    # ğŸ”§ å¤„ç†ä¸åŒçš„logitsæ ¼å¼
                    if logits.dim() == 1:
                        # äºŒåˆ†ç±»å•è¾“å‡ºï¼š[batch_size] -> [batch_size, 2]
                        probabilities = torch.stack([1 - torch.sigmoid(logits), torch.sigmoid(logits)], dim=1)
                    elif logits.dim() == 2 and logits.shape[1] == 1:
                        # äºŒåˆ†ç±»å•è¾“å‡ºï¼š[batch_size, 1] -> [batch_size, 2]
                        sigmoid_probs = torch.sigmoid(logits.squeeze(1))
                        probabilities = torch.stack([1 - sigmoid_probs, sigmoid_probs], dim=1)
                    elif logits.dim() == 2 and logits.shape[1] == 2:
                        # äºŒåˆ†ç±»åŒè¾“å‡ºï¼š[batch_size, 2]
                        probabilities = torch.softmax(logits, dim=1)
                    else:
                        logger.warning(f"âš ï¸ Unexpected logits shape: {logits.shape}")
                        continue

                    # å¤„ç†æ¯ä¸ªæ ·æœ¬
                    for i in range(batch_size):
                        sample_idx = batch_idx * candidate_dataloader.batch_size + i
                        if sample_idx >= len(candidate_samples):
                            break

                        sample_id = candidate_samples[sample_idx]
                        prob = probabilities[i]

                        # è·å–æœ€é«˜ç½®ä¿¡åº¦çš„ç±»åˆ«
                        max_prob, predicted_class = torch.max(prob, dim=0)
                        confidence = max_prob.item()

                        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç½®ä¿¡åº¦é˜ˆå€¼
                        if confidence >= confidence_threshold:
                            pseudo_labels.append(
                                {
                                    "sample_id": sample_id,
                                    "label": predicted_class.item(),
                                    "confidence": confidence,
                                    "uncertainty": self.state.uncertainty_scores.get(sample_id, 0.0),
                                    "source": "pseudo_label",
                                }
                            )

                            # ğŸ”§ è°ƒè¯•ä¿¡æ¯
                            if len(pseudo_labels) <= 5:  # åªæ˜¾ç¤ºå‰å‡ ä¸ª
                                logger.debug(
                                    f"  âœ… Added pseudo label: {sample_id} -> class {predicted_class.item()} (conf: {confidence:.3f})"
                                )

                except Exception as e:
                    logger.error(f"âŒ Error during model inference for batch {batch_idx}: {e}")
                    continue

            # åº”ç”¨ç±»åˆ«å¹³è¡¡ç­–ç•¥
            if self.pseudo_config.get("use_class_balance", True) and pseudo_labels:
                pseudo_labels = self._apply_class_balance(pseudo_labels)

            logger.info(f"âœ… Generated {len(pseudo_labels)} high-confidence pseudo labels")
            logger.info(f"ğŸ“Š Confidence threshold: {confidence_threshold}")

            return pseudo_labels

    def _create_candidate_dataloader(self, datamodule, candidate_samples: List[str]) -> DataLoader:
        """
        ä¸ºå€™é€‰æ ·æœ¬åˆ›å»ºæ•°æ®åŠ è½½å™¨

        é‡ç”¨ç°æœ‰çš„æ•°æ®å¤„ç†é€»è¾‘ï¼Œç¡®ä¿ä¸è®­ç»ƒæµç¨‹ä¸€è‡´
        """
        from torch.utils.data import Subset

        # è·å–å®Œæ•´çš„æµ‹è¯•æ•°æ®é›†ï¼ˆä½œä¸ºæœªæ ‡æ³¨æ± ï¼‰
        full_dataset = datamodule.test_dataset

        # åˆ›å»ºæ ·æœ¬IDåˆ°ç´¢å¼•çš„æ˜ å°„
        sample_to_idx = {}
        for idx in range(len(full_dataset)):
            sample_id = full_dataset.data_index.iloc[idx]["ID"]
            sample_to_idx[sample_id] = idx

        # è·å–å€™é€‰æ ·æœ¬çš„ç´¢å¼•
        candidate_indices = []
        for sample_id in candidate_samples:
            if sample_id in sample_to_idx:
                candidate_indices.append(sample_to_idx[sample_id])
            else:
                logger.warning(f"âš ï¸ Sample {sample_id} not found in dataset")

        if not candidate_indices:
            raise ValueError("No valid candidate samples found in dataset")

        # åˆ›å»ºå­é›†
        candidate_subset = Subset(full_dataset, candidate_indices)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        candidate_dataloader = DataLoader(
            candidate_subset,
            batch_size=self.config.get("data", {}).get("batch_size", 32),
            shuffle=False,  # ä¿æŒé¡ºåºä»¥ä¾¿åŒ¹é…sample_id
            num_workers=self.config.get("data", {}).get("num_workers", 4),
            pin_memory=True,
        )

        logger.info(f"ğŸ“¦ Created dataloader for {len(candidate_indices)} candidate samples")
        return candidate_dataloader

    def _apply_class_balance(self, pseudo_labels: List[Dict]) -> List[Dict]:
        """
        åº”ç”¨ç±»åˆ«å¹³è¡¡ç­–ç•¥

        ç¡®ä¿ä¼ªæ ‡ç­¾åœ¨ä¸åŒç±»åˆ«é—´ç›¸å¯¹å¹³è¡¡ï¼Œé¿å…æ¨¡å‹åå‘æŸä¸€ç±»åˆ«
        """
        if not pseudo_labels:
            return pseudo_labels

        # ç»Ÿè®¡å„ç±»åˆ«çš„ä¼ªæ ‡ç­¾æ•°é‡
        class_counts = {}
        for pseudo in pseudo_labels:
            label = pseudo["label"]
            class_counts[label] = class_counts.get(label, 0) + 1

        logger.info(f"ğŸ“Š Original pseudo label distribution: {class_counts}")

        # è®¡ç®—å¹³è¡¡åçš„ç›®æ ‡æ•°é‡ï¼ˆå–æœ€å°ç±»åˆ«çš„æ•°é‡ï¼‰
        min_count = min(class_counts.values())
        max_pseudo_per_class = max(min_count, self.pseudo_config.get("min_pseudo_per_class", 10))

        # æŒ‰ç±»åˆ«åˆ†ç»„å¹¶é™åˆ¶æ•°é‡
        balanced_pseudo_labels = []
        class_samples = {label: [] for label in class_counts.keys()}

        # å°†ä¼ªæ ‡ç­¾æŒ‰ç±»åˆ«åˆ†ç»„
        for pseudo in pseudo_labels:
            class_samples[pseudo["label"]].append(pseudo)

        # æ¯ä¸ªç±»åˆ«é€‰æ‹©æœ€é«˜ç½®ä¿¡åº¦çš„æ ·æœ¬
        for label, samples in class_samples.items():
            # æŒ‰ç½®ä¿¡åº¦æ’åº
            sorted_samples = sorted(samples, key=lambda x: x["confidence"], reverse=True)
            selected_samples = sorted_samples[:max_pseudo_per_class]
            balanced_pseudo_labels.extend(selected_samples)

        # ç»Ÿè®¡å¹³è¡¡åçš„åˆ†å¸ƒ
        final_counts = {}
        for pseudo in balanced_pseudo_labels:
            label = pseudo["label"]
            final_counts[label] = final_counts.get(label, 0) + 1

        logger.info(f"ğŸ“Š Balanced pseudo label distribution: {final_counts}")

        return balanced_pseudo_labels

    def _save_pseudo_labels(self, pseudo_labels: List[Dict]) -> Path:
        """ä¿å­˜ä¼ªæ ‡ç­¾åˆ°æ–‡ä»¶"""
        pseudo_labels_file = self.active_dir / f"pseudo_labels_iter_{self.state.iteration}.json"

        # åˆ›å»ºè¯¦ç»†çš„ä¼ªæ ‡ç­¾æ–‡ä»¶
        pseudo_labels_data = {
            "iteration": self.state.iteration,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "experiment_name": self.state.experiment_name,
            "config": {
                "confidence_threshold": self.confidence_threshold,
                "uncertainty_threshold": self.uncertainty_threshold,
                "max_pseudo_samples": self.max_pseudo_samples,
            },
            "pseudo_labels": pseudo_labels,
            "statistics": self._compute_pseudo_label_stats(pseudo_labels),
        }

        with open(pseudo_labels_file, "w", encoding="utf-8") as f:
            json.dump(pseudo_labels_data, f, indent=2, ensure_ascii=False)

        return pseudo_labels_file

    def _compute_pseudo_label_stats(self, pseudo_labels: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—ä¼ªæ ‡ç­¾ç»Ÿè®¡ä¿¡æ¯"""
        if not pseudo_labels:
            return {}

        # ç±»åˆ«åˆ†å¸ƒ
        class_counts = {}
        confidence_sum = {}
        uncertainty_sum = {}

        for pseudo in pseudo_labels:
            label = pseudo["label"]
            confidence = pseudo["confidence"]
            uncertainty = pseudo["uncertainty"]

            class_counts[label] = class_counts.get(label, 0) + 1
            confidence_sum[label] = confidence_sum.get(label, 0) + confidence
            uncertainty_sum[label] = uncertainty_sum.get(label, 0) + uncertainty

        # è®¡ç®—å¹³å‡å€¼
        avg_confidence = {label: confidence_sum[label] / class_counts[label] for label in class_counts}
        avg_uncertainty = {label: uncertainty_sum[label] / class_counts[label] for label in class_counts}

        return {
            "class_distribution": class_counts,
            "average_confidence": avg_confidence,
            "average_uncertainty": avg_uncertainty,
            "total_samples": len(pseudo_labels),
            "overall_avg_confidence": sum(p["confidence"] for p in pseudo_labels) / len(pseudo_labels),
            "overall_avg_uncertainty": sum(p["uncertainty"] for p in pseudo_labels) / len(pseudo_labels),
        }


# =============================================================================
# åœ¨ ActiveLearningStepManager ä¸­æ·»åŠ æ–°çš„æ–¹æ³•
# =============================================================================


class ActiveLearningStepManager:
    """ä¸»åŠ¨å­¦ä¹ æ­¥éª¤ç®¡ç†å™¨ - æ·»åŠ ä¼ªæ ‡ç­¾ç”Ÿæˆæ”¯æŒ"""

    @staticmethod
    def run_uncertainty_estimation(config: Dict[str, Any], state_path: Optional[str] = None) -> Dict[str, Any]:
        """è¿è¡Œä¸ç¡®å®šæ€§ä¼°è®¡æ­¥éª¤"""
        estimator = UncertaintyEstimator(config, state_path)
        return estimator.run()

    @staticmethod
    def run_pseudo_labeling(config: Dict[str, Any], state_path: Optional[str] = None) -> Dict[str, Any]:
        """è¿è¡Œä¼ªæ ‡ç­¾ç”Ÿæˆæ­¥éª¤ï¼ˆæ–°å¢ï¼‰"""
        generator = PseudoLabelGenerator(config, state_path)
        return generator.run()

    @staticmethod
    def run_sample_selection(config: Dict[str, Any], state_path: Optional[str] = None) -> Dict[str, Any]:
        """è¿è¡Œæ ·æœ¬é€‰æ‹©æ­¥éª¤"""
        selector = SampleSelector(config, state_path)
        return selector.run()

    @staticmethod
    def run_retraining(
        config: Dict[str, Any], state_path: Optional[str] = None, annotation_file: Optional[str] = None
    ) -> Dict[str, Any]:
        """è¿è¡Œæ¨¡å‹é‡è®­ç»ƒæ­¥éª¤"""
        retrainer = ActiveRetrainer(config, state_path, annotation_file)
        return retrainer.run()
