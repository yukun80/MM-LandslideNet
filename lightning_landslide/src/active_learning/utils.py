# =============================================================================
# lightning_landslide/src/active_learning/utils.py
# =============================================================================

"""
ä¸»åŠ¨å­¦ä¹ å·¥å…·å‡½æ•°

æä¾›ä¸»åŠ¨å­¦ä¹ è¿‡ç¨‹ä¸­å¸¸ç”¨çš„å·¥å…·å‡½æ•°å’Œè¾…åŠ©ç±»ã€‚
"""

import torch
import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Any, Optional, Union
from pathlib import Path
import json
import pickle
import time
from datetime import datetime
from functools import wraps
import psutil
import gc

logger = logging.getLogger(__name__)


class Timer:
    """ç®€å•çš„è®¡æ—¶å™¨ç±»"""

    def __init__(self, name: str = "Timer"):
        self.name = name
        self.start_time = None
        self.end_time = None

    def __enter__(self):
        self.start_time = time.time()
        logger.debug(f"â±ï¸ {self.name} started")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end_time = time.time()
        duration = self.end_time - self.start_time
        logger.info(f"â±ï¸ {self.name} completed in {duration:.2f}s")

    def elapsed(self) -> float:
        """è·å–å·²ç»è¿‡çš„æ—¶é—´"""
        if self.start_time is None:
            return 0.0
        current_time = time.time()
        return current_time - self.start_time


class MemoryMonitor:
    """å†…å­˜ä½¿ç”¨ç›‘æ§å™¨"""

    def __init__(self, name: str = "MemoryMonitor"):
        self.name = name
        self.initial_memory = None

    def __enter__(self):
        self.initial_memory = self._get_memory_usage()
        logger.debug(f"ğŸ§  {self.name} - Initial memory: {self.initial_memory:.1f} MB")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        current_memory = self._get_memory_usage()
        memory_diff = current_memory - self.initial_memory
        logger.info(f"ğŸ§  {self.name} - Memory change: {memory_diff:+.1f} MB (Current: {current_memory:.1f} MB)")

    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024

    def current_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡"""
        return self._get_memory_usage()


def memory_cleanup():
    """æ¸…ç†å†…å­˜"""
    logger.debug("ğŸ§¹ Performing memory cleanup...")
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


def timing_decorator(func):
    """è®¡æ—¶è£…é¥°å™¨"""

    @wraps(func)
    def wrapper(*args, **kwargs):
        with Timer(f"{func.__name__}"):
            return func(*args, **kwargs)

    return wrapper


def memory_monitor_decorator(func):
    """å†…å­˜ç›‘æ§è£…é¥°å™¨"""

    @wraps(func)
    def wrapper(*args, **kwargs):
        with MemoryMonitor(f"{func.__name__}"):
            result = func(*args, **kwargs)
            memory_cleanup()
            return result

    return wrapper


def safe_json_serialize(obj: Any) -> str:
    """å®‰å…¨çš„JSONåºåˆ—åŒ–"""

    def json_serializer(o):
        if isinstance(o, np.ndarray):
            return o.tolist()
        elif isinstance(o, (np.integer, np.floating)):
            return o.item()
        elif isinstance(o, Path):
            return str(o)
        elif isinstance(o, datetime):
            return o.isoformat()
        elif hasattr(o, "__dict__"):
            return o.__dict__
        else:
            return str(o)

    try:
        return json.dumps(obj, default=json_serializer, indent=2)
    except Exception as e:
        logger.warning(f"JSON serialization failed: {e}")
        return str(obj)


def calculate_class_weights(labels: List[int]) -> Dict[int, float]:
    """
    è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†ä¸å¹³è¡¡æ•°æ®

    Args:
        labels: æ ‡ç­¾åˆ—è¡¨

    Returns:
        ç±»åˆ«æƒé‡å­—å…¸
    """
    from collections import Counter
    import numpy as np

    label_counts = Counter(labels)
    total_samples = len(labels)
    n_classes = len(label_counts)

    # è®¡ç®—å¹³è¡¡æƒé‡
    weights = {}
    for label, count in label_counts.items():
        weight = total_samples / (n_classes * count)
        weights[label] = weight

    logger.info(f"ğŸ“Š Calculated class weights: {weights}")
    return weights


def stratified_split_indices(
    labels: List[int], test_size: float = 0.2, random_state: int = None
) -> Tuple[List[int], List[int]]:
    """
    åˆ†å±‚æŠ½æ ·åˆ†å‰²æ•°æ®ç´¢å¼•

    Args:
        labels: æ ‡ç­¾åˆ—è¡¨
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        random_state: éšæœºç§å­

    Returns:
        (è®­ç»ƒç´¢å¼•, æµ‹è¯•ç´¢å¼•) å…ƒç»„
    """
    from sklearn.model_selection import train_test_split

    indices = list(range(len(labels)))
    train_indices, test_indices = train_test_split(
        indices, test_size=test_size, stratify=labels, random_state=random_state
    )

    return train_indices, test_indices


def compute_confidence_intervals(scores: List[float], confidence: float = 0.95) -> Tuple[float, float, float]:
    """
    è®¡ç®—ç½®ä¿¡åŒºé—´

    Args:
        scores: åˆ†æ•°åˆ—è¡¨
        confidence: ç½®ä¿¡æ°´å¹³

    Returns:
        (å‡å€¼, ä¸‹ç•Œ, ä¸Šç•Œ) å…ƒç»„
    """
    import scipy.stats as stats

    scores = np.array(scores)
    mean = np.mean(scores)
    sem = stats.sem(scores)  # æ ‡å‡†è¯¯å·®

    # è®¡ç®—ç½®ä¿¡åŒºé—´
    alpha = 1 - confidence
    df = len(scores) - 1
    t_critical = stats.t.ppf(1 - alpha / 2, df)

    margin_error = t_critical * sem
    lower_bound = mean - margin_error
    upper_bound = mean + margin_error

    return mean, lower_bound, upper_bound


def normalize_uncertainty_scores(scores: np.ndarray, method: str = "minmax") -> np.ndarray:
    """
    å½’ä¸€åŒ–ä¸ç¡®å®šæ€§åˆ†æ•°

    Args:
        scores: ä¸ç¡®å®šæ€§åˆ†æ•°æ•°ç»„
        method: å½’ä¸€åŒ–æ–¹æ³• ("minmax", "zscore", "robust")

    Returns:
        å½’ä¸€åŒ–åçš„åˆ†æ•°
    """
    if method == "minmax":
        min_score, max_score = scores.min(), scores.max()
        if max_score > min_score:
            return (scores - min_score) / (max_score - min_score)
        else:
            return np.zeros_like(scores)

    elif method == "zscore":
        mean, std = scores.mean(), scores.std()
        if std > 0:
            return (scores - mean) / std
        else:
            return np.zeros_like(scores)

    elif method == "robust":
        q25, q75 = np.percentile(scores, [25, 75])
        median = np.median(scores)
        iqr = q75 - q25
        if iqr > 0:
            return (scores - median) / iqr
        else:
            return np.zeros_like(scores)

    else:
        raise ValueError(f"Unknown normalization method: {method}")


def create_experiment_summary(results: Dict[str, Any], output_path: Path) -> str:
    """
    åˆ›å»ºå®éªŒæ‘˜è¦

    Args:
        results: å®éªŒç»“æœå­—å…¸
        output_path: è¾“å‡ºè·¯å¾„

    Returns:
        æ‘˜è¦æ–‡ä»¶è·¯å¾„
    """
    summary = {
        "experiment_info": {
            "timestamp": datetime.now().isoformat(),
            "experiment_name": results.get("experiment_name", "unknown"),
            "version": results.get("version", "unknown"),
        },
        "performance_summary": {
            "best_performance": max(results.get("performance_history", {}).get("val_f1", [0])),
            "final_performance": (
                results.get("performance_history", {}).get("val_f1", [0])[-1]
                if results.get("performance_history", {}).get("val_f1")
                else 0
            ),
            "convergence_iteration": results.get("convergence_iteration", 0),
            "total_training_time": results.get("total_training_time", 0),
        },
        "data_summary": {
            "initial_samples": (
                results.get("data_usage_history", {}).get("training_samples", [0])[0]
                if results.get("data_usage_history", {}).get("training_samples")
                else 0
            ),
            "final_samples": (
                results.get("data_usage_history", {}).get("training_samples", [0])[-1]
                if results.get("data_usage_history", {}).get("training_samples")
                else 0
            ),
            "pseudo_labels_used": (
                results.get("data_usage_history", {}).get("pseudo_labels", [0])[-1]
                if results.get("data_usage_history", {}).get("pseudo_labels")
                else 0
            ),
            "new_annotations": (
                results.get("data_usage_history", {}).get("new_annotations", [0])[-1]
                if results.get("data_usage_history", {}).get("new_annotations")
                else 0
            ),
        },
    }

    # ä¿å­˜æ‘˜è¦
    summary_path = output_path / "experiment_summary.json"
    with open(summary_path, "w") as f:
        json.dump(summary, f, indent=2, default=str)

    logger.info(f"ğŸ“‹ Experiment summary saved: {summary_path}")
    return str(summary_path)


def validate_data_paths(config: Dict[str, Any]) -> bool:
    """
    éªŒè¯æ•°æ®è·¯å¾„çš„æœ‰æ•ˆæ€§

    Args:
        config: é…ç½®å­—å…¸

    Returns:
        éªŒè¯æ˜¯å¦é€šè¿‡
    """
    data_config = config.get("data", {}).get("params", {})

    required_paths = [
        ("train_data_dir", "è®­ç»ƒæ•°æ®ç›®å½•"),
        ("test_data_dir", "æµ‹è¯•æ•°æ®ç›®å½•"),
        ("train_csv", "è®­ç»ƒæ ‡ç­¾æ–‡ä»¶"),
        ("test_csv", "æµ‹è¯•æ ‡ç­¾æ–‡ä»¶"),
    ]

    all_valid = True

    for path_key, description in required_paths:
        if path_key not in data_config:
            logger.error(f"âŒ Missing {description} in configuration")
            all_valid = False
            continue

        path = Path(data_config[path_key])
        if not path.exists():
            logger.error(f"âŒ {description} not found: {path}")
            all_valid = False
        else:
            logger.debug(f"âœ… {description} found: {path}")

    return all_valid


def estimate_training_time(config: Dict[str, Any], sample_size: int = 100) -> float:
    """
    ä¼°ç®—è®­ç»ƒæ—¶é—´

    Args:
        config: é…ç½®å­—å…¸
        sample_size: ä¼°ç®—æ ·æœ¬å¤§å°

    Returns:
        é¢„ä¼°è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰
    """
    # ç®€å•çš„æ—¶é—´ä¼°ç®—å…¬å¼
    base_time_per_epoch = 60  # ç§’
    max_epochs = config.get("trainer", {}).get("params", {}).get("max_epochs", 30)
    max_iterations = config.get("active_pseudo_learning", {}).get("max_iterations", 5)

    # è€ƒè™‘ä¸»åŠ¨å­¦ä¹ çš„é¢å¤–å¼€é”€
    active_learning_overhead = 1.5  # 50%çš„é¢å¤–å¼€é”€

    estimated_time = base_time_per_epoch * max_epochs * max_iterations * active_learning_overhead

    logger.info(f"â° Estimated training time: {estimated_time/60:.1f} minutes")
    return estimated_time


def create_debug_checkpoint(data: Dict[str, Any], checkpoint_dir: Path, name: str = "debug") -> str:
    """
    åˆ›å»ºè°ƒè¯•æ£€æŸ¥ç‚¹

    Args:
        data: è¦ä¿å­˜çš„æ•°æ®
        checkpoint_dir: æ£€æŸ¥ç‚¹ç›®å½•
        name: æ£€æŸ¥ç‚¹åç§°

    Returns:
        æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
    """
    checkpoint_dir = Path(checkpoint_dir)
    checkpoint_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    checkpoint_path = checkpoint_dir / f"{name}_checkpoint_{timestamp}.pkl"

    try:
        with open(checkpoint_path, "wb") as f:
            pickle.dump(data, f)

        logger.info(f"ğŸ› Debug checkpoint saved: {checkpoint_path}")
        return str(checkpoint_path)

    except Exception as e:
        logger.error(f"Failed to save debug checkpoint: {e}")
        return ""


def load_debug_checkpoint(checkpoint_path: str) -> Dict[str, Any]:
    """
    åŠ è½½è°ƒè¯•æ£€æŸ¥ç‚¹

    Args:
        checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„

    Returns:
        åŠ è½½çš„æ•°æ®
    """
    try:
        with open(checkpoint_path, "rb") as f:
            data = pickle.load(f)

        logger.info(f"ğŸ› Debug checkpoint loaded: {checkpoint_path}")
        return data

    except Exception as e:
        logger.error(f"Failed to load debug checkpoint: {e}")
        return {}
