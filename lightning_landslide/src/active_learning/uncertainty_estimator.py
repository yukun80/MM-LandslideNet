# =============================================================================
# lightning_landslide/src/active_learning/uncertainty_estimator.py
# =============================================================================

"""
ä¸ç¡®å®šæ€§ä¼°è®¡å™¨ - è¯„ä¼°æ¨¡å‹é¢„æµ‹çš„ä¸ç¡®å®šæ€§

è¿™ä¸ªæ¨¡å—å®ç°äº†å¤šç§ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•ï¼Œç”¨äºä¸»åŠ¨å­¦ä¹ ä¸­çš„æ ·æœ¬é€‰æ‹©ã€‚
è®¾è®¡æ€è·¯ï¼šç»“åˆè®¤çŸ¥ä¸ç¡®å®šæ€§å’Œå¶ç„¶ä¸ç¡®å®šæ€§ï¼Œå…¨é¢è¯„ä¼°æ¨¡å‹çš„ç½®ä¿¡åº¦ã€‚

æ ¸å¿ƒæ–¹æ³•ï¼š
1. MC Dropout: é€šè¿‡å¤šæ¬¡dropoutæ¨ç†ä¼°è®¡è®¤çŸ¥ä¸ç¡®å®šæ€§
2. Deep Ensemble: å¤šæ¨¡å‹æŠ•ç¥¨è¯„ä¼°æ¨¡å‹ä¸ç¡®å®šæ€§
3. Temperature Scaling: æ ¡å‡†ç½®ä¿¡åº¦åˆ†æ•°
4. Prediction Entropy: è¡¡é‡é¢„æµ‹åˆ†å¸ƒçš„ä¸ç¡®å®šæ€§
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Union, Callable
from pathlib import Path
import json
from dataclasses import dataclass
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)


@dataclass
class UncertaintyResults:
    """ä¸ç¡®å®šæ€§ä¼°è®¡ç»“æœå®¹å™¨"""

    sample_ids: List[str]
    predictions: np.ndarray  # (N, num_classes)
    uncertainty_scores: np.ndarray  # (N,)
    epistemic_uncertainty: np.ndarray  # è®¤çŸ¥ä¸ç¡®å®šæ€§
    aleatoric_uncertainty: np.ndarray  # å¶ç„¶ä¸ç¡®å®šæ€§
    prediction_entropy: np.ndarray  # é¢„æµ‹ç†µ
    confidence_scores: np.ndarray  # ç½®ä¿¡åº¦åˆ†æ•°
    calibrated_confidence: np.ndarray  # æ ¡å‡†åç½®ä¿¡åº¦


class BaseUncertaintyEstimator(ABC):
    """ä¸ç¡®å®šæ€§ä¼°è®¡å™¨åŸºç±»"""

    def __init__(self, device: str = "auto"):
        self.device = (
            torch.device("cuda" if torch.cuda.is_available() else "cpu") if device == "auto" else torch.device(device)
        )

    @abstractmethod
    def estimate_uncertainty(self, model, dataloader, **kwargs) -> UncertaintyResults:
        """ä¼°è®¡ä¸ç¡®å®šæ€§çš„æŠ½è±¡æ–¹æ³•"""
        pass

    def _compute_entropy(self, probs: np.ndarray) -> np.ndarray:
        """è®¡ç®—é¢„æµ‹ç†µ"""
        # é¿å…log(0)çš„æƒ…å†µ
        probs = np.clip(probs, 1e-8, 1.0)
        return -np.sum(probs * np.log(probs), axis=1)

    def _compute_confidence(self, probs: np.ndarray) -> np.ndarray:
        """è®¡ç®—ç½®ä¿¡åº¦(æœ€å¤§æ¦‚ç‡)"""
        return np.max(probs, axis=1)


class MCDropoutEstimator(BaseUncertaintyEstimator):
    """
    Monte Carlo Dropoutä¸ç¡®å®šæ€§ä¼°è®¡å™¨

    åŸç†ï¼šåœ¨æ¨ç†æ—¶ä¿æŒdropoutå¼€å¯ï¼Œé€šè¿‡å¤šæ¬¡å‰å‘ä¼ æ’­çš„å˜åŒ–
    æ¥ä¼°è®¡æ¨¡å‹çš„è®¤çŸ¥ä¸ç¡®å®šæ€§ã€‚
    """

    def __init__(
        self, n_forward_passes: int = 50, use_temperature_scaling: bool = False, device: str = "auto"  # æ·»åŠ è¿™ä¸ªå‚æ•°
    ):
        super().__init__(device)
        self.n_forward_passes = n_forward_passes
        self.use_temperature_scaling = use_temperature_scaling  # ä¿å­˜å‚æ•°

        # åˆå§‹åŒ–æ¸©åº¦ç¼©æ”¾å™¨
        if self.use_temperature_scaling:
            self.temperature_scaler = TemperatureScaling()
        else:
            self.temperature_scaler = None

    def fit_temperature_scaling(self, model, val_dataloader):
        """æ‹Ÿåˆæ¸©åº¦ç¼©æ”¾å‚æ•°"""
        if not self.use_temperature_scaling or self.temperature_scaler is None:
            return

        logger.info("ğŸ“ Fitting temperature scaling...")

        model.eval()
        all_logits = []
        all_labels = []

        with torch.no_grad():
            for data, labels in val_dataloader:
                data, labels = data.to(self.device), labels.to(self.device)
                logits = model(data)
                all_logits.append(logits)
                all_labels.append(labels)

        all_logits = torch.cat(all_logits, dim=0)
        all_labels = torch.cat(all_labels, dim=0)

        self.temperature_scaler.fit(all_logits, all_labels)

    def estimate_uncertainty(self, model, dataloader, **kwargs) -> UncertaintyResults:
        """ä½¿ç”¨MC Dropoutä¼°è®¡ä¸ç¡®å®šæ€§"""
        logger.info(f"ğŸ² Running MC Dropout with {self.n_forward_passes} forward passes...")

        # === å…³é”®ä¿®å¤ï¼šç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š ===
        model = model.to(self.device)
        model.eval()
        self._enable_dropout(model)

        all_predictions = []
        all_sample_ids = []

        with torch.no_grad():
            for batch_idx, (data, labels) in enumerate(dataloader):
                # === å…³é”®ä¿®å¤ï¼šç¡®ä¿æ•°æ®åœ¨æ­£ç¡®è®¾å¤‡ä¸Š ===
                data = data.to(self.device)
                sample_ids = [f"sample_{batch_idx}_{i}" for i in range(len(data))]

                batch_predictions = []
                for _ in range(self.n_forward_passes):
                    logits = model(data)

                    if self.use_temperature_scaling and self.temperature_scaler is not None:
                        probs = self.temperature_scaler.calibrate(logits)
                    else:
                        probs = F.softmax(logits, dim=1)

                    batch_predictions.append(probs.cpu().numpy())

                batch_predictions = np.stack(batch_predictions, axis=0)
                all_predictions.append(batch_predictions)
                all_sample_ids.extend(sample_ids)

        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„é¢„æµ‹
        all_predictions = np.concatenate(all_predictions, axis=1)  # (n_passes, total_samples, num_classes)

        # è®¡ç®—å„ç§ä¸ç¡®å®šæ€§æŒ‡æ ‡
        mean_predictions = np.mean(all_predictions, axis=0)  # (total_samples, num_classes)
        prediction_variance = np.var(all_predictions, axis=0)  # (total_samples, num_classes)

        # è®¤çŸ¥ä¸ç¡®å®šæ€§ï¼šå¤šæ¬¡é¢„æµ‹çš„æ–¹å·®
        epistemic_uncertainty = np.mean(prediction_variance, axis=1)

        # å¶ç„¶ä¸ç¡®å®šæ€§ï¼šå¹³å‡é¢„æµ‹çš„ç†µ
        aleatoric_uncertainty = self._compute_entropy(mean_predictions)

        # é¢„æµ‹ç†µ
        prediction_entropy = self._compute_entropy(mean_predictions)

        # ç½®ä¿¡åº¦
        confidence_scores = self._compute_confidence(mean_predictions)

        # æ€»ä¸ç¡®å®šæ€§ï¼šè®¤çŸ¥ + å¶ç„¶
        uncertainty_scores = epistemic_uncertainty + aleatoric_uncertainty

        # æ ¡å‡†åç½®ä¿¡åº¦ï¼ˆå¦‚æœä½¿ç”¨æ¸©åº¦ç¼©æ”¾ï¼‰
        calibrated_confidence = confidence_scores  # å·²ç»åœ¨å‰å‘ä¼ æ’­ä¸­åº”ç”¨äº†æ¸©åº¦ç¼©æ”¾

        self._disable_dropout(model)  # æ¢å¤æ­£å¸¸çŠ¶æ€

        return UncertaintyResults(
            sample_ids=all_sample_ids,
            predictions=mean_predictions,
            uncertainty_scores=uncertainty_scores,
            epistemic_uncertainty=epistemic_uncertainty,
            aleatoric_uncertainty=aleatoric_uncertainty,
            prediction_entropy=prediction_entropy,
            confidence_scores=confidence_scores,
            calibrated_confidence=calibrated_confidence,
        )

    def _enable_dropout(self, model):
        """åœ¨æ¨ç†æ—¶å¯ç”¨dropout"""
        for module in model.modules():
            if isinstance(module, nn.Dropout):
                module.train()

    def _disable_dropout(self, model):
        """æ¢å¤æ­£å¸¸æ¨ç†çŠ¶æ€"""
        model.eval()


class DeepEnsembleEstimator(BaseUncertaintyEstimator):
    """
    Deep Ensembleä¸ç¡®å®šæ€§ä¼°è®¡å™¨

    åŸç†ï¼šè®­ç»ƒå¤šä¸ªä¸åŒåˆå§‹åŒ–çš„æ¨¡å‹ï¼Œé€šè¿‡æ¨¡å‹é—´çš„é¢„æµ‹å·®å¼‚
    æ¥ä¼°è®¡ä¸ç¡®å®šæ€§ã€‚
    """

    def __init__(self, model_paths: List[str], device: str = "auto"):
        super().__init__(device)
        self.model_paths = model_paths
        self.models = []

    def load_ensemble_models(self, model_class, **model_kwargs):
        """åŠ è½½é›†æˆæ¨¡å‹"""
        logger.info(f"ğŸ¯ Loading {len(self.model_paths)} models for ensemble...")

        for path in self.model_paths:
            model = model_class(**model_kwargs)
            checkpoint = torch.load(path, map_location=self.device)
            model.load_state_dict(checkpoint["state_dict"])
            model.to(self.device)
            model.eval()
            self.models.append(model)

        logger.info(f"âœ“ Loaded {len(self.models)} models for ensemble")

    def estimate_uncertainty(self, model=None, dataloader=None, **kwargs) -> UncertaintyResults:
        """ä½¿ç”¨æ¨¡å‹é›†æˆä¼°è®¡ä¸ç¡®å®šæ€§"""
        if not self.models:
            raise ValueError("No models loaded. Call load_ensemble_models() first.")

        logger.info(f"ğŸª Running Deep Ensemble with {len(self.models)} models...")

        all_predictions = []
        all_sample_ids = []

        with torch.no_grad():
            for batch_idx, (data, labels) in enumerate(dataloader):
                data = data.to(self.device)
                sample_ids = [f"sample_{batch_idx}_{i}" for i in range(len(data))]

                # æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹
                batch_predictions = []
                for model in self.models:
                    outputs = model(data)
                    probs = F.softmax(outputs, dim=1)
                    batch_predictions.append(probs.cpu().numpy())

                batch_predictions = np.stack(batch_predictions, axis=0)  # (n_models, batch_size, num_classes)
                all_predictions.append(batch_predictions)
                all_sample_ids.extend(sample_ids)

        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        all_predictions = np.concatenate(all_predictions, axis=1)  # (n_models, total_samples, num_classes)

        # è®¡ç®—ä¸ç¡®å®šæ€§æŒ‡æ ‡
        mean_predictions = np.mean(all_predictions, axis=0)
        prediction_variance = np.var(all_predictions, axis=0)

        # æ¨¡å‹é—´çš„ä¸ä¸€è‡´æ€§ä½œä¸ºä¸ç¡®å®šæ€§
        epistemic_uncertainty = np.mean(prediction_variance, axis=1)
        aleatoric_uncertainty = self._compute_entropy(mean_predictions)
        prediction_entropy = self._compute_entropy(mean_predictions)
        confidence_scores = self._compute_confidence(mean_predictions)
        uncertainty_scores = epistemic_uncertainty + aleatoric_uncertainty

        return UncertaintyResults(
            sample_ids=all_sample_ids,
            predictions=mean_predictions,
            uncertainty_scores=uncertainty_scores,
            epistemic_uncertainty=epistemic_uncertainty,
            aleatoric_uncertainty=aleatoric_uncertainty,
            prediction_entropy=prediction_entropy,
            confidence_scores=confidence_scores,
            calibrated_confidence=confidence_scores,
        )


class TemperatureScaling:
    """
    æ¸©åº¦ç¼©æ”¾æ ¡å‡†å™¨

    åŸç†ï¼šé€šè¿‡å­¦ä¹ ä¸€ä¸ªæ¸©åº¦å‚æ•°Tï¼Œå°†æ¨¡å‹è¾“å‡ºé™¤ä»¥Tæ¥æ ¡å‡†ç½®ä¿¡åº¦ï¼Œ
    ä½¿å¾—é¢„æµ‹æ¦‚ç‡æ›´å‡†ç¡®åœ°åæ˜ çœŸå®çš„æ­£ç¡®ç‡ã€‚
    """

    def __init__(self):
        self.temperature = nn.Parameter(torch.ones(1))
        self.is_fitted = False

    def fit(self, logits: torch.Tensor, labels: torch.Tensor, max_iter: int = 50):
        """åœ¨éªŒè¯é›†ä¸Šæ‹Ÿåˆæ¸©åº¦å‚æ•°"""
        logger.info("ğŸŒ¡ï¸ Fitting temperature scaling...")

        optimizer = torch.optim.LBFGS([self.temperature], lr=0.01, max_iter=max_iter)

        def eval_loss():
            optimizer.zero_grad()
            loss = F.cross_entropy(logits / self.temperature, labels)
            loss.backward()
            return loss

        optimizer.step(eval_loss)
        self.is_fitted = True

        logger.info(f"âœ“ Temperature scaling fitted: T = {self.temperature.item():.3f}")

    def calibrate(self, logits: torch.Tensor) -> torch.Tensor:
        """åº”ç”¨æ¸©åº¦ç¼©æ”¾"""
        if not self.is_fitted:
            logger.warning("Temperature scaling not fitted, using T=1.0")
            return F.softmax(logits, dim=1)

        return F.softmax(logits / self.temperature, dim=1)


class HybridUncertaintyEstimator(BaseUncertaintyEstimator):
    """
    æ··åˆä¸ç¡®å®šæ€§ä¼°è®¡å™¨

    ç»“åˆå¤šç§æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œæä¾›æ›´robustçš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚
    """

    def __init__(
        self,
        use_mc_dropout: bool = True,
        n_forward_passes: int = 30,
        use_temperature_scaling: bool = True,
        device: str = "auto",
    ):
        super().__init__(device)
        self.use_mc_dropout = use_mc_dropout
        self.n_forward_passes = n_forward_passes
        self.use_temperature_scaling = use_temperature_scaling

        if use_mc_dropout:
            self.mc_estimator = MCDropoutEstimator(n_forward_passes, device)
        if use_temperature_scaling:
            self.temperature_scaler = TemperatureScaling()

    def fit_temperature_scaling(self, model, val_dataloader):
        """æ‹Ÿåˆæ¸©åº¦ç¼©æ”¾å‚æ•°"""
        if not self.use_temperature_scaling:
            return

        logger.info("ğŸ“ Fitting temperature scaling on validation set...")

        model.eval()
        all_logits = []
        all_labels = []

        with torch.no_grad():
            for data, labels in val_dataloader:
                data, labels = data.to(self.device), labels.to(self.device)
                logits = model(data)
                all_logits.append(logits)
                all_labels.append(labels)

        all_logits = torch.cat(all_logits, dim=0)
        all_labels = torch.cat(all_labels, dim=0)

        self.temperature_scaler.fit(all_logits, all_labels)

    def estimate_uncertainty(self, model, dataloader, **kwargs) -> UncertaintyResults:
        """ä½¿ç”¨æ··åˆæ–¹æ³•ä¼°è®¡ä¸ç¡®å®šæ€§"""
        logger.info("ğŸ”€ Running hybrid uncertainty estimation...")

        if self.use_mc_dropout:
            # ä½¿ç”¨MC Dropout
            mc_results = self.mc_estimator.estimate_uncertainty(model, dataloader)
            return mc_results
        else:
            # å•æ¬¡æ¨ç† + æ¸©åº¦ç¼©æ”¾
            model.eval()
            all_predictions = []
            all_sample_ids = []

            with torch.no_grad():
                for batch_idx, (data, labels) in enumerate(dataloader):
                    data = data.to(self.device)
                    sample_ids = [f"sample_{batch_idx}_{i}" for i in range(len(data))]

                    logits = model(data)

                    if self.use_temperature_scaling:
                        probs = self.temperature_scaler.calibrate(logits)
                    else:
                        probs = F.softmax(logits, dim=1)

                    all_predictions.append(probs.cpu().numpy())
                    all_sample_ids.extend(sample_ids)

            predictions = np.concatenate(all_predictions, axis=0)

            # è®¡ç®—åŸºäºç†µçš„ä¸ç¡®å®šæ€§
            prediction_entropy = self._compute_entropy(predictions)
            confidence_scores = self._compute_confidence(predictions)

            # ç®€å•çš„ä¸ç¡®å®šæ€§ä¼°è®¡
            uncertainty_scores = prediction_entropy

            return UncertaintyResults(
                sample_ids=all_sample_ids,
                predictions=predictions,
                uncertainty_scores=uncertainty_scores,
                epistemic_uncertainty=uncertainty_scores * 0.5,  # ç®€åŒ–
                aleatoric_uncertainty=uncertainty_scores * 0.5,
                prediction_entropy=prediction_entropy,
                confidence_scores=confidence_scores,
                calibrated_confidence=confidence_scores,
            )


def create_uncertainty_estimator(method: str, **kwargs) -> BaseUncertaintyEstimator:
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºä¸ç¡®å®šæ€§ä¼°è®¡å™¨"""
    estimators = {
        "mc_dropout": MCDropoutEstimator,
        "deep_ensemble": DeepEnsembleEstimator,
        "hybrid": HybridUncertaintyEstimator,
    }

    if method not in estimators:
        raise ValueError(f"Unknown uncertainty method: {method}. Available: {list(estimators.keys())}")

    return estimators[method](**kwargs)
