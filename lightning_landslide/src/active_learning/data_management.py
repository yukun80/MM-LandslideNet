# =============================================================================
# lightning_landslide/src/active_learning/data_management.py
# =============================================================================

"""
æ•°æ®ç®¡ç†å’Œäººå·¥æ ‡æ³¨æ¥å£

è¿™ä¸ªæ¨¡å—æä¾›äº†ä¸»åŠ¨å­¦ä¹ è¿‡ç¨‹ä¸­å¤æ‚çš„æ•°æ®çŠ¶æ€ç®¡ç†åŠŸèƒ½ï¼š
1. æ•°æ®ç‰ˆæœ¬æ§åˆ¶å’Œè¿½è¸ª
2. ä¼ªæ ‡ç­¾æ•°æ®é›†çš„åˆ›å»ºå’Œç®¡ç†
3. äººå·¥æ ‡æ³¨æ¥å£çš„æŠ½è±¡å’Œå®ç°
4. æ•°æ®è´¨é‡æ§åˆ¶å’ŒéªŒè¯
5. å¢é‡æ•°æ®é›†çš„é«˜æ•ˆç®¡ç†
"""

import torch
import torch.utils.data as data
import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Optional, Union, Any
from pathlib import Path
import json
import shutil
from datetime import datetime
from abc import ABC, abstractmethod
from dataclasses import dataclass, asdict
import pickle
from collections import defaultdict

logger = logging.getLogger(__name__)


@dataclass
class DataSample:
    """æ•°æ®æ ·æœ¬çš„ç»Ÿä¸€è¡¨ç¤º"""

    sample_id: str
    file_path: str
    label: Optional[int] = None
    confidence: Optional[float] = None
    source: str = "original"  # "original", "pseudo", "annotated"
    iteration: int = 0
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class DatasetVersion:
    """æ•°æ®é›†ç‰ˆæœ¬ä¿¡æ¯"""

    version_id: str
    iteration: int
    timestamp: str
    total_samples: int
    original_samples: int
    pseudo_samples: int
    annotated_samples: int
    class_distribution: Dict[int, int]
    description: str
    performance_snapshot: Dict[str, float] = None

    def __post_init__(self):
        if self.performance_snapshot is None:
            self.performance_snapshot = {}


class BaseAnnotationInterface(ABC):
    """
    äººå·¥æ ‡æ³¨æ¥å£åŸºç±»

    è¿™ä¸ªæŠ½è±¡ç±»å®šä¹‰äº†äººå·¥æ ‡æ³¨çš„æ ‡å‡†æ¥å£ï¼Œ
    å¯ä»¥å¯¹æ¥ä¸åŒçš„æ ‡æ³¨å·¥å…·æˆ–ç³»ç»Ÿã€‚
    """

    @abstractmethod
    def request_annotation(self, sample_ids: List[str], sample_paths: List[str]) -> Dict[str, int]:
        """
        è¯·æ±‚äººå·¥æ ‡æ³¨

        Args:
            sample_ids: æ ·æœ¬IDåˆ—è¡¨
            sample_paths: æ ·æœ¬æ–‡ä»¶è·¯å¾„åˆ—è¡¨

        Returns:
            {sample_id: label} çš„æ ‡æ³¨ç»“æœå­—å…¸
        """
        pass

    @abstractmethod
    def get_annotation_status(self, request_id: str) -> str:
        """è·å–æ ‡æ³¨çŠ¶æ€: 'pending', 'completed', 'failed'"""
        pass

    @abstractmethod
    def is_available(self) -> bool:
        """æ£€æŸ¥æ ‡æ³¨æœåŠ¡æ˜¯å¦å¯ç”¨"""
        pass


class SimulatedAnnotationInterface(BaseAnnotationInterface):
    """
    æ¨¡æ‹Ÿæ ‡æ³¨æ¥å£

    ç”¨äºæµ‹è¯•å’Œæ¼”ç¤ºï¼Œé€šè¿‡é¢„å®šä¹‰çš„çœŸå®æ ‡ç­¾æ¥æ¨¡æ‹Ÿäººå·¥æ ‡æ³¨è¿‡ç¨‹ã€‚
    åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œåº”è¯¥æ›¿æ¢ä¸ºçœŸå®çš„æ ‡æ³¨æ¥å£ã€‚
    """

    def __init__(self, ground_truth_file: str, simulation_delay: float = 0.1):
        """
        åˆå§‹åŒ–æ¨¡æ‹Ÿæ ‡æ³¨æ¥å£

        Args:
            ground_truth_file: çœŸå®æ ‡ç­¾æ–‡ä»¶è·¯å¾„
            simulation_delay: æ¨¡æ‹Ÿæ ‡æ³¨å»¶è¿Ÿï¼ˆç§’ï¼‰
        """
        self.ground_truth = self._load_ground_truth(ground_truth_file)
        self.simulation_delay = simulation_delay
        self.annotation_requests = {}

        logger.info(f"ğŸ“ Simulated annotation interface initialized with {len(self.ground_truth)} ground truth labels")

    def _load_ground_truth(self, file_path: str) -> Dict[str, int]:
        """åŠ è½½çœŸå®æ ‡ç­¾"""
        try:
            if Path(file_path).suffix == ".csv":
                df = pd.read_csv(file_path)
                # å‡è®¾CSVæ ¼å¼ä¸º: ID, label
                return dict(zip(df["ID"].astype(str), df["label"]))
            elif Path(file_path).suffix == ".json":
                with open(file_path, "r") as f:
                    return json.load(f)
            else:
                logger.warning(f"Unsupported ground truth file format: {file_path}")
                return {}
        except Exception as e:
            logger.error(f"Failed to load ground truth: {e}")
            return {}

    def request_annotation(self, sample_ids: List[str], sample_paths: List[str]) -> Dict[str, int]:
        """æ¨¡æ‹Ÿæ ‡æ³¨è¿‡ç¨‹"""
        import time

        time.sleep(self.simulation_delay)  # æ¨¡æ‹Ÿæ ‡æ³¨æ—¶é—´

        annotations = {}
        for sample_id in sample_ids:
            if sample_id in self.ground_truth:
                annotations[sample_id] = self.ground_truth[sample_id]
            else:
                # å¦‚æœæ²¡æœ‰çœŸå®æ ‡ç­¾ï¼Œéšæœºåˆ†é…ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰
                annotations[sample_id] = np.random.randint(0, 2)
                logger.warning(f"No ground truth for {sample_id}, using random label")

        logger.info(f"ğŸ“ Simulated annotation completed for {len(annotations)} samples")
        return annotations

    def get_annotation_status(self, request_id: str) -> str:
        return "completed"  # æ¨¡æ‹Ÿæ ‡æ³¨ç«‹å³å®Œæˆ

    def is_available(self) -> bool:
        return True


class WebAnnotationInterface(BaseAnnotationInterface):
    """
    Webæ ‡æ³¨æ¥å£

    å¯¹æ¥åŸºäºWebçš„æ ‡æ³¨å¹³å°ï¼Œå¦‚LabelStudioã€Prodigyç­‰ã€‚
    è¿™é‡Œæä¾›äº†ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶ã€‚
    """

    def __init__(self, api_endpoint: str, api_key: str, project_id: str):
        self.api_endpoint = api_endpoint
        self.api_key = api_key
        self.project_id = project_id
        self.pending_requests = {}

    def request_annotation(self, sample_ids: List[str], sample_paths: List[str]) -> Dict[str, int]:
        """é€šè¿‡Web APIè¯·æ±‚æ ‡æ³¨"""
        # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„APIè°ƒç”¨
        logger.info(f"ğŸŒ Requesting web annotation for {len(sample_ids)} samples")

        # ç¤ºä¾‹å®ç° - å®é™…ä¸­éœ€è¦æ ¹æ®å…·ä½“APIä¿®æ”¹
        # import requests
        #
        # payload = {
        #     "samples": [{"id": sid, "path": path} for sid, path in zip(sample_ids, sample_paths)],
        #     "project_id": self.project_id
        # }
        #
        # response = requests.post(
        #     f"{self.api_endpoint}/annotation/request",
        #     json=payload,
        #     headers={"Authorization": f"Bearer {self.api_key}"}
        # )
        #
        # return response.json()["annotations"]

        # æš‚æ—¶è¿”å›ç©ºå­—å…¸ï¼Œå®é™…å®ç°æ—¶éœ€è¦å®Œå–„
        return {}

    def get_annotation_status(self, request_id: str) -> str:
        # å®é™…å®ç°ä¸­æŸ¥è¯¢APIçŠ¶æ€
        return "pending"

    def is_available(self) -> bool:
        # å®é™…å®ç°ä¸­æ£€æŸ¥APIè¿æ¥
        return False


class EnhancedDataManager:
    """
    å¢å¼ºçš„æ•°æ®ç®¡ç†å™¨

    æä¾›å®Œæ•´çš„æ•°æ®ç‰ˆæœ¬æ§åˆ¶ã€è´¨é‡ç®¡ç†å’Œå¢é‡æ›´æ–°åŠŸèƒ½ã€‚
    """

    def __init__(self, base_config: Dict, output_dir: Path, annotation_interface: BaseAnnotationInterface = None):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨

        Args:
            base_config: åŸºç¡€æ•°æ®é…ç½®
            output_dir: è¾“å‡ºç›®å½•
            annotation_interface: æ ‡æ³¨æ¥å£å®ä¾‹
        """
        self.base_config = base_config
        self.output_dir = Path(output_dir)
        self.annotation_interface = annotation_interface

        # æ•°æ®çŠ¶æ€ç®¡ç†
        self.data_samples = {}  # {sample_id: DataSample}
        self.dataset_versions = []
        self.current_version = None

        # ç›®å½•ç»“æ„
        self.data_dir = self.output_dir / "data_management"
        self.versions_dir = self.data_dir / "versions"
        self.annotations_dir = self.data_dir / "annotations"
        self.metadata_dir = self.data_dir / "metadata"

        self._setup_directories()
        self._initialize_original_data()

        logger.info("ğŸ“Š Enhanced data manager initialized")

    def _setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        for dir_path in [self.data_dir, self.versions_dir, self.annotations_dir, self.metadata_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

    def _initialize_original_data(self):
        """åˆå§‹åŒ–åŸå§‹æ•°æ®"""
        logger.info("ğŸ“‚ Initializing original training data...")

        # åŠ è½½åŸå§‹è®­ç»ƒæ•°æ®ä¿¡æ¯
        train_csv = Path(self.base_config["data"]["params"]["train_csv"])
        if train_csv.exists():
            df = pd.read_csv(train_csv)

            for _, row in df.iterrows():
                sample = DataSample(
                    sample_id=str(row["ID"]),
                    file_path=str(Path(self.base_config["data"]["params"]["train_data_dir"]) / f"{row['ID']}.npy"),
                    label=int(row["label"]) if "label" in row else None,
                    source="original",
                    iteration=0,
                )
                self.data_samples[sample.sample_id] = sample

        # åˆ›å»ºåˆå§‹ç‰ˆæœ¬
        self._create_version_snapshot(0, "Initial training data")

        logger.info(f"âœ… Loaded {len(self.data_samples)} original samples")

    def add_pseudo_labels(
        self, pseudo_samples: List[Dict], iteration: int, confidence_scores: Dict[str, float] = None
    ) -> int:
        """
        æ·»åŠ ä¼ªæ ‡ç­¾æ ·æœ¬

        Args:
            pseudo_samples: ä¼ªæ ‡ç­¾æ ·æœ¬åˆ—è¡¨
            iteration: å½“å‰è¿­ä»£è½®æ¬¡
            confidence_scores: ç½®ä¿¡åº¦åˆ†æ•°å­—å…¸

        Returns:
            æˆåŠŸæ·»åŠ çš„æ ·æœ¬æ•°é‡
        """
        logger.info(f"ğŸ·ï¸ Adding {len(pseudo_samples)} pseudo-labeled samples for iteration {iteration}")

        added_count = 0
        confidence_scores = confidence_scores or {}

        for sample_info in pseudo_samples:
            sample_id = sample_info["sample_id"]
            predicted_label = sample_info["predicted_label"]

            # æ£€æŸ¥æ ·æœ¬æ˜¯å¦å·²å­˜åœ¨
            if sample_id in self.data_samples and self.data_samples[sample_id].source != "original":
                logger.warning(f"Sample {sample_id} already exists with source {self.data_samples[sample_id].source}")
                continue

            # åˆ›å»ºä¼ªæ ‡ç­¾æ ·æœ¬
            sample = DataSample(
                sample_id=sample_id,
                file_path=self._get_test_sample_path(sample_id),
                label=predicted_label,
                confidence=confidence_scores.get(sample_id, sample_info.get("confidence", 0.0)),
                source="pseudo",
                iteration=iteration,
                metadata={
                    "quality_score": sample_info.get("quality_score", 0.0),
                    "uncertainty": sample_info.get("uncertainty", 0.0),
                },
            )

            self.data_samples[sample_id] = sample
            added_count += 1

        logger.info(f"âœ… Successfully added {added_count} pseudo-labeled samples")
        return added_count

    def request_annotations(self, sample_ids: List[str], iteration: int, timeout: float = 300.0) -> Dict[str, int]:
        """
        è¯·æ±‚äººå·¥æ ‡æ³¨

        Args:
            sample_ids: éœ€è¦æ ‡æ³¨çš„æ ·æœ¬IDåˆ—è¡¨
            iteration: å½“å‰è¿­ä»£è½®æ¬¡
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            æ ‡æ³¨ç»“æœå­—å…¸
        """
        if not self.annotation_interface or not self.annotation_interface.is_available():
            logger.warning("No annotation interface available, skipping annotation request")
            return {}

        logger.info(f"ğŸ“ Requesting annotations for {len(sample_ids)} samples")

        # å‡†å¤‡æ ·æœ¬è·¯å¾„
        sample_paths = [self._get_test_sample_path(sid) for sid in sample_ids]

        # å‘èµ·æ ‡æ³¨è¯·æ±‚
        try:
            annotations = self.annotation_interface.request_annotation(sample_ids, sample_paths)

            # ä¿å­˜æ ‡æ³¨ç»“æœ
            self._save_annotation_results(annotations, iteration)

            # æ›´æ–°æ•°æ®æ ·æœ¬
            self._update_samples_with_annotations(annotations, iteration)

            logger.info(f"âœ… Received annotations for {len(annotations)} samples")
            return annotations

        except Exception as e:
            logger.error(f"âŒ Annotation request failed: {e}")
            return {}

    def _update_samples_with_annotations(self, annotations: Dict[str, int], iteration: int):
        """ç”¨æ ‡æ³¨ç»“æœæ›´æ–°æ ·æœ¬"""
        for sample_id, label in annotations.items():
            if sample_id in self.data_samples:
                # æ›´æ–°ç°æœ‰æ ·æœ¬
                sample = self.data_samples[sample_id]
                sample.label = label
                sample.source = "annotated"
                sample.iteration = iteration
            else:
                # åˆ›å»ºæ–°çš„æ ‡æ³¨æ ·æœ¬
                sample = DataSample(
                    sample_id=sample_id,
                    file_path=self._get_test_sample_path(sample_id),
                    label=label,
                    source="annotated",
                    iteration=iteration,
                )
                self.data_samples[sample_id] = sample

    def create_combined_dataset(self, iteration: int) -> "CombinedDataset":
        """
        åˆ›å»ºåˆå¹¶çš„æ•°æ®é›†

        Args:
            iteration: å½“å‰è¿­ä»£è½®æ¬¡

        Returns:
            åŒ…å«æ‰€æœ‰æ•°æ®çš„ç»„åˆæ•°æ®é›†
        """
        logger.info(f"ğŸ”— Creating combined dataset for iteration {iteration}")

        # ç­›é€‰æœ‰æ•ˆæ ·æœ¬ï¼ˆæœ‰æ ‡ç­¾çš„æ ·æœ¬ï¼‰
        valid_samples = [sample for sample in self.data_samples.values() if sample.label is not None]

        # æŒ‰æ¥æºåˆ†ç±»
        by_source = defaultdict(list)
        for sample in valid_samples:
            by_source[sample.source].append(sample)

        logger.info(f"ğŸ“Š Dataset composition:")
        for source, samples in by_source.items():
            logger.info(f"  {source}: {len(samples)} samples")

        # åˆ›å»ºç‰ˆæœ¬å¿«ç…§
        self._create_version_snapshot(iteration, f"Combined dataset for iteration {iteration}")

        return CombinedDataset(valid_samples, self.base_config)

    def _create_version_snapshot(self, iteration: int, description: str):
        """åˆ›å»ºæ•°æ®ç‰ˆæœ¬å¿«ç…§"""
        version_id = f"v{iteration}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # ç»Ÿè®¡æ•°æ®
        by_source = defaultdict(int)
        class_dist = defaultdict(int)

        for sample in self.data_samples.values():
            if sample.label is not None:
                by_source[sample.source] += 1
                class_dist[sample.label] += 1

        version = DatasetVersion(
            version_id=version_id,
            iteration=iteration,
            timestamp=datetime.now().isoformat(),
            total_samples=sum(by_source.values()),
            original_samples=by_source["original"],
            pseudo_samples=by_source["pseudo"],
            annotated_samples=by_source["annotated"],
            class_distribution=dict(class_dist),
            description=description,
        )

        self.dataset_versions.append(version)
        self.current_version = version

        # ä¿å­˜ç‰ˆæœ¬ä¿¡æ¯
        version_file = self.versions_dir / f"{version_id}.json"
        with open(version_file, "w") as f:
            json.dump(asdict(version), f, indent=2)

        # ä¿å­˜æ ·æœ¬è¯¦æƒ…
        samples_file = self.versions_dir / f"{version_id}_samples.pkl"
        with open(samples_file, "wb") as f:
            pickle.dump(dict(self.data_samples), f)

        logger.info(f"ğŸ“¸ Created version snapshot: {version_id}")

    def _get_test_sample_path(self, sample_id: str) -> str:
        """è·å–æµ‹è¯•æ ·æœ¬çš„æ–‡ä»¶è·¯å¾„"""
        test_data_dir = Path(self.base_config["data"]["params"]["test_data_dir"])
        return str(test_data_dir / f"{sample_id}.npy")

    def _save_annotation_results(self, annotations: Dict[str, int], iteration: int):
        """ä¿å­˜æ ‡æ³¨ç»“æœ"""
        annotation_file = self.annotations_dir / f"iteration_{iteration}_annotations.json"

        annotation_data = {
            "iteration": iteration,
            "timestamp": datetime.now().isoformat(),
            "annotations": annotations,
            "count": len(annotations),
        }

        with open(annotation_file, "w") as f:
            json.dump(annotation_data, f, indent=2)

    def get_data_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        by_source = defaultdict(int)
        by_iteration = defaultdict(int)
        class_dist = defaultdict(int)

        for sample in self.data_samples.values():
            if sample.label is not None:
                by_source[sample.source] += 1
                by_iteration[sample.iteration] += 1
                class_dist[sample.label] += 1

        return {
            "total_samples": len([s for s in self.data_samples.values() if s.label is not None]),
            "by_source": dict(by_source),
            "by_iteration": dict(by_iteration),
            "class_distribution": dict(class_dist),
            "versions_count": len(self.dataset_versions),
            "current_version": self.current_version.version_id if self.current_version else None,
        }

    def export_training_data(self, output_path: Path, iteration: int = None):
        """
        å¯¼å‡ºè®­ç»ƒæ•°æ®ä¸ºæ ‡å‡†æ ¼å¼

        Args:
            output_path: è¾“å‡ºè·¯å¾„
            iteration: æŒ‡å®šè¿­ä»£è½®æ¬¡ï¼ŒNoneè¡¨ç¤ºå¯¼å‡ºæ‰€æœ‰æ•°æ®
        """
        logger.info(f"ğŸ“¤ Exporting training data to {output_path}")

        # ç­›é€‰æ•°æ®
        if iteration is not None:
            samples = [s for s in self.data_samples.values() if s.label is not None and s.iteration <= iteration]
        else:
            samples = [s for s in self.data_samples.values() if s.label is not None]

        # åˆ›å»ºDataFrame
        data_records = []
        for sample in samples:
            record = {
                "ID": sample.sample_id,
                "label": sample.label,
                "source": sample.source,
                "iteration": sample.iteration,
                "confidence": sample.confidence or 0.0,
                "file_path": sample.file_path,
            }
            if sample.metadata:
                record.update(sample.metadata)
            data_records.append(record)

        df = pd.DataFrame(data_records)
        df.to_csv(output_path, index=False)

        logger.info(f"âœ… Exported {len(samples)} samples to {output_path}")


class CombinedDataset(data.Dataset):
    """
    ç»„åˆæ•°æ®é›†ç±»

    å°†åŸå§‹æ•°æ®ã€ä¼ªæ ‡ç­¾æ•°æ®å’Œæ–°æ ‡æ³¨æ•°æ®åˆå¹¶ä¸ºä¸€ä¸ªç»Ÿä¸€çš„æ•°æ®é›†ã€‚
    """

    def __init__(self, samples: List[DataSample], base_config: Dict):
        """
        åˆå§‹åŒ–ç»„åˆæ•°æ®é›†

        Args:
            samples: æ•°æ®æ ·æœ¬åˆ—è¡¨
            base_config: åŸºç¡€é…ç½®
        """
        self.samples = samples
        self.base_config = base_config

        # åˆ›å»ºç´¢å¼•æ˜ å°„
        self.sample_ids = [s.sample_id for s in samples]
        self.labels = [s.label for s in samples]

        # æ•°æ®å˜æ¢ï¼ˆå¤ç”¨åŸæœ‰çš„å˜æ¢é€»è¾‘ï¼‰
        self.transform = self._create_transform()

        logger.info(f"ğŸ”— CombinedDataset created with {len(samples)} samples")
        self._log_composition()

    def _create_transform(self):
        """åˆ›å»ºæ•°æ®å˜æ¢"""
        # è¿™é‡Œåº”è¯¥å¤ç”¨åŸæœ‰çš„æ•°æ®å˜æ¢é€»è¾‘
        # ç®€åŒ–å®ç°ï¼Œå®é™…ä¸­éœ€è¦æ ¹æ®å…·ä½“çš„å˜æ¢é…ç½®
        return None

    def _log_composition(self):
        """è®°å½•æ•°æ®é›†ç»„æˆ"""
        by_source = defaultdict(int)
        by_class = defaultdict(int)

        for sample in self.samples:
            by_source[sample.source] += 1
            by_class[sample.label] += 1

        logger.info("ğŸ“Š Dataset composition:")
        for source, count in by_source.items():
            logger.info(f"  {source}: {count} samples")

        logger.info("ğŸ“Š Class distribution:")
        for cls, count in by_class.items():
            logger.info(f"  class {cls}: {count} samples")

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """è·å–å•ä¸ªæ ·æœ¬"""
        sample = self.samples[idx]

        try:
            # åŠ è½½æ•°æ®æ–‡ä»¶
            if Path(sample.file_path).exists():
                data = np.load(sample.file_path)
                data = torch.from_numpy(data).float()
            else:
                logger.warning(f"File not found: {sample.file_path}, using dummy data")
                data = torch.zeros((5, 64, 64))  # é»˜è®¤å½¢çŠ¶

            # æ ‡ç­¾
            label = torch.tensor(sample.label, dtype=torch.long)

            # åº”ç”¨å˜æ¢
            if self.transform:
                data = self.transform(data)

            return data, label

        except Exception as e:
            logger.error(f"Error loading sample {sample.sample_id}: {e}")
            # è¿”å›dummyæ•°æ®
            return torch.zeros((5, 64, 64)), torch.tensor(0, dtype=torch.long)


def create_annotation_interface(interface_type: str, **kwargs) -> BaseAnnotationInterface:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºæ ‡æ³¨æ¥å£

    Args:
        interface_type: æ¥å£ç±»å‹ ("simulated", "web", "manual")
        **kwargs: æ¥å£ç‰¹å®šå‚æ•°

    Returns:
        æ ‡æ³¨æ¥å£å®ä¾‹
    """
    interfaces = {
        "simulated": SimulatedAnnotationInterface,
        "web": WebAnnotationInterface,
    }

    if interface_type not in interfaces:
        raise ValueError(f"Unknown annotation interface type: {interface_type}")

    return interfaces[interface_type](**kwargs)


def create_enhanced_data_manager(
    base_config: Dict, output_dir: Path, annotation_config: Dict = None
) -> EnhancedDataManager:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºå¢å¼ºæ•°æ®ç®¡ç†å™¨

    Args:
        base_config: åŸºç¡€æ•°æ®é…ç½®
        output_dir: è¾“å‡ºç›®å½•
        annotation_config: æ ‡æ³¨æ¥å£é…ç½®

    Returns:
        æ•°æ®ç®¡ç†å™¨å®ä¾‹
    """
    annotation_interface = None

    if annotation_config:
        annotation_interface = create_annotation_interface(
            interface_type=annotation_config.get("type", "simulated"), **annotation_config.get("params", {})
        )

    return EnhancedDataManager(
        base_config=base_config, output_dir=output_dir, annotation_interface=annotation_interface
    )
