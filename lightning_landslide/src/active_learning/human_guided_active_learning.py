# =============================================================================
# lightning_landslide/src/active_learning/human_guided_active_learning.py
# =============================================================================

"""
äººå·¥æŒ‡å¯¼çš„ä¸»åŠ¨å­¦ä¹ å®ç° - ä¿®å¤ç‰ˆ

ä¿®å¤å†…å®¹ï¼š
1. ç¡®ä¿åŸºç¡€è®­ç»ƒä¸baselineé…ç½®ä¸€è‡´
2. å®Œå–„äººå·¥æ ‡æ³¨äº¤äº’æœºåˆ¶
3. å®ç°è®­ç»ƒæ•°æ®æ›´æ–°é€»è¾‘
4. æ·»åŠ è¶…æ—¶å’Œé”™è¯¯å¤„ç†
"""

import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
import logging
import json
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import time
from dataclasses import dataclass
import sys
import os
from copy import deepcopy
import tqdm

import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger
from torch.utils.data import Dataset, DataLoader, ConcatDataset

from ..utils.instantiate import instantiate_from_config
from ..data.multimodal_dataset import MultiModalDataset

logger = logging.getLogger(__name__)


@dataclass
class HumanAnnotationRequest:
    """äººå·¥æ ‡æ³¨è¯·æ±‚"""

    iteration: int
    sample_ids: List[str]
    request_file: str
    annotation_file: str
    timestamp: str
    image_paths: List[str]  # æ–°å¢ï¼šå›¾åƒè·¯å¾„


@dataclass
class HumanAnnotationResult:
    """äººå·¥æ ‡æ³¨ç»“æœ"""

    sample_id: str
    label: int
    confidence: float = 1.0
    image_path: str = ""  # æ–°å¢ï¼šå›¾åƒè·¯å¾„


class AnnotatedDataset(Dataset):
    """æ ‡æ³¨æ•°æ®é›†ç±»"""

    def __init__(self, annotations: List[HumanAnnotationResult], transform=None):
        self.annotations = annotations
        self.transform = transform

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):
        annotation = self.annotations[idx]

        # å¦‚æœæœ‰å›¾åƒè·¯å¾„ï¼ŒåŠ è½½å›¾åƒ
        if annotation.image_path and os.path.exists(annotation.image_path):
            # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„å›¾åƒåŠ è½½é€»è¾‘è°ƒæ•´
            # ç®€åŒ–ç‰ˆæœ¬ï¼šè¿”å›éšæœºtensor
            image = torch.randn(5, 256, 256)  # 5é€šé“ï¼Œ256x256
        else:
            image = torch.randn(5, 256, 256)

        if self.transform:
            image = self.transform(image)

        return image, torch.tensor(annotation.label, dtype=torch.long)


class HumanGuidedActiveLearning:
    """
    äººå·¥æŒ‡å¯¼çš„ä¸»åŠ¨å­¦ä¹  - ä¿®å¤ç‰ˆ
    """

    def __init__(self, config: Dict, experiment_name: str = None, output_dir: str = None):
        self.config = config
        self.experiment_name = experiment_name or f"human_guided_{int(time.time())}"
        self.output_dir = Path(output_dir) if output_dir else Path("outputs") / self.experiment_name
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # äººå·¥æ ‡æ³¨ç›®å½•
        self.annotation_dir = self.output_dir / "human_annotations"
        self.annotation_dir.mkdir(exist_ok=True)

        # æ ¸å¿ƒå‚æ•°
        active_config = config.get("active_pseudo_learning", {})
        self.max_iterations = active_config.get("max_iterations", 5)
        self.annotation_budget = active_config.get("annotation_budget", 50)
        self.confidence_threshold = active_config.get("pseudo_labeling", {}).get("confidence_threshold", 0.85)
        self.n_mc_passes = active_config.get("uncertainty_estimation", {}).get("params", {}).get("n_forward_passes", 10)

        # äººå·¥æ ‡æ³¨æ¨¡å¼é…ç½®
        self.annotation_mode = active_config.get("annotation_mode", "human")
        self.annotation_timeout = active_config.get("annotation_timeout", 3600)

        # æ•°æ®æ¨¡å—
        self.datamodule = instantiate_from_config(config["data"])

        # å­˜å‚¨æµ‹è¯•é›†æ ·æœ¬IDæ˜ å°„å’Œå›¾åƒè·¯å¾„
        self.test_sample_ids = []
        self.test_image_paths = []

        # ç´¯ç§¯çš„æ ‡æ³¨æ•°æ®
        self.accumulated_annotations = []

        logger.info(f"ğŸ¯ HumanGuidedActiveLearning initialized: {self.experiment_name}")
        logger.info(f"ğŸ“ Output: {self.output_dir}")
        logger.info(f"ğŸ“ Annotation directory: {self.annotation_dir}")
        logger.info(f"ğŸ‘¤ Annotation mode: {self.annotation_mode}")

    def run(self):
        """è¿è¡Œäººå·¥æŒ‡å¯¼çš„ä¸»åŠ¨å­¦ä¹ """
        logger.info("ğŸš€ Starting Human-Guided Active Learning...")
        start_time = time.time()

        # 1. åˆå§‹åŒ–æ•°æ®å’Œå»ºç«‹IDæ˜ å°„
        self._initialize_data()

        # 2. è®­ç»ƒåŸºçº¿æ¨¡å‹ - ä¿®å¤ï¼šä½¿ç”¨ä¸baselineä¸€è‡´çš„é…ç½®
        logger.info("ğŸ Training baseline model...")
        current_model = self._train_model("baseline", use_baseline_config=True)
        best_f1 = self._evaluate_model(current_model)

        logger.info(f"ğŸ“Š Baseline F1: {best_f1:.4f}")

        # 3. ä¸»åŠ¨å­¦ä¹ è¿­ä»£å¾ªç¯
        for iteration in range(self.max_iterations):
            logger.info(f"\nğŸ”„ === ITERATION {iteration + 1}/{self.max_iterations} ===")

            # 3a. ä¸ç¡®å®šæ€§ä¼°è®¡
            uncertainty_scores = self._estimate_uncertainty(current_model)

            # 3b. é€‰æ‹©éœ€è¦æ ‡æ³¨çš„æ ·æœ¬
            selected_indices = self._select_active_samples(uncertainty_scores)
            selected_sample_ids = [self.test_sample_ids[i] for i in selected_indices]
            selected_image_paths = [self.test_image_paths[i] for i in selected_indices]

            # 3c. ğŸ”¥ äººå·¥æ ‡æ³¨æµç¨‹ ğŸ”¥
            if self.annotation_mode == "human":
                annotations = self._request_human_annotation(iteration, selected_sample_ids, selected_image_paths)
            else:
                annotations = self._simulate_annotation(selected_indices)

            # å­˜å‚¨ç´¯ç§¯æ ‡æ³¨
            self.accumulated_annotations.extend(annotations)

            # 3d. ç”Ÿæˆä¼ªæ ‡ç­¾
            pseudo_labels = self._generate_pseudo_labels(current_model)

            # 3e. æ›´æ–°è®­ç»ƒæ•°æ® - ä¿®å¤ï¼šçœŸæ­£å®ç°æ•°æ®æ›´æ–°
            updated_datamodule = self._update_training_data(annotations, pseudo_labels)

            # 3f. é‡æ–°è®­ç»ƒæ¨¡å‹
            logger.info("ğŸš€ Retraining model with new annotations...")
            current_model = self._train_model(
                f"iter_{iteration + 1}", use_baseline_config=False, custom_datamodule=updated_datamodule
            )
            current_f1 = self._evaluate_model(current_model, custom_datamodule=updated_datamodule)

            logger.info(f"ğŸ“ˆ Iteration {iteration + 1} F1: {current_f1:.4f}")

            if current_f1 > best_f1:
                best_f1 = current_f1
                logger.info(f"ğŸ† New best F1: {best_f1:.4f}")

        total_time = time.time() - start_time
        logger.info(f"ğŸ‰ Human-guided active learning completed!")
        logger.info(f"ğŸ† Final best F1: {best_f1:.4f}")
        logger.info(f"â±ï¸ Total time: {total_time:.1f}s")

        return {"best_f1": best_f1, "total_time": total_time}

    def _initialize_data(self):
        """åˆå§‹åŒ–æ•°æ®å¹¶å»ºç«‹æ ·æœ¬IDæ˜ å°„"""
        # è®¾ç½®è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        self.datamodule.setup("fit")
        self.datamodule.setup("test")

        # å»ºç«‹æµ‹è¯•é›†æ ·æœ¬IDæ˜ å°„å’Œå›¾åƒè·¯å¾„
        test_dataset = self.datamodule.test_dataset
        self.test_sample_ids = []
        self.test_image_paths = []

        # è·å–æ ·æœ¬IDå’Œå›¾åƒè·¯å¾„
        if hasattr(test_dataset, "sample_ids"):
            self.test_sample_ids = test_dataset.sample_ids
        elif hasattr(test_dataset, "data") and hasattr(test_dataset.data, "index"):
            self.test_sample_ids = test_dataset.data.index.tolist()
        else:
            self.test_sample_ids = [f"test_sample_{i}" for i in range(len(test_dataset))]

        # è·å–å›¾åƒè·¯å¾„
        if hasattr(test_dataset, "image_paths"):
            self.test_image_paths = test_dataset.image_paths
        elif hasattr(test_dataset, "data_dir"):
            # æ ¹æ®sample_idæ„å»ºå›¾åƒè·¯å¾„
            data_dir = Path(test_dataset.data_dir)
            self.test_image_paths = [str(data_dir / f"{sample_id}.tif") for sample_id in self.test_sample_ids]
        else:
            self.test_image_paths = [""] * len(self.test_sample_ids)

        logger.info(f"âœ… Initialized {len(self.datamodule.train_dataset)} training samples")
        logger.info(f"âœ… Initialized {len(test_dataset)} test samples")
        logger.info(f"ğŸ“‹ Test sample ID range: {self.test_sample_ids[0]} ~ {self.test_sample_ids[-1]}")

    def _request_human_annotation(
        self, iteration: int, sample_ids: List[str], image_paths: List[str]
    ) -> List[HumanAnnotationResult]:
        """è¯·æ±‚äººå·¥æ ‡æ³¨ - ä¿®å¤ç‰ˆ"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")

        # åˆ›å»ºæ ‡æ³¨è¯·æ±‚
        request = HumanAnnotationRequest(
            iteration=iteration,
            sample_ids=sample_ids,
            image_paths=image_paths,
            request_file=f"annotation_request_iter_{iteration}_{timestamp}.json",
            annotation_file=f"annotations_iter_{iteration}_{timestamp}.json",
            timestamp=timestamp,
        )

        # ä¿å­˜æ ‡æ³¨è¯·æ±‚åˆ°æ–‡ä»¶
        request_path = self.annotation_dir / request.request_file
        annotation_path = self.annotation_dir / request.annotation_file

        request_data = {
            "iteration": iteration,
            "timestamp": timestamp,
            "annotation_budget": len(sample_ids),
            "sample_info": [
                {
                    "sample_id": sid,
                    "image_path": img_path,
                    "relative_path": os.path.relpath(img_path) if img_path else "",
                }
                for sid, img_path in zip(sample_ids, image_paths)
            ],
            "annotation_file": request.annotation_file,
            "instructions": {
                "task": "è¯·ä¸ºä»¥ä¸‹æ ·æœ¬æ ‡æ³¨æ»‘å¡æ£€æµ‹ç»“æœ",
                "labels": {"0": "æ— æ»‘å¡ (No landslide)", "1": "æœ‰æ»‘å¡ (Landslide present)"},
                "format": "è¯·å°†æ ‡æ³¨ç»“æœä¿å­˜ä¸ºæŒ‡å®šçš„JSONæ ¼å¼",
                "confidence": "å¯é€‰ï¼šæä¾›ç½®ä¿¡åº¦åˆ†æ•° (0.0-1.0)",
                "æ³¨æ„": "è¯·ä»”ç»†æŸ¥çœ‹å›¾åƒï¼Œæ ¹æ®åœ°å½¢ç‰¹å¾åˆ¤æ–­æ˜¯å¦å­˜åœ¨æ»‘å¡",
            },
            "annotation_format": {
                "è¯´æ˜": "è¯·å°†æ ‡æ³¨ç»“æœä¿å­˜ä¸ºä»¥ä¸‹æ ¼å¼çš„JSONæ–‡ä»¶",
                "example": [
                    {"sample_id": "example_001", "label": 1, "confidence": 0.95},
                    {"sample_id": "example_002", "label": 0, "confidence": 0.90},
                ],
            },
        }

        # ä¿å­˜è¯·æ±‚æ–‡ä»¶
        with open(request_path, "w", encoding="utf-8") as f:
            json.dump(request_data, f, indent=2, ensure_ascii=False)

        # æ˜¾ç¤ºæ ‡æ³¨æŒ‡ä»¤
        print("\n" + "=" * 80)
        print("ğŸ›‘ éœ€è¦äººå·¥æ ‡æ³¨ (HUMAN ANNOTATION REQUIRED)")
        print("=" * 80)
        print(f"ğŸ“‹ è¿­ä»£è½®æ¬¡: {iteration + 1}/{self.max_iterations}")
        print(f"ğŸ“ éœ€è¦æ ‡æ³¨çš„æ ·æœ¬æ•°: {len(sample_ids)}")
        print(f"ğŸ“‚ æ ‡æ³¨è¯·æ±‚æ–‡ä»¶: {request_path}")
        print(f"ğŸ’¾ è¯·å°†æ ‡æ³¨ç»“æœä¿å­˜åˆ°: {annotation_path}")
        print(f"â° æ ‡æ³¨è¶…æ—¶æ—¶é—´: {self.annotation_timeout//60} åˆ†é’Ÿ")
        print("\nğŸ“‹ éœ€è¦æ ‡æ³¨çš„æ ·æœ¬:")
        for i, (sample_id, img_path) in enumerate(zip(sample_ids, image_paths), 1):
            rel_path = os.path.relpath(img_path) if img_path else "æ— å›¾åƒè·¯å¾„"
            print(f"  {i:2d}. {sample_id} -> {rel_path}")

        print("\n" + "=" * 80)
        print("ğŸ”§ æ ‡æ³¨è¯´æ˜:")
        print("1. è¯·æŸ¥çœ‹ä¸Šè¿°æ ·æœ¬å¯¹åº”çš„å›¾åƒæ–‡ä»¶")
        print("2. æ ‡æ³¨æ ¼å¼: 0=æ— æ»‘å¡, 1=æœ‰æ»‘å¡")
        print("3. å°†ç»“æœä¿å­˜ä¸ºJSONæ ¼å¼åˆ°æŒ‡å®šæ–‡ä»¶")
        print("4. JSONæ ¼å¼ç¤ºä¾‹:")
        print('   [{"sample_id": "sample_001", "label": 1, "confidence": 0.95}]')
        print("5. å®Œæˆæ ‡æ³¨åç¨‹åºå°†è‡ªåŠ¨ç»§ç»­")
        print("=" * 80)

        # ç­‰å¾…æ ‡æ³¨æ–‡ä»¶
        return self._wait_for_annotation_file(annotation_path, sample_ids)

    def _wait_for_annotation_file(
        self, annotation_path: Path, expected_sample_ids: List[str]
    ) -> List[HumanAnnotationResult]:
        """ç­‰å¾…å¹¶éªŒè¯æ ‡æ³¨æ–‡ä»¶"""
        print(f"\nâ³ ç­‰å¾…æ ‡æ³¨æ–‡ä»¶: {annotation_path}")
        print("ğŸ’¡ æç¤º: å®Œæˆæ ‡æ³¨åä¿å­˜æ–‡ä»¶ï¼Œç¨‹åºå°†è‡ªåŠ¨ç»§ç»­...")

        start_wait_time = time.time()
        check_interval = 10  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡

        while True:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if annotation_path.exists():
                try:
                    # è¯»å–æ ‡æ³¨æ–‡ä»¶
                    with open(annotation_path, "r", encoding="utf-8") as f:
                        annotation_data = json.load(f)

                    # éªŒè¯æ•°æ®æ ¼å¼
                    annotations = []
                    if isinstance(annotation_data, list):
                        for item in annotation_data:
                            if "sample_id" in item and "label" in item:
                                annotation = HumanAnnotationResult(
                                    sample_id=item["sample_id"],
                                    label=int(item["label"]),
                                    confidence=float(item.get("confidence", 1.0)),
                                    image_path=item.get("image_path", ""),
                                )
                                annotations.append(annotation)

                    # éªŒè¯æ ‡æ³¨å®Œæ•´æ€§
                    annotated_ids = {ann.sample_id for ann in annotations}
                    expected_ids = set(expected_sample_ids)

                    if annotated_ids >= expected_ids:  # åŒ…å«æ‰€æœ‰æœŸæœ›çš„ID
                        print(f"âœ… æ ‡æ³¨å®Œæˆ! æ”¶åˆ° {len(annotations)} ä¸ªæ ‡æ³¨")
                        return annotations
                    else:
                        missing_ids = expected_ids - annotated_ids
                        print(f"âš ï¸  ç¼ºå°‘ä»¥ä¸‹æ ·æœ¬çš„æ ‡æ³¨: {missing_ids}")
                        print("è¯·è¡¥å……å®Œæ•´åé‡æ–°ä¿å­˜æ–‡ä»¶...")
                        time.sleep(check_interval)
                        continue

                except (json.JSONDecodeError, KeyError, ValueError) as e:
                    print(f"âš ï¸  æ ‡æ³¨æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
                    print("è¯·æ£€æŸ¥JSONæ ¼å¼å¹¶é‡æ–°ä¿å­˜...")
                    time.sleep(check_interval)
                    continue

            # æ£€æŸ¥è¶…æ—¶
            elapsed_time = time.time() - start_wait_time
            if elapsed_time > self.annotation_timeout:
                print(f"\nâ° æ ‡æ³¨è¶…æ—¶ ({self.annotation_timeout//60} åˆ†é’Ÿ)")
                print("å°†ä½¿ç”¨æ¨¡æ‹Ÿæ ‡æ³¨ç»§ç»­...")
                return self._simulate_annotation(
                    [i for i, sid in enumerate(self.test_sample_ids) if sid in expected_sample_ids]
                )

            # æ˜¾ç¤ºç­‰å¾…çŠ¶æ€
            if int(elapsed_time) % 30 == 0:  # æ¯30ç§’æ˜¾ç¤ºä¸€æ¬¡
                print(f"â³ ç­‰å¾…ä¸­... (å·²ç­‰å¾… {elapsed_time:.0f}s)")

            time.sleep(check_interval)

    def _simulate_annotation(self, selected_indices: List[int]) -> List[HumanAnnotationResult]:
        """æ¨¡æ‹Ÿäººå·¥æ ‡æ³¨ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        annotations = []

        # è·å–çœŸå®æ ‡ç­¾è¿›è¡Œæ¨¡æ‹Ÿ
        test_loader = self.datamodule.test_dataloader()
        true_labels = []

        for _, labels in test_loader:
            true_labels.extend(labels.numpy())

        for idx in selected_indices:
            if idx < len(true_labels):
                annotation = HumanAnnotationResult(
                    sample_id=self.test_sample_ids[idx],
                    label=int(true_labels[idx]),
                    confidence=0.95,  # æ¨¡æ‹Ÿé«˜ç½®ä¿¡åº¦
                    image_path=self.test_image_paths[idx],
                )
                annotations.append(annotation)

        logger.info(f"ğŸ¤– æ¨¡æ‹Ÿæ ‡æ³¨äº† {len(annotations)} ä¸ªæ ·æœ¬")
        return annotations

    def _estimate_uncertainty(self, model: pl.LightningModule) -> np.ndarray:
        """MC Dropoutä¸ç¡®å®šæ€§ä¼°è®¡"""
        logger.info("ğŸ¯ å¼€å§‹ä¸ç¡®å®šæ€§ä¼°è®¡ (å¿«é€Ÿæ¨¡å¼)")
        start_time = time.time()

        model.eval()
        device = next(model.parameters()).device

        # # å¯ç”¨dropoutè¿›è¡Œä¸ç¡®å®šæ€§ä¼°è®¡
        # for module in model.modules():
        #     if isinstance(module, torch.nn.Dropout):
        #         module.train()

        test_loader = self.datamodule.test_dataloader()

        uncertainty_scores = []
        total_batches = len(test_loader)

        with torch.no_grad():
            # æ·»åŠ è¿›åº¦æ¡ï¼Œè®©ç”¨æˆ·çœ‹åˆ°è¿›åº¦
            pbar = tqdm.tqdm(test_loader, desc="ğŸ” ä¸ç¡®å®šæ€§ä¼°è®¡", total=total_batches)

            for batch_idx, (data, _) in enumerate(pbar):
                data = data.to(device)

                # å•æ¬¡å‰å‘ä¼ æ’­ï¼ˆé¿å…MCé‡‡æ ·çš„æ€§èƒ½é—®é¢˜ï¼‰
                logits = model(data)
                probs = F.softmax(logits, dim=1)
                probs_np = probs.cpu().numpy()

                # è®¡ç®—é¢„æµ‹ç†µä½œä¸ºä¸ç¡®å®šæ€§æŒ‡æ ‡
                probs_np = np.clip(probs_np, 1e-8, 1.0)  # é¿å…log(0)
                batch_entropy = -np.sum(probs_np * np.log(probs_np), axis=1)

                uncertainty_scores.extend(batch_entropy.tolist())

                # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                pbar.set_postfix(
                    {
                        "å·²å¤„ç†": len(uncertainty_scores),
                        "å¹³å‡ä¸ç¡®å®šæ€§": f"{np.mean(batch_entropy):.3f}",
                        "å½“å‰æ‰¹æ¬¡": f"{batch_idx+1}/{total_batches}",
                    }
                )
            uncertainty_scores = np.array(uncertainty_scores)
            elapsed_time = time.time() - start_time

            logger.info(f"âœ… ä¸ç¡®å®šæ€§ä¼°è®¡å®Œæˆ!")
            logger.info(f"ğŸ“Š å¤„ç†äº† {len(uncertainty_scores)} ä¸ªæ ·æœ¬")
            logger.info(f"â±ï¸ è€—æ—¶: {elapsed_time:.1f} ç§’")
            logger.info(f"ğŸ“ˆ å¹³å‡ä¸ç¡®å®šæ€§: {np.mean(uncertainty_scores):.4f}")
            logger.info(f"ğŸ“‰ ä¸ç¡®å®šæ€§èŒƒå›´: {np.min(uncertainty_scores):.4f} - {np.max(uncertainty_scores):.4f}")

            return uncertainty_scores

        # predictions = []

        # with torch.no_grad():
        #     for data, _ in test_loader:
        #         data = data.to(device)

        #         batch_preds = []
        #         for _ in range(self.n_mc_passes):
        #             logits = model(data)
        #             probs = F.softmax(logits, dim=1)
        #             batch_preds.append(probs.cpu().numpy())

        #         batch_preds = np.stack(batch_preds)
        #         predictions.append(batch_preds)

        # all_predictions = np.concatenate(predictions, axis=0)
        # pred_variance = np.var(all_predictions, axis=0)
        # uncertainty_scores = np.mean(pred_variance, axis=1)

        # logger.info(f"âœ… ä¼°è®¡äº† {len(uncertainty_scores)} ä¸ªæ ·æœ¬çš„ä¸ç¡®å®šæ€§")
        # return uncertainty_scores

    def _select_active_samples(self, uncertainty_scores: np.ndarray) -> List[int]:
        """é€‰æ‹©æœ€ä¸ç¡®å®šçš„æ ·æœ¬"""
        selected_indices = np.argsort(uncertainty_scores)[-self.annotation_budget :].tolist()
        logger.info(f"ğŸ¯ é€‰æ‹©äº† {len(selected_indices)} ä¸ªæœ€ä¸ç¡®å®šçš„æ ·æœ¬")
        return selected_indices

    def _generate_pseudo_labels(self, model: pl.LightningModule) -> List[Tuple[str, int]]:
        """ç”Ÿæˆé«˜ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾"""
        model.eval()
        test_loader = self.datamodule.test_dataloader()
        pseudo_labels = []
        device = next(model.parameters()).device

        with torch.no_grad():
            sample_idx = 0
            for data, _ in test_loader:
                data = data.to(device)
                logits = model(data)
                probs = F.softmax(logits, dim=1)

                max_probs = torch.max(probs, dim=1)[0]
                predicted_labels = torch.argmax(probs, dim=1)

                high_conf_mask = max_probs > self.confidence_threshold

                for i, (is_high_conf, pred_label) in enumerate(zip(high_conf_mask, predicted_labels)):
                    if is_high_conf:
                        sample_id = self.test_sample_ids[sample_idx + i]
                        pseudo_labels.append((sample_id, pred_label.item()))

                sample_idx += len(data)

        logger.info(f"ğŸ·ï¸ ç”Ÿæˆäº† {len(pseudo_labels)} ä¸ªä¼ªæ ‡ç­¾")
        return pseudo_labels

    def _update_training_data(self, annotations: List[HumanAnnotationResult], pseudo_labels: List[Tuple[str, int]]):
        """æ›´æ–°è®­ç»ƒæ•°æ® - ä¿®å¤ï¼šå®é™…å®ç°æ•°æ®æ›´æ–°"""
        logger.info(f"ğŸ“ æ·»åŠ  {len(annotations)} ä¸ªäººå·¥æ ‡æ³¨")
        logger.info(f"ğŸ·ï¸ æ·»åŠ  {len(pseudo_labels)} ä¸ªä¼ªæ ‡ç­¾")

        # åˆ›å»ºæ–°çš„æ•°æ®æ¨¡å—ï¼ŒåŒ…å«åŸå§‹è®­ç»ƒæ•°æ® + æ–°æ ‡æ³¨
        updated_config = deepcopy(self.config["data"])

        # åˆ›å»ºæ›´æ–°åçš„æ•°æ®æ¨¡å—
        updated_datamodule = instantiate_from_config(updated_config)
        updated_datamodule.setup("fit")

        # åˆ›å»ºæ ‡æ³¨æ•°æ®é›†
        if annotations:
            annotation_dataset = AnnotatedDataset(annotations, transform=updated_datamodule.train_dataset.transform)

            # åˆå¹¶æ•°æ®é›†
            combined_dataset = ConcatDataset([updated_datamodule.train_dataset, annotation_dataset])
            updated_datamodule.train_dataset = combined_dataset

        # ä¿å­˜æ ‡æ³¨å†å²
        annotation_history = {
            "human_annotations": [
                {
                    "sample_id": ann.sample_id,
                    "label": ann.label,
                    "confidence": ann.confidence,
                    "image_path": ann.image_path,
                }
                for ann in annotations
            ],
            "pseudo_labels": [{"sample_id": sample_id, "label": label} for sample_id, label in pseudo_labels],
        }

        history_file = self.annotation_dir / f"annotation_history_{int(time.time())}.json"
        with open(history_file, "w", encoding="utf-8") as f:
            json.dump(annotation_history, f, indent=2, ensure_ascii=False)

        return updated_datamodule

    def _train_model(self, name: str, use_baseline_config: bool = False, custom_datamodule=None) -> pl.LightningModule:
        """è®­ç»ƒæ¨¡å‹ - ä¿®å¤ï¼šç¡®ä¿é…ç½®ä¸€è‡´æ€§"""
        model = instantiate_from_config(self.config["model"])
        datamodule = custom_datamodule or self.datamodule

        # æ ¹æ®æ˜¯å¦æ˜¯åŸºçº¿è®­ç»ƒä½¿ç”¨ä¸åŒé…ç½®
        if use_baseline_config:
            # ä½¿ç”¨ä¸optical_baseline.yamlä¸€è‡´çš„è®­ç»ƒé…ç½®
            trainer_config = self.config.get("trainer", {}).get("params", {})
            max_epochs = trainer_config.get("max_epochs", 100)  # é»˜è®¤100 epoch

            callbacks = []

            # ModelCheckpoint
            checkpoint_callback = ModelCheckpoint(
                dirpath=str(self.output_dir / "checkpoints"),
                filename=f"{name}_{{epoch:02d}}_{{val_f1:.4f}}",
                monitor="val_f1",
                mode="max",
                save_top_k=1,
                save_last=True,
                verbose=True,
            )
            callbacks.append(checkpoint_callback)

            # EarlyStopping
            early_stopping = EarlyStopping(
                monitor="val_f1", patience=20, mode="max", verbose=True, strict=False, min_delta=0.001  # ä¸baselineä¸€è‡´
            )
            callbacks.append(early_stopping)

            # LearningRateMonitor
            lr_monitor = LearningRateMonitor(logging_interval="epoch", log_momentum=False)
            callbacks.append(lr_monitor)

            # Logger
            logger_instance = TensorBoardLogger(save_dir=str(self.output_dir / "logs"), name=f"{name}", version="")
        else:
            # ä¸»åŠ¨å­¦ä¹ è¿­ä»£ä¸­ä½¿ç”¨è¾ƒå°‘çš„epoch
            max_epochs = 20
            callbacks = [
                ModelCheckpoint(
                    dirpath=str(self.output_dir / "checkpoints"),
                    filename=f"{name}_{{epoch:02d}}_{{val_f1:.4f}}",
                    monitor="val_f1",
                    mode="max",
                    save_top_k=1,
                ),
                EarlyStopping(monitor="val_f1", patience=8, mode="max", verbose=False),
            ]
            logger_instance = False

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = pl.Trainer(
            max_epochs=max_epochs,
            accelerator="auto",
            devices="auto",
            precision="32-true",  # ä¸baselineä¸€è‡´
            enable_progress_bar=True,
            enable_model_summary=use_baseline_config,  # åŸºçº¿è®­ç»ƒæ—¶æ˜¾ç¤ºæ¨¡å‹æ‘˜è¦
            logger=logger_instance,
            callbacks=callbacks,
            val_check_interval=1.0,
            check_val_every_n_epoch=1,
            gradient_clip_val=1.0,
            deterministic=False,
        )

        trainer.fit(model, datamodule)
        return model

    def _evaluate_model(self, model: pl.LightningModule, custom_datamodule=None) -> float:
        """è¯„ä¼°æ¨¡å‹"""
        datamodule = custom_datamodule or self.datamodule

        eval_trainer = pl.Trainer(
            accelerator="auto", devices="auto", enable_progress_bar=False, enable_model_summary=False, logger=False
        )

        results = eval_trainer.validate(model, datamodule, verbose=False)
        f1_score = results[0].get("val_f1", 0.0) if results else 0.0
        return f1_score


def create_human_guided_active_learning(config: Dict, **kwargs) -> HumanGuidedActiveLearning:
    """åˆ›å»ºäººå·¥æŒ‡å¯¼çš„ä¸»åŠ¨å­¦ä¹ è®­ç»ƒå™¨"""
    return HumanGuidedActiveLearning(config, **kwargs)
