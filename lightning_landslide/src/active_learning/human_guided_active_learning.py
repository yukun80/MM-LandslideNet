# =============================================================================
# lightning_landslide/src/active_learning/human_guided_active_learning.py
# =============================================================================

"""
äººå·¥æŒ‡å¯¼çš„ä¸»åŠ¨å­¦ä¹ å®ç°

è®¾è®¡æ€è·¯ï¼š
1. ç¨‹åºé€‰æ‹©æœ€ä¸ç¡®å®šçš„æ ·æœ¬
2. è¾“å‡ºæ ·æœ¬IDåˆ—è¡¨ç»™äººå·¥ä¸“å®¶
3. æš‚åœç¨‹åºç­‰å¾…äººå·¥æ ‡æ³¨
4. è¯»å–äººå·¥æ ‡æ³¨ç»“æœ
5. ç»§ç»­è®­ç»ƒæµç¨‹
"""

import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
import logging
import json
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import time
from dataclasses import dataclass
import sys

import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping

from ..utils.instantiate import instantiate_from_config

logger = logging.getLogger(__name__)


@dataclass
class HumanAnnotationRequest:
    """äººå·¥æ ‡æ³¨è¯·æ±‚"""

    iteration: int
    sample_ids: List[str]
    request_file: str
    annotation_file: str
    timestamp: str


@dataclass
class HumanAnnotationResult:
    """äººå·¥æ ‡æ³¨ç»“æœ"""

    sample_id: str
    label: int
    confidence: float = 1.0  # ä¸“å®¶æ ‡æ³¨ç½®ä¿¡åº¦


class HumanGuidedActiveLearning:
    """
    äººå·¥æŒ‡å¯¼çš„ä¸»åŠ¨å­¦ä¹ 

    æ ¸å¿ƒç‰¹ç‚¹ï¼š
    1. çœŸå®çš„äººå·¥æ ‡æ³¨æµç¨‹
    2. ç¨‹åºæš‚åœ/æ¢å¤æœºåˆ¶
    3. æ ‡æ³¨è´¨é‡è·Ÿè¸ª
    4. ç®€å•çš„æ–‡ä»¶æ¥å£
    """

    def __init__(self, config: Dict, experiment_name: str = None, output_dir: str = None):
        self.config = config
        self.experiment_name = experiment_name or f"human_guided_{int(time.time())}"
        self.output_dir = Path(output_dir) if output_dir else Path("outputs") / self.experiment_name
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # äººå·¥æ ‡æ³¨ç›®å½•
        self.annotation_dir = self.output_dir / "human_annotations"
        self.annotation_dir.mkdir(exist_ok=True)

        # æ ¸å¿ƒå‚æ•°
        active_config = config.get("active_pseudo_learning", {})
        self.max_iterations = active_config.get("max_iterations", 5)
        self.annotation_budget = active_config.get("annotation_budget", 50)
        self.confidence_threshold = active_config.get("pseudo_labeling", {}).get("confidence_threshold", 0.85)
        self.n_mc_passes = active_config.get("uncertainty_estimation", {}).get("params", {}).get("n_forward_passes", 10)

        # äººå·¥æ ‡æ³¨æ¨¡å¼é…ç½®
        self.annotation_mode = active_config.get("annotation_mode", "human")  # "human" | "simulated"
        self.annotation_timeout = active_config.get("annotation_timeout", 3600)  # 1å°æ—¶è¶…æ—¶

        # æ•°æ®æ¨¡å—
        self.datamodule = instantiate_from_config(config["data"])

        # å­˜å‚¨æµ‹è¯•é›†æ ·æœ¬IDæ˜ å°„
        self.test_sample_ids = []

        logger.info(f"ğŸ¯ HumanGuidedActiveLearning initialized: {self.experiment_name}")
        logger.info(f"ğŸ“ Output: {self.output_dir}")
        logger.info(f"ğŸ“ Annotation directory: {self.annotation_dir}")
        logger.info(f"ğŸ‘¤ Annotation mode: {self.annotation_mode}")

    def run(self):
        """è¿è¡Œäººå·¥æŒ‡å¯¼çš„ä¸»åŠ¨å­¦ä¹ """
        logger.info("ğŸš€ Starting Human-Guided Active Learning...")
        start_time = time.time()

        # 1. åˆå§‹åŒ–æ•°æ®å’Œå»ºç«‹IDæ˜ å°„
        self._initialize_data()

        # 2. è®­ç»ƒåŸºçº¿æ¨¡å‹
        logger.info("ğŸ Training baseline model...")
        current_model = self._train_model("baseline")
        best_f1 = self._evaluate_model(current_model)

        logger.info(f"ğŸ“Š Baseline F1: {best_f1:.4f}")

        # 3. ä¸»åŠ¨å­¦ä¹ è¿­ä»£å¾ªç¯
        for iteration in range(self.max_iterations):
            logger.info(f"\nğŸ”„ === ITERATION {iteration + 1}/{self.max_iterations} ===")

            # 3a. ä¸ç¡®å®šæ€§ä¼°è®¡
            uncertainty_scores = self._estimate_uncertainty(current_model)

            # 3b. é€‰æ‹©éœ€è¦æ ‡æ³¨çš„æ ·æœ¬
            selected_indices = self._select_active_samples(uncertainty_scores)
            selected_sample_ids = [self.test_sample_ids[i] for i in selected_indices]

            # 3c. ğŸ”¥ äººå·¥æ ‡æ³¨æµç¨‹ ğŸ”¥
            if self.annotation_mode == "human":
                annotations = self._request_human_annotation(iteration, selected_sample_ids)
            else:
                annotations = self._simulate_annotation(selected_indices)

            # 3d. ç”Ÿæˆä¼ªæ ‡ç­¾
            pseudo_labels = self._generate_pseudo_labels(current_model)

            # 3e. æ›´æ–°è®­ç»ƒæ•°æ®ï¼ˆæ·»åŠ äººå·¥æ ‡æ³¨ + ä¼ªæ ‡ç­¾ï¼‰
            self._update_training_data(annotations, pseudo_labels)

            # 3f. é‡æ–°è®­ç»ƒæ¨¡å‹
            logger.info("ğŸš€ Retraining model with new annotations...")
            current_model = self._train_model(f"iter_{iteration + 1}")
            current_f1 = self._evaluate_model(current_model)

            logger.info(f"ğŸ“ˆ Iteration {iteration + 1} F1: {current_f1:.4f}")

            if current_f1 > best_f1:
                best_f1 = current_f1
                logger.info(f"ğŸ† New best F1: {best_f1:.4f}")

        total_time = time.time() - start_time
        logger.info(f"ğŸ‰ Human-guided active learning completed!")
        logger.info(f"ğŸ† Final best F1: {best_f1:.4f}")
        logger.info(f"â±ï¸ Total time: {total_time:.1f}s")

        return {"best_f1": best_f1, "total_time": total_time}

    def _initialize_data(self):
        """åˆå§‹åŒ–æ•°æ®å¹¶å»ºç«‹æ ·æœ¬IDæ˜ å°„"""
        # è®¾ç½®è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        self.datamodule.setup("fit")
        self.datamodule.setup("test")

        # å»ºç«‹æµ‹è¯•é›†æ ·æœ¬IDæ˜ å°„
        test_dataset = self.datamodule.test_dataset
        self.test_sample_ids = []

        # å¦‚æœæ•°æ®é›†æœ‰sample_idå±æ€§ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨ç´¢å¼•
        if hasattr(test_dataset, "sample_ids"):
            self.test_sample_ids = test_dataset.sample_ids
        elif hasattr(test_dataset, "data") and hasattr(test_dataset.data, "index"):
            self.test_sample_ids = test_dataset.data.index.tolist()
        else:
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„IDï¼Œä½¿ç”¨æ•°æ®é›†ç´¢å¼•
            self.test_sample_ids = [f"test_sample_{i}" for i in range(len(test_dataset))]

        logger.info(f"âœ… Initialized {len(self.datamodule.train_dataset)} training samples")
        logger.info(f"âœ… Initialized {len(test_dataset)} test samples")
        logger.info(f"ğŸ“‹ Test sample ID range: {self.test_sample_ids[0]} ~ {self.test_sample_ids[-1]}")

    def _request_human_annotation(self, iteration: int, sample_ids: List[str]) -> List[HumanAnnotationResult]:
        """è¯·æ±‚äººå·¥æ ‡æ³¨"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")

        # åˆ›å»ºæ ‡æ³¨è¯·æ±‚
        request = HumanAnnotationRequest(
            iteration=iteration,
            sample_ids=sample_ids,
            request_file=f"annotation_request_iter_{iteration}_{timestamp}.json",
            annotation_file=f"annotations_iter_{iteration}_{timestamp}.json",
            timestamp=timestamp,
        )

        # ä¿å­˜æ ‡æ³¨è¯·æ±‚åˆ°æ–‡ä»¶
        request_path = self.annotation_dir / request.request_file
        annotation_path = self.annotation_dir / request.annotation_file

        request_data = {
            "iteration": iteration,
            "timestamp": timestamp,
            "annotation_budget": len(sample_ids),
            "sample_ids": sample_ids,
            "annotation_file": request.annotation_file,
            "instructions": {
                "task": "Please annotate the following samples for landslide detection",
                "labels": {"0": "No landslide", "1": "Landslide present"},
                "format": "Save annotations in the specified JSON format",
                "confidence": "Optional: provide confidence score (0.0-1.0)",
            },
            "annotation_format": {
                "example": [
                    {"sample_id": "example_001", "label": 1, "confidence": 0.95},
                    {"sample_id": "example_002", "label": 0, "confidence": 0.90},
                ]
            },
        }

        # ä¿å­˜è¯·æ±‚æ–‡ä»¶
        with open(request_path, "w", encoding="utf-8") as f:
            json.dump(request_data, f, indent=2, ensure_ascii=False)

        # æ˜¾ç¤ºæ ‡æ³¨æŒ‡ä»¤
        print("\n" + "=" * 80)
        print("ğŸ›‘ HUMAN ANNOTATION REQUIRED")
        print("=" * 80)
        print(f"ğŸ“‹ Iteration: {iteration + 1}")
        print(f"ğŸ“ Samples to annotate: {len(sample_ids)}")
        print(f"ğŸ“‚ Request file: {request_path}")
        print(f"ğŸ’¾ Save annotations to: {annotation_path}")
        print("\nğŸ“‹ Samples to annotate:")
        for i, sample_id in enumerate(sample_ids, 1):
            print(f"  {i:2d}. {sample_id}")

        print("\nğŸ“ Annotation format (JSON):")
        print(json.dumps(request_data["annotation_format"]["example"], indent=2))

        print(f"\nâ° Please complete annotation within {self.annotation_timeout//60} minutes")
        print("ğŸ”„ Waiting for annotation completion...")
        print("=" * 80)

        # ç­‰å¾…æ ‡æ³¨å®Œæˆ
        annotations = self._wait_for_annotations(annotation_path)

        logger.info(f"âœ… Received {len(annotations)} human annotations")
        return annotations

    def _wait_for_annotations(self, annotation_path: Path) -> List[HumanAnnotationResult]:
        """ç­‰å¾…äººå·¥æ ‡æ³¨å®Œæˆ"""
        start_wait_time = time.time()

        while True:
            # æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if annotation_path.exists():
                try:
                    # å°è¯•è¯»å–æ ‡æ³¨æ–‡ä»¶
                    with open(annotation_path, "r", encoding="utf-8") as f:
                        annotation_data = json.load(f)

                    # è§£ææ ‡æ³¨ç»“æœ
                    annotations = []
                    for item in annotation_data:
                        annotation = HumanAnnotationResult(
                            sample_id=item["sample_id"],
                            label=int(item["label"]),
                            confidence=float(item.get("confidence", 1.0)),
                        )
                        annotations.append(annotation)

                    print(f"\nâœ… Successfully loaded {len(annotations)} annotations!")
                    return annotations

                except (json.JSONDecodeError, KeyError, ValueError) as e:
                    print(f"âš ï¸  Error reading annotation file: {e}")
                    print("Please check the JSON format and try again.")
                    time.sleep(5)
                    continue

            # æ£€æŸ¥è¶…æ—¶
            elapsed_time = time.time() - start_wait_time
            if elapsed_time > self.annotation_timeout:
                print(f"\nâ° Annotation timeout ({self.annotation_timeout//60} minutes)")
                print("Using simulated annotations for this iteration...")
                return self._simulate_annotation([])  # ç©ºçš„æ¨¡æ‹Ÿæ ‡æ³¨

            # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
            time.sleep(10)
            print(f"â³ Waiting... ({elapsed_time:.0f}s elapsed)")

    def _simulate_annotation(self, selected_indices: List[int]) -> List[HumanAnnotationResult]:
        """æ¨¡æ‹Ÿäººå·¥æ ‡æ³¨ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        annotations = []

        # è·å–çœŸå®æ ‡ç­¾è¿›è¡Œæ¨¡æ‹Ÿ
        test_loader = self.datamodule.test_dataloader()
        true_labels = []

        for _, labels in test_loader:
            true_labels.extend(labels.numpy())

        for idx in selected_indices:
            if idx < len(true_labels):
                annotation = HumanAnnotationResult(
                    sample_id=self.test_sample_ids[idx], label=int(true_labels[idx]), confidence=0.95  # æ¨¡æ‹Ÿé«˜ç½®ä¿¡åº¦
                )
                annotations.append(annotation)

        logger.info(f"ğŸ¤– Simulated {len(annotations)} annotations")
        return annotations

    def _estimate_uncertainty(self, model: pl.LightningModule) -> np.ndarray:
        """MC Dropoutä¸ç¡®å®šæ€§ä¼°è®¡"""
        model.eval()

        # å¯ç”¨dropout
        for module in model.modules():
            if isinstance(module, torch.nn.Dropout):
                module.train()

        test_loader = self.datamodule.test_dataloader()
        predictions = []
        device = next(model.parameters()).device

        with torch.no_grad():
            for data, _ in test_loader:
                data = data.to(device)

                batch_preds = []
                for _ in range(self.n_mc_passes):
                    logits = model(data)
                    probs = F.softmax(logits, dim=1)
                    batch_preds.append(probs.cpu().numpy())

                batch_preds = np.stack(batch_preds)
                predictions.append(batch_preds)

        all_predictions = np.concatenate(predictions, axis=1)
        pred_variance = np.var(all_predictions, axis=0)
        uncertainty_scores = np.mean(pred_variance, axis=1)

        logger.info(f"âœ… Estimated uncertainty for {len(uncertainty_scores)} samples")
        return uncertainty_scores

    def _select_active_samples(self, uncertainty_scores: np.ndarray) -> List[int]:
        """é€‰æ‹©æœ€ä¸ç¡®å®šçš„æ ·æœ¬"""
        selected_indices = np.argsort(uncertainty_scores)[-self.annotation_budget :].tolist()
        logger.info(f"ğŸ¯ Selected {len(selected_indices)} most uncertain samples")
        return selected_indices

    def _generate_pseudo_labels(self, model: pl.LightningModule) -> List[Tuple[str, int]]:
        """ç”Ÿæˆé«˜ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾"""
        model.eval()
        test_loader = self.datamodule.test_dataloader()
        pseudo_labels = []
        device = next(model.parameters()).device

        with torch.no_grad():
            sample_idx = 0
            for data, _ in test_loader:
                data = data.to(device)
                logits = model(data)
                probs = F.softmax(logits, dim=1)

                max_probs = torch.max(probs, dim=1)[0]
                predicted_labels = torch.argmax(probs, dim=1)

                high_conf_mask = max_probs > self.confidence_threshold

                for i, (is_high_conf, pred_label) in enumerate(zip(high_conf_mask, predicted_labels)):
                    if is_high_conf:
                        sample_id = self.test_sample_ids[sample_idx + i]
                        pseudo_labels.append((sample_id, pred_label.item()))

                sample_idx += len(data)

        logger.info(f"ğŸ·ï¸ Generated {len(pseudo_labels)} pseudo labels")
        return pseudo_labels

    def _update_training_data(self, annotations: List[HumanAnnotationResult], pseudo_labels: List[Tuple[str, int]]):
        """æ›´æ–°è®­ç»ƒæ•°æ®ï¼ˆå®é™…å®ç°ä¸­éœ€è¦æ ¹æ®å…·ä½“æ•°æ®ç»“æ„è°ƒæ•´ï¼‰"""
        # è¿™é‡Œæ˜¯ç®€åŒ–å®ç°ï¼Œå®é™…ä¸­éœ€è¦ï¼š
        # 1. å°†äººå·¥æ ‡æ³¨æ ·æœ¬æ·»åŠ åˆ°è®­ç»ƒé›†
        # 2. å°†ä¼ªæ ‡ç­¾æ ·æœ¬æ·»åŠ åˆ°è®­ç»ƒé›†
        # 3. é‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨

        logger.info(f"ğŸ“ Added {len(annotations)} human annotations")
        logger.info(f"ğŸ·ï¸ Added {len(pseudo_labels)} pseudo labels")

        # ä¿å­˜æ ‡æ³¨å†å²
        annotation_history = {
            "human_annotations": [
                {"sample_id": ann.sample_id, "label": ann.label, "confidence": ann.confidence} for ann in annotations
            ],
            "pseudo_labels": [{"sample_id": sample_id, "label": label} for sample_id, label in pseudo_labels],
        }

        history_file = self.annotation_dir / f"annotation_history_{int(time.time())}.json"
        with open(history_file, "w") as f:
            json.dump(annotation_history, f, indent=2)

    def _train_model(self, name: str) -> pl.LightningModule:
        """è®­ç»ƒæ¨¡å‹"""
        model = instantiate_from_config(self.config["model"])

        callbacks = [
            ModelCheckpoint(
                dirpath=str(self.output_dir / "checkpoints"),
                filename=f"{name}_{{epoch:02d}}_{{val_f1:.4f}}",
                monitor="val_f1",
                mode="max",
                save_top_k=1,
            ),
            EarlyStopping(monitor="val_f1", patience=8, mode="max", verbose=False),
        ]

        trainer = pl.Trainer(
            max_epochs=15,
            accelerator="auto",
            devices="auto",
            precision="32-true",
            enable_progress_bar=True,  # å¯ä»¥å¯ç”¨è¿›åº¦æ¡
            enable_model_summary=False,
            logger=False,
            callbacks=callbacks,
        )

        trainer.fit(model, self.datamodule)
        return model

    def _evaluate_model(self, model: pl.LightningModule) -> float:
        """è¯„ä¼°æ¨¡å‹"""
        eval_trainer = pl.Trainer(
            accelerator="auto", devices="auto", enable_progress_bar=False, enable_model_summary=False, logger=False
        )

        results = eval_trainer.validate(model, self.datamodule, verbose=False)
        f1_score = results[0].get("val_f1", 0.0) if results else 0.0
        return f1_score


def create_human_guided_active_learning(config: Dict, **kwargs) -> HumanGuidedActiveLearning:
    """åˆ›å»ºäººå·¥æŒ‡å¯¼çš„ä¸»åŠ¨å­¦ä¹ è®­ç»ƒå™¨"""
    return HumanGuidedActiveLearning(config, **kwargs)
