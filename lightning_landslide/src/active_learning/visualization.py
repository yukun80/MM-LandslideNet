# =============================================================================
# lightning_landslide/src/active_learning/visualization.py
# =============================================================================

"""
ä¸»åŠ¨å­¦ä¹ å¯è§†åŒ–åˆ†æå’Œç›‘æ§å·¥å…·

è¿™ä¸ªæ¨¡å—æä¾›äº†ä¸°å¯Œçš„å¯è§†åŒ–åŠŸèƒ½ï¼Œå¸®åŠ©ç†è§£å’Œç›‘æ§ä¸»åŠ¨å­¦ä¹ è¿‡ç¨‹ï¼š
1. å®æ—¶è®­ç»ƒç›‘æ§
2. ä¸ç¡®å®šæ€§åˆ†æå¯è§†åŒ–
3. æ•°æ®åˆ†å¸ƒå˜åŒ–è¿½è¸ª
4. æ€§èƒ½è¶‹åŠ¿åˆ†æ
5. äº¤äº’å¼æŠ¥å‘Šç”Ÿæˆ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import logging
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import json
from datetime import datetime
from dataclasses import dataclass
import warnings

warnings.filterwarnings("ignore")

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams["font.sans-serif"] = ["SimHei", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False
sns.set_style("whitegrid")

logger = logging.getLogger(__name__)


@dataclass
class VisualizationConfig:
    """å¯è§†åŒ–é…ç½®"""

    style: str = "seaborn"
    color_palette: str = "husl"
    figure_size: Tuple[int, int] = (12, 8)
    dpi: int = 300
    save_format: str = "png"
    interactive: bool = True


class ActiveLearningVisualizer:
    """
    ä¸»åŠ¨å­¦ä¹ å¯è§†åŒ–å™¨

    æä¾›å…¨é¢çš„å¯è§†åŒ–åˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬é™æ€å›¾è¡¨å’Œäº¤äº’å¼æŠ¥å‘Šã€‚
    """

    def __init__(self, output_dir: Path, config: VisualizationConfig = None):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨

        Args:
            output_dir: è¾“å‡ºç›®å½•
            config: å¯è§†åŒ–é…ç½®
        """
        self.output_dir = Path(output_dir)
        self.config = config or VisualizationConfig()

        # åˆ›å»ºå¯è§†åŒ–ç›®å½•
        self.viz_dir = self.output_dir / "visualizations"
        self.viz_dir.mkdir(parents=True, exist_ok=True)

        # è®¾ç½®æ ·å¼
        self._setup_style()

        logger.info(f"ğŸ“Š ActiveLearningVisualizer initialized: {self.viz_dir}")

    def _setup_style(self):
        """è®¾ç½®å¯è§†åŒ–æ ·å¼"""
        if self.config.style == "seaborn":
            sns.set_style("whitegrid")
            sns.set_palette(self.config.color_palette)

        plt.rcParams["figure.figsize"] = self.config.figure_size
        plt.rcParams["figure.dpi"] = self.config.dpi

    def create_training_overview(
        self,
        performance_history: Dict[str, List[float]],
        data_usage_history: Dict[str, List[int]],
        iteration_results: List[Dict],
    ) -> str:
        """
        åˆ›å»ºè®­ç»ƒè¿‡ç¨‹æ€»è§ˆ

        Args:
            performance_history: æ€§èƒ½å†å²æ•°æ®
            data_usage_history: æ•°æ®ä½¿ç”¨å†å²
            iteration_results: è¿­ä»£ç»“æœåˆ—è¡¨

        Returns:
            ä¿å­˜çš„å›¾ç‰‡è·¯å¾„
        """
        logger.info("ğŸ“ˆ Creating training overview visualization...")

        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        fig.suptitle("Active Learning Training Overview", fontsize=16, fontweight="bold")

        iterations = list(range(1, len(performance_history["val_f1"]) + 1))

        # 1. æ€§èƒ½æ”¹è¿›æ›²çº¿
        axes[0, 0].plot(
            iterations, performance_history["val_f1"], "b-o", linewidth=2, markersize=6, label="Validation F1"
        )
        axes[0, 0].plot(
            iterations, performance_history.get("train_f1", []), "g-s", linewidth=2, markersize=6, label="Training F1"
        )
        axes[0, 0].set_title("Model Performance Over Iterations", fontweight="bold")
        axes[0, 0].set_xlabel("Iteration")
        axes[0, 0].set_ylabel("F1 Score")
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_ylim(0, 1)

        # æ·»åŠ æ€§èƒ½æå‡æ ‡æ³¨
        if len(performance_history["val_f1"]) > 1:
            improvement = performance_history["val_f1"][-1] - performance_history["val_f1"][0]
            axes[0, 0].annotate(
                f"Improvement: +{improvement:.3f}",
                xy=(iterations[-1], performance_history["val_f1"][-1]),
                xytext=(10, 10),
                textcoords="offset points",
                bbox=dict(boxstyle="round,pad=0.5", fc="yellow", alpha=0.7),
                arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=0"),
            )

        # 2. æ•°æ®å¢é•¿è¶‹åŠ¿
        axes[0, 1].plot(
            iterations, data_usage_history["training_samples"], "r-o", linewidth=2, markersize=6, label="Total Training"
        )
        axes[0, 1].plot(
            iterations,
            data_usage_history.get("pseudo_labels", []),
            "orange",
            marker="s",
            linewidth=2,
            markersize=6,
            label="Pseudo Labels",
        )
        axes[0, 1].plot(
            iterations,
            data_usage_history.get("new_annotations", []),
            "purple",
            marker="^",
            linewidth=2,
            markersize=6,
            label="New Annotations",
        )
        axes[0, 1].set_title("Data Usage Over Iterations", fontweight="bold")
        axes[0, 1].set_xlabel("Iteration")
        axes[0, 1].set_ylabel("Number of Samples")
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # 3. æŸå¤±ä¸‹é™
        if "val_loss" in performance_history:
            axes[0, 2].plot(
                iterations, performance_history["val_loss"], "r-o", linewidth=2, markersize=6, label="Validation Loss"
            )
            axes[0, 2].set_title("Loss Reduction Over Iterations", fontweight="bold")
            axes[0, 2].set_xlabel("Iteration")
            axes[0, 2].set_ylabel("Loss")
            axes[0, 2].legend()
            axes[0, 2].grid(True, alpha=0.3)

        # 4. æ•°æ®æ•ˆç‡åˆ†æ
        if len(performance_history["val_f1"]) > 1:
            sample_counts = data_usage_history["training_samples"]
            f1_scores = performance_history["val_f1"]

            axes[1, 0].scatter(
                sample_counts,
                f1_scores,
                c=range(len(sample_counts)),
                cmap="viridis",
                s=80,
                alpha=0.7,
                edgecolors="black",
                linewidth=1,
            )
            axes[1, 0].plot(sample_counts, f1_scores, "b--", alpha=0.5, linewidth=1)
            axes[1, 0].set_title("Performance vs Data Usage", fontweight="bold")
            axes[1, 0].set_xlabel("Training Samples")
            axes[1, 0].set_ylabel("F1 Score")
            axes[1, 0].grid(True, alpha=0.3)

            # æ·»åŠ é¢œè‰²æ¡
            cbar = plt.colorbar(axes[1, 0].collections[0], ax=axes[1, 0])
            cbar.set_label("Iteration")

        # 5. è®­ç»ƒæ—¶é—´åˆ†æ
        if iteration_results:
            training_times = [result.get("training_time", 0) for result in iteration_results]
            cumulative_times = np.cumsum(training_times)

            axes[1, 1].bar(
                iterations,
                training_times,
                alpha=0.7,
                color="lightblue",
                edgecolor="navy",
                linewidth=1,
                label="Per Iteration",
            )
            axes[1, 1].plot(iterations, cumulative_times, "r-o", linewidth=2, markersize=6, label="Cumulative")
            axes[1, 1].set_title("Training Time Analysis", fontweight="bold")
            axes[1, 1].set_xlabel("Iteration")
            axes[1, 1].set_ylabel("Time (seconds)")
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)

        # 6. æ•°æ®ç»„æˆé¥¼å›¾
        if iteration_results:
            latest_result = iteration_results[-1]
            total_samples = latest_result.get("total_training_samples", 0)
            pseudo_count = latest_result.get("pseudo_label_count", 0)
            new_annotations = latest_result.get("new_annotations_count", 0)
            original_count = total_samples - pseudo_count - new_annotations

            sizes = [original_count, pseudo_count, new_annotations]
            labels = ["Original", "Pseudo Labels", "New Annotations"]
            colors = ["#ff9999", "#66b3ff", "#99ff99"]

            wedges, texts, autotexts = axes[1, 2].pie(
                sizes, labels=labels, colors=colors, autopct="%1.1f%%", startangle=90, explode=(0.05, 0.05, 0.05)
            )
            axes[1, 2].set_title("Final Data Composition", fontweight="bold")

            # ç¾åŒ–é¥¼å›¾
            for autotext in autotexts:
                autotext.set_color("white")
                autotext.set_fontweight("bold")

        plt.tight_layout()

        # ä¿å­˜å›¾ç‰‡
        output_path = self.viz_dir / "training_overview.png"
        plt.savefig(output_path, dpi=self.config.dpi, bbox_inches="tight", facecolor="white", edgecolor="none")
        plt.close()

        logger.info(f"ğŸ’¾ Training overview saved: {output_path}")
        return str(output_path)

    def create_uncertainty_analysis(self, uncertainty_results: Dict, pseudo_label_results: Dict, iteration: int) -> str:
        """
        åˆ›å»ºä¸ç¡®å®šæ€§åˆ†æå›¾è¡¨

        Args:
            uncertainty_results: ä¸ç¡®å®šæ€§ä¼°è®¡ç»“æœ
            pseudo_label_results: ä¼ªæ ‡ç­¾ç”Ÿæˆç»“æœ
            iteration: è¿­ä»£è½®æ¬¡

        Returns:
            ä¿å­˜çš„å›¾ç‰‡è·¯å¾„
        """
        logger.info(f"ğŸ² Creating uncertainty analysis for iteration {iteration}...")

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f"Uncertainty Analysis - Iteration {iteration}", fontsize=16, fontweight="bold")

        # ä»ç»“æœä¸­æå–æ•°æ®
        uncertainty_scores = uncertainty_results.get("uncertainty_scores", [])
        confidence_scores = uncertainty_results.get("confidence_scores", [])
        predictions = uncertainty_results.get("predictions", [])

        if len(uncertainty_scores) == 0:
            logger.warning("No uncertainty data available for visualization")
            return ""

        # 1. ä¸ç¡®å®šæ€§åˆ†å¸ƒ
        axes[0, 0].hist(uncertainty_scores, bins=30, alpha=0.7, color="skyblue", edgecolor="black", density=True)
        axes[0, 0].axvline(
            np.mean(uncertainty_scores),
            color="red",
            linestyle="--",
            linewidth=2,
            label=f"Mean: {np.mean(uncertainty_scores):.3f}",
        )
        axes[0, 0].set_title("Uncertainty Score Distribution", fontweight="bold")
        axes[0, 0].set_xlabel("Uncertainty Score")
        axes[0, 0].set_ylabel("Density")
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # 2. ç½®ä¿¡åº¦åˆ†å¸ƒ
        axes[0, 1].hist(confidence_scores, bins=30, alpha=0.7, color="lightgreen", edgecolor="black", density=True)
        axes[0, 1].axvline(
            np.mean(confidence_scores),
            color="red",
            linestyle="--",
            linewidth=2,
            label=f"Mean: {np.mean(confidence_scores):.3f}",
        )
        axes[0, 1].set_title("Confidence Score Distribution", fontweight="bold")
        axes[0, 1].set_xlabel("Confidence Score")
        axes[0, 1].set_ylabel("Density")
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # 3. ç½®ä¿¡åº¦vsä¸ç¡®å®šæ€§æ•£ç‚¹å›¾
        scatter = axes[0, 2].scatter(
            confidence_scores,
            uncertainty_scores,
            alpha=0.6,
            c=confidence_scores,
            cmap="viridis",
            s=30,
            edgecolors="black",
            linewidth=0.5,
        )
        axes[0, 2].set_title("Confidence vs Uncertainty", fontweight="bold")
        axes[0, 2].set_xlabel("Confidence Score")
        axes[0, 2].set_ylabel("Uncertainty Score")
        axes[0, 2].grid(True, alpha=0.3)
        plt.colorbar(scatter, ax=axes[0, 2], label="Confidence")

        # 4. ä¼ªæ ‡ç­¾è´¨é‡åˆ†æ
        if pseudo_label_results and "high_confidence_samples" in pseudo_label_results:
            high_conf_samples = pseudo_label_results["high_confidence_samples"]
            if high_conf_samples:
                quality_scores = [s.get("quality_score", 0) for s in high_conf_samples]
                axes[1, 0].hist(quality_scores, bins=20, alpha=0.7, color="gold", edgecolor="black", density=True)
                axes[1, 0].axvline(
                    np.mean(quality_scores),
                    color="red",
                    linestyle="--",
                    linewidth=2,
                    label=f"Mean: {np.mean(quality_scores):.3f}",
                )
                axes[1, 0].set_title("Pseudo Label Quality Distribution", fontweight="bold")
                axes[1, 0].set_xlabel("Quality Score")
                axes[1, 0].set_ylabel("Density")
                axes[1, 0].legend()
                axes[1, 0].grid(True, alpha=0.3)

        # 5. é¢„æµ‹ç±»åˆ«åˆ†å¸ƒ
        if len(predictions) > 0:
            if predictions[0].ndim > 1:  # å¤šç±»åˆ«é¢„æµ‹
                predicted_classes = np.argmax(predictions, axis=1)
            else:  # äºŒåˆ†ç±»
                predicted_classes = (np.array(predictions) > 0.5).astype(int)

            class_counts = np.bincount(predicted_classes)
            class_labels = [f"Class {i}" for i in range(len(class_counts))]

            bars = axes[1, 1].bar(
                class_labels,
                class_counts,
                alpha=0.7,
                color=["#ff7f0e", "#2ca02c", "#d62728", "#9467bd"][: len(class_counts)],
                edgecolor="black",
                linewidth=1,
            )
            axes[1, 1].set_title("Predicted Class Distribution", fontweight="bold")
            axes[1, 1].set_xlabel("Class")
            axes[1, 1].set_ylabel("Count")
            axes[1, 1].grid(True, alpha=0.3)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, count in zip(bars, class_counts):
                axes[1, 1].text(
                    bar.get_x() + bar.get_width() / 2,
                    bar.get_height() + max(class_counts) * 0.01,
                    str(count),
                    ha="center",
                    va="bottom",
                    fontweight="bold",
                )

        # 6. æ ·æœ¬é€‰æ‹©ç»Ÿè®¡
        if pseudo_label_results:
            stats = pseudo_label_results.get("statistics", {})
            categories = ["High Conf.", "Low Conf.", "Excluded"]
            counts = [
                stats.get("high_confidence_count", 0),
                stats.get("low_confidence_count", 0),
                stats.get("excluded_count", 0),
            ]
            colors = ["green", "orange", "red"]

            bars = axes[1, 2].bar(categories, counts, alpha=0.7, color=colors, edgecolor="black", linewidth=1)
            axes[1, 2].set_title("Sample Selection Results", fontweight="bold")
            axes[1, 2].set_xlabel("Category")
            axes[1, 2].set_ylabel("Count")
            axes[1, 2].grid(True, alpha=0.3)

            # æ·»åŠ ç™¾åˆ†æ¯”æ ‡ç­¾
            total = sum(counts) if sum(counts) > 0 else 1
            for bar, count in zip(bars, counts):
                percentage = count / total * 100
                axes[1, 2].text(
                    bar.get_x() + bar.get_width() / 2,
                    bar.get_height() + max(counts) * 0.01,
                    f"{count}\n({percentage:.1f}%)",
                    ha="center",
                    va="bottom",
                    fontweight="bold",
                )

        plt.tight_layout()

        # ä¿å­˜å›¾ç‰‡
        output_path = self.viz_dir / f"uncertainty_analysis_iter_{iteration}.png"
        plt.savefig(output_path, dpi=self.config.dpi, bbox_inches="tight", facecolor="white", edgecolor="none")
        plt.close()

        logger.info(f"ğŸ’¾ Uncertainty analysis saved: {output_path}")
        return str(output_path)

    def create_interactive_dashboard(self, complete_results: Dict, experiment_name: str) -> str:
        """
        åˆ›å»ºäº¤äº’å¼ä»ªè¡¨æ¿

        Args:
            complete_results: å®Œæ•´çš„å®éªŒç»“æœ
            experiment_name: å®éªŒåç§°

        Returns:
            ä¿å­˜çš„HTMLæ–‡ä»¶è·¯å¾„
        """
        if not self.config.interactive:
            logger.info("Interactive visualization disabled")
            return ""

        logger.info("ğŸ›ï¸ Creating interactive dashboard...")

        # åˆ›å»ºå­å›¾
        fig = make_subplots(
            rows=3,
            cols=2,
            subplot_titles=(
                "Performance Over Time",
                "Data Usage Growth",
                "Training Time Analysis",
                "Uncertainty Distribution",
                "Data Composition",
                "Performance Correlation",
            ),
            specs=[
                [{"secondary_y": True}, {"secondary_y": False}],
                [{"secondary_y": False}, {"secondary_y": False}],
                [{"type": "pie"}, {"secondary_y": False}],
            ],
        )

        # æå–æ•°æ®
        performance_history = complete_results.get("performance_history", {})
        data_usage_history = complete_results.get("data_usage_history", {})
        iteration_results = complete_results.get("iteration_results", [])

        iterations = list(range(1, len(performance_history.get("val_f1", [])) + 1))

        if len(iterations) == 0:
            logger.warning("No data available for interactive dashboard")
            return ""

        # 1. æ€§èƒ½éšæ—¶é—´å˜åŒ–
        fig.add_trace(
            go.Scatter(
                x=iterations,
                y=performance_history.get("val_f1", []),
                mode="lines+markers",
                name="Validation F1",
                line=dict(color="blue", width=3),
                marker=dict(size=8),
            ),
            row=1,
            col=1,
        )

        if "train_f1" in performance_history:
            fig.add_trace(
                go.Scatter(
                    x=iterations,
                    y=performance_history["train_f1"],
                    mode="lines+markers",
                    name="Training F1",
                    line=dict(color="green", width=2),
                    marker=dict(size=6),
                ),
                row=1,
                col=1,
            )

        # 2. æ•°æ®ä½¿ç”¨å¢é•¿
        fig.add_trace(
            go.Scatter(
                x=iterations,
                y=data_usage_history.get("training_samples", []),
                mode="lines+markers",
                name="Total Training",
                line=dict(color="red", width=3),
                marker=dict(size=8),
            ),
            row=1,
            col=2,
        )

        if "pseudo_labels" in data_usage_history:
            fig.add_trace(
                go.Scatter(
                    x=iterations,
                    y=data_usage_history["pseudo_labels"],
                    mode="lines+markers",
                    name="Pseudo Labels",
                    line=dict(color="orange", width=2),
                    marker=dict(size=6),
                ),
                row=1,
                col=2,
            )

        # 3. è®­ç»ƒæ—¶é—´åˆ†æ
        if iteration_results:
            training_times = [result.get("training_time", 0) for result in iteration_results]
            fig.add_trace(
                go.Bar(x=iterations, y=training_times, name="Training Time", marker_color="lightblue"), row=2, col=1
            )

        # 4. ä¸ç¡®å®šæ€§åˆ†å¸ƒï¼ˆæœ€æ–°è¿­ä»£ï¼‰
        if iteration_results:
            latest_uncertainty = iteration_results[-1].get("uncertainty_results", {})
            uncertainty_scores = latest_uncertainty.get("uncertainty_scores", [])

            if len(uncertainty_scores) > 0:
                fig.add_trace(
                    go.Histogram(
                        x=uncertainty_scores, name="Uncertainty Distribution", marker_color="skyblue", opacity=0.7
                    ),
                    row=2,
                    col=2,
                )

        # 5. æ•°æ®ç»„æˆé¥¼å›¾
        if iteration_results:
            latest_result = iteration_results[-1]
            total_samples = latest_result.get("total_training_samples", 0)
            pseudo_count = latest_result.get("pseudo_label_count", 0)
            new_annotations = latest_result.get("new_annotations_count", 0)
            original_count = total_samples - pseudo_count - new_annotations

            fig.add_trace(
                go.Pie(
                    values=[original_count, pseudo_count, new_annotations],
                    labels=["Original", "Pseudo Labels", "New Annotations"],
                    name="Data Composition",
                ),
                row=3,
                col=1,
            )

        # 6. æ€§èƒ½ç›¸å…³æ€§åˆ†æ
        if len(iterations) > 1:
            sample_counts = data_usage_history.get("training_samples", [])
            f1_scores = performance_history.get("val_f1", [])

            fig.add_trace(
                go.Scatter(
                    x=sample_counts,
                    y=f1_scores,
                    mode="markers+lines",
                    name="Performance vs Data",
                    marker=dict(
                        size=10,
                        color=iterations,
                        colorscale="viridis",
                        showscale=True,
                        colorbar=dict(title="Iteration"),
                    ),
                    line=dict(color="gray", width=1, dash="dash"),
                ),
                row=3,
                col=2,
            )

        # æ›´æ–°å¸ƒå±€
        fig.update_layout(
            title_text=f"Active Learning Interactive Dashboard - {experiment_name}",
            title_x=0.5,
            title_font_size=20,
            showlegend=True,
            height=1200,
            template="plotly_white",
        )

        # ä¿å­˜äº¤äº’å¼å›¾è¡¨
        output_path = self.viz_dir / "interactive_dashboard.html"
        fig.write_html(str(output_path))

        logger.info(f"ğŸ’¾ Interactive dashboard saved: {output_path}")
        return str(output_path)

    def create_comparison_report(self, multiple_results: Dict[str, Dict], baseline_name: str = "baseline") -> str:
        """
        åˆ›å»ºå¤šå®éªŒå¯¹æ¯”æŠ¥å‘Š

        Args:
            multiple_results: å¤šä¸ªå®éªŒç»“æœå­—å…¸
            baseline_name: åŸºçº¿å®éªŒåç§°

        Returns:
            ä¿å­˜çš„å›¾ç‰‡è·¯å¾„
        """
        logger.info("âš–ï¸ Creating comparison report...")

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle("Active Learning Methods Comparison", fontsize=16, fontweight="bold")

        experiment_names = list(multiple_results.keys())
        n_experiments = len(experiment_names)

        if n_experiments < 2:
            logger.warning("Need at least 2 experiments for comparison")
            return ""

        # æå–å¯¹æ¯”æ•°æ®
        final_performances = []
        total_training_times = []
        data_efficiency_scores = []
        convergence_iterations = []

        for name, result in multiple_results.items():
            # æœ€ç»ˆæ€§èƒ½
            performance_history = result.get("performance_history", {})
            val_f1_history = performance_history.get("val_f1", [])
            if val_f1_history:
                final_performances.append(val_f1_history[-1])
            else:
                final_performances.append(0)

            # æ€»è®­ç»ƒæ—¶é—´
            total_time = result.get("total_training_time", 0)
            total_training_times.append(total_time)

            # æ•°æ®æ•ˆç‡ï¼ˆæ€§èƒ½æå‡/æ•°æ®å¢é•¿æ¯”ä¾‹ï¼‰
            data_usage = result.get("data_usage_history", {})
            training_samples = data_usage.get("training_samples", [])
            if len(val_f1_history) > 1 and len(training_samples) > 1:
                perf_improvement = val_f1_history[-1] - val_f1_history[0]
                data_growth_ratio = training_samples[-1] / training_samples[0]
                efficiency = perf_improvement / (data_growth_ratio - 1) if data_growth_ratio > 1 else 0
                data_efficiency_scores.append(efficiency)
            else:
                data_efficiency_scores.append(0)

            # æ”¶æ•›è¿­ä»£æ¬¡æ•°
            convergence_iter = result.get("convergence_iteration", len(val_f1_history))
            convergence_iterations.append(convergence_iter)

        # 1. æœ€ç»ˆæ€§èƒ½å¯¹æ¯”
        colors = plt.cm.Set3(np.linspace(0, 1, n_experiments))
        bars1 = axes[0, 0].bar(
            experiment_names, final_performances, color=colors, alpha=0.8, edgecolor="black", linewidth=1
        )
        axes[0, 0].set_title("Final Performance Comparison", fontweight="bold")
        axes[0, 0].set_ylabel("F1 Score")
        axes[0, 0].tick_params(axis="x", rotation=45)
        axes[0, 0].grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, perf in zip(bars1, final_performances):
            axes[0, 0].text(
                bar.get_x() + bar.get_width() / 2,
                bar.get_height() + 0.005,
                f"{perf:.3f}",
                ha="center",
                va="bottom",
                fontweight="bold",
            )

        # 2. è®­ç»ƒæ—¶é—´å¯¹æ¯”
        bars2 = axes[0, 1].bar(
            experiment_names, total_training_times, color=colors, alpha=0.8, edgecolor="black", linewidth=1
        )
        axes[0, 1].set_title("Training Time Comparison", fontweight="bold")
        axes[0, 1].set_ylabel("Time (seconds)")
        axes[0, 1].tick_params(axis="x", rotation=45)
        axes[0, 1].grid(True, alpha=0.3)

        # 3. æ•°æ®æ•ˆç‡å¯¹æ¯”
        bars3 = axes[1, 0].bar(
            experiment_names, data_efficiency_scores, color=colors, alpha=0.8, edgecolor="black", linewidth=1
        )
        axes[1, 0].set_title("Data Efficiency Comparison", fontweight="bold")
        axes[1, 0].set_ylabel("Efficiency Score")
        axes[1, 0].tick_params(axis="x", rotation=45)
        axes[1, 0].grid(True, alpha=0.3)

        # 4. æ”¶æ•›é€Ÿåº¦å¯¹æ¯”
        bars4 = axes[1, 1].bar(
            experiment_names, convergence_iterations, color=colors, alpha=0.8, edgecolor="black", linewidth=1
        )
        axes[1, 1].set_title("Convergence Speed Comparison", fontweight="bold")
        axes[1, 1].set_ylabel("Iterations to Convergence")
        axes[1, 1].tick_params(axis="x", rotation=45)
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()

        # ä¿å­˜å¯¹æ¯”å›¾è¡¨
        output_path = self.viz_dir / "methods_comparison.png"
        plt.savefig(output_path, dpi=self.config.dpi, bbox_inches="tight", facecolor="white", edgecolor="none")
        plt.close()

        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_df = pd.DataFrame(
            {
                "Experiment": experiment_names,
                "Final F1": final_performances,
                "Training Time (s)": total_training_times,
                "Data Efficiency": data_efficiency_scores,
                "Convergence Iterations": convergence_iterations,
            }
        )

        # è®¡ç®—ç›¸å¯¹äºåŸºçº¿çš„æ”¹è¿›
        if baseline_name in experiment_names:
            baseline_idx = experiment_names.index(baseline_name)
            baseline_f1 = final_performances[baseline_idx]
            comparison_df["F1 Improvement"] = comparison_df["Final F1"] - baseline_f1
            comparison_df["F1 Improvement %"] = (comparison_df["F1 Improvement"] / baseline_f1 * 100).round(2)

        # ä¿å­˜è¡¨æ ¼
        table_path = self.viz_dir / "methods_comparison.csv"
        comparison_df.to_csv(table_path, index=False)

        logger.info(f"ğŸ’¾ Comparison report saved: {output_path}")
        logger.info(f"ğŸ“Š Comparison table saved: {table_path}")

        return str(output_path)

    def create_summary_report(self, complete_results: Dict, experiment_name: str) -> str:
        """
        åˆ›å»ºå®éªŒæ€»ç»“æŠ¥å‘Š

        Args:
            complete_results: å®Œæ•´å®éªŒç»“æœ
            experiment_name: å®éªŒåç§°

        Returns:
            ä¿å­˜çš„æŠ¥å‘Šè·¯å¾„
        """
        logger.info("ğŸ“‹ Creating summary report...")

        # åˆ›å»ºHTMLæŠ¥å‘Š
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Active Learning Experiment Report - {experiment_name}</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 10px; }}
                .section {{ margin: 20px 0; }}
                .metric {{ display: inline-block; margin: 10px; padding: 15px; 
                          background-color: #e8f4fd; border-radius: 5px; min-width: 120px; }}
                .metric-value {{ font-size: 24px; font-weight: bold; color: #2c5aa0; }}
                .metric-label {{ font-size: 14px; color: #666; }}
                table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
                th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
                .improvement {{ color: green; font-weight: bold; }}
                .decline {{ color: red; font-weight: bold; }}
            </style>
        </head>
        <body>
        """

        # æŠ¥å‘Šå¤´éƒ¨
        html_content += f"""
        <div class="header">
            <h1>ğŸ¯ Active Learning Experiment Report</h1>
            <h2>{experiment_name}</h2>
            <p><strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        """

        # å…³é”®æŒ‡æ ‡
        performance_history = complete_results.get("performance_history", {})
        data_usage_history = complete_results.get("data_usage_history", {})

        if performance_history.get("val_f1"):
            initial_f1 = performance_history["val_f1"][0]
            final_f1 = performance_history["val_f1"][-1]
            improvement = final_f1 - initial_f1

            html_content += f"""
            <div class="section">
                <h3>ğŸ“Š Key Performance Metrics</h3>
                <div class="metric">
                    <div class="metric-value">{final_f1:.4f}</div>
                    <div class="metric-label">Final F1 Score</div>
                </div>
                <div class="metric">
                    <div class="metric-value {'improvement' if improvement > 0 else 'decline'}">
                        {improvement:+.4f}
                    </div>
                    <div class="metric-label">Performance Improvement</div>
                </div>
                <div class="metric">
                    <div class="metric-value">{complete_results.get('convergence_iteration', 0)}</div>
                    <div class="metric-label">Iterations to Convergence</div>
                </div>
                <div class="metric">
                    <div class="metric-value">{complete_results.get('total_training_time', 0):.0f}s</div>
                    <div class="metric-label">Total Training Time</div>
                </div>
            </div>
            """

        # æ•°æ®ä½¿ç”¨ç»Ÿè®¡
        if data_usage_history.get("training_samples"):
            initial_samples = data_usage_history["training_samples"][0]
            final_samples = data_usage_history["training_samples"][-1]
            data_growth = final_samples - initial_samples

            html_content += f"""
            <div class="section">
                <h3>ğŸ“ˆ Data Usage Analysis</h3>
                <table>
                    <tr><th>Metric</th><th>Value</th><th>Description</th></tr>
                    <tr><td>Initial Training Samples</td><td>{initial_samples:,}</td><td>Original labeled data</td></tr>
                    <tr><td>Final Training Samples</td><td>{final_samples:,}</td><td>Total after augmentation</td></tr>
                    <tr><td>Data Growth</td><td class="improvement">+{data_growth:,}</td><td>Additional samples added</td></tr>
                    <tr><td>Pseudo Labels</td><td>{data_usage_history.get('pseudo_labels', [0])[-1]:,}</td><td>High-confidence predictions</td></tr>
                    <tr><td>New Annotations</td><td>{data_usage_history.get('new_annotations', [0])[-1]:,}</td><td>Human-labeled samples</td></tr>
                </table>
            </div>
            """

        # è¿­ä»£è¯¦æƒ…
        iteration_results = complete_results.get("iteration_results", [])
        if iteration_results:
            html_content += """
            <div class="section">
                <h3>ğŸ”„ Iteration Details</h3>
                <table>
                    <tr><th>Iteration</th><th>Val F1</th><th>Training Samples</th><th>Pseudo Labels</th><th>Training Time</th></tr>
            """

            for i, result in enumerate(iteration_results):
                val_f1 = (
                    performance_history.get("val_f1", [0] * (i + 1))[i]
                    if i < len(performance_history.get("val_f1", []))
                    else 0
                )
                html_content += f"""
                <tr>
                    <td>{result.get('iteration', i+1)}</td>
                    <td>{val_f1:.4f}</td>
                    <td>{result.get('total_training_samples', 0):,}</td>
                    <td>{result.get('pseudo_label_count', 0):,}</td>
                    <td>{result.get('training_time', 0):.1f}s</td>
                </tr>
                """

            html_content += "</table></div>"

        # ç»“è®ºå’Œå»ºè®®
        html_content += f"""
        <div class="section">
            <h3>ğŸ’¡ Conclusions and Recommendations</h3>
            <ul>
                <li><strong>Performance:</strong> {'Successful improvement' if improvement > 0 else 'Limited improvement'} 
                    achieved through active learning strategy</li>
                <li><strong>Data Efficiency:</strong> Added {data_growth:,} samples with 
                    {data_usage_history.get('new_annotations', [0])[-1]:,} requiring human annotation</li>
                <li><strong>Convergence:</strong> Model converged after {complete_results.get('convergence_iteration', 0)} iterations</li>
                <li><strong>Scalability:</strong> Method {'scales well' if complete_results.get('convergence_iteration', 0) <= 5 else 'may need optimization'} 
                    for production use</li>
            </ul>
        </div>
        """

        html_content += """
        </body>
        </html>
        """

        # ä¿å­˜HTMLæŠ¥å‘Š
        report_path = self.viz_dir / f"{experiment_name}_summary_report.html"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        logger.info(f"ğŸ“‹ Summary report saved: {report_path}")
        return str(report_path)


def create_visualizer(output_dir: Path, config: Dict = None) -> ActiveLearningVisualizer:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºå¯è§†åŒ–å™¨

    Args:
        output_dir: è¾“å‡ºç›®å½•
        config: å¯è§†åŒ–é…ç½®

    Returns:
        å¯è§†åŒ–å™¨å®ä¾‹
    """
    viz_config = VisualizationConfig()
    if config:
        for key, value in config.items():
            if hasattr(viz_config, key):
                setattr(viz_config, key, value)

    return ActiveLearningVisualizer(output_dir, viz_config)
