# =============================================================================
# lightning_landslide/src/active_learning/active_learning_selector.py
# =============================================================================

"""
ä¸»åŠ¨å­¦ä¹ é€‰æ‹©å™¨ - æ™ºèƒ½é€‰æ‹©æœ€æœ‰ä»·å€¼çš„æ ·æœ¬è¿›è¡Œäººå·¥æ ‡æ³¨

è¿™ä¸ªæ¨¡å—å®ç°äº†å¤šç§ä¸»åŠ¨å­¦ä¹ ç­–ç•¥ï¼Œä»æ— æ ‡æ³¨æ•°æ®ä¸­é€‰æ‹©æœ€æœ‰ä»·å€¼çš„æ ·æœ¬
è¿›è¡Œäººå·¥æ ‡æ³¨ï¼Œä»¥æœ€å°çš„æ ‡æ³¨æˆæœ¬è·å¾—æœ€å¤§çš„æ¨¡å‹æ€§èƒ½æå‡ã€‚

æ ¸å¿ƒç­–ç•¥ï¼š
1. ä¸ç¡®å®šæ€§é‡‡æ ·ï¼šé€‰æ‹©æ¨¡å‹æœ€ä¸ç¡®å®šçš„æ ·æœ¬
2. å¤šæ ·æ€§é‡‡æ ·ï¼šé¿å…é€‰æ‹©å†—ä½™æ ·æœ¬ï¼Œä¿è¯ä»£è¡¨æ€§
3. é¢„æœŸæ¨¡å‹æ”¹å˜ï¼šé€‰æ‹©å¯¹æ¨¡å‹å‚æ•°å½±å“æœ€å¤§çš„æ ·æœ¬
4. æŸ¥è¯¢-by-committeeï¼šåˆ©ç”¨æ¨¡å‹å·®å¼‚é€‰æ‹©äº‰è®®æ ·æœ¬
5. æ··åˆç­–ç•¥ï¼šç»“åˆå¤šç§æ–¹æ³•çš„ä¼˜åŠ¿
"""

import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Optional, Union, Callable
from pathlib import Path
import json
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

from .uncertainty_estimator import UncertaintyResults
from .pseudo_label_generator import PseudoLabelSample

logger = logging.getLogger(__name__)


@dataclass
class ActiveLearningQuery:
    """ä¸»åŠ¨å­¦ä¹ æŸ¥è¯¢ç»“æœ"""

    sample_id: str
    uncertainty_score: float
    diversity_score: float
    expected_impact: float
    combined_score: float
    selection_reason: str


@dataclass
class ActiveLearningResults:
    """ä¸»åŠ¨å­¦ä¹ é€‰æ‹©ç»“æœ"""

    selected_samples: List[ActiveLearningQuery]
    candidate_pool: List[ActiveLearningQuery]
    selection_statistics: Dict
    diversity_analysis: Dict

    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "selected_samples": [asdict(s) for s in self.selected_samples],
            "candidate_pool": [asdict(s) for s in self.candidate_pool],
            "selection_statistics": self.selection_statistics,
            "diversity_analysis": self.diversity_analysis,
        }


class BaseActiveLearningStrategy(ABC):
    """ä¸»åŠ¨å­¦ä¹ ç­–ç•¥åŸºç±»"""

    def __init__(self, name: str):
        self.name = name

    @abstractmethod
    def score_samples(
        self, uncertainty_results: UncertaintyResults, feature_embeddings: Optional[np.ndarray] = None, **kwargs
    ) -> np.ndarray:
        """ä¸ºæ ·æœ¬è®¡ç®—ä¸»åŠ¨å­¦ä¹ åˆ†æ•°"""
        pass


class UncertaintyStrategy(BaseActiveLearningStrategy):
    """
    ä¸ç¡®å®šæ€§é‡‡æ ·ç­–ç•¥

    é€‰æ‹©æ¨¡å‹é¢„æµ‹ä¸ç¡®å®šæ€§æœ€é«˜çš„æ ·æœ¬ã€‚è¿™æ˜¯æœ€ç›´è§‚å’Œå¸¸ç”¨çš„ç­–ç•¥ï¼Œ
    åŸºäºçš„å‡è®¾æ˜¯ï¼šæ¨¡å‹ä¸ç¡®å®šçš„æ ·æœ¬å¾€å¾€æ˜¯æœ€éš¾çš„ï¼Œå­¦ä¹ è¿™äº›æ ·æœ¬
    èƒ½æœ€å¤§åŒ–æ¨¡å‹çš„æ”¹è¿›ã€‚
    """

    def __init__(self, uncertainty_method: str = "entropy"):
        super().__init__("uncertainty")
        self.uncertainty_method = uncertainty_method

    def score_samples(
        self, uncertainty_results: UncertaintyResults, feature_embeddings: Optional[np.ndarray] = None, **kwargs
    ) -> np.ndarray:
        """åŸºäºä¸ç¡®å®šæ€§ä¸ºæ ·æœ¬è¯„åˆ†"""

        if self.uncertainty_method == "entropy":
            return uncertainty_results.prediction_entropy
        elif self.uncertainty_method == "confidence":
            return 1.0 - uncertainty_results.confidence_scores  # ä½ç½®ä¿¡åº¦ = é«˜ä¸ç¡®å®šæ€§
        elif self.uncertainty_method == "combined":
            return uncertainty_results.uncertainty_scores
        else:
            raise ValueError(f"Unknown uncertainty method: {self.uncertainty_method}")


class DiversityStrategy(BaseActiveLearningStrategy):
    """
    å¤šæ ·æ€§é‡‡æ ·ç­–ç•¥

    é€‰æ‹©åœ¨ç‰¹å¾ç©ºé—´ä¸­å½¼æ­¤å·®å¼‚è¾ƒå¤§çš„æ ·æœ¬ï¼Œé¿å…é€‰æ‹©è¿‡äºç›¸ä¼¼çš„æ ·æœ¬ã€‚
    è¿™æœ‰åŠ©äºè·å¾—æ›´å…·ä»£è¡¨æ€§çš„è®­ç»ƒæ•°æ®ã€‚
    """

    def __init__(self, distance_metric: str = "cosine", n_clusters: int = None):
        super().__init__("diversity")
        self.distance_metric = distance_metric
        self.n_clusters = n_clusters

    def score_samples(
        self, uncertainty_results: UncertaintyResults, feature_embeddings: Optional[np.ndarray] = None, **kwargs
    ) -> np.ndarray:
        """åŸºäºå¤šæ ·æ€§ä¸ºæ ·æœ¬è¯„åˆ†"""

        if feature_embeddings is None:
            logger.warning("No feature embeddings provided for diversity strategy")
            return np.ones(len(uncertainty_results.sample_ids))

        # è®¡ç®—æ ·æœ¬é—´çš„è·ç¦»çŸ©é˜µ
        if self.distance_metric == "cosine":
            similarity_matrix = cosine_similarity(feature_embeddings)
            distance_matrix = 1.0 - similarity_matrix
        elif self.distance_metric == "euclidean":
            distance_matrix = euclidean_distances(feature_embeddings)
        else:
            raise ValueError(f"Unknown distance metric: {self.distance_metric}")

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¤šæ ·æ€§åˆ†æ•°ï¼ˆåˆ°å…¶ä»–æ ·æœ¬çš„å¹³å‡è·ç¦»ï¼‰
        diversity_scores = np.mean(distance_matrix, axis=1)

        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        diversity_scores = (diversity_scores - diversity_scores.min()) / (
            diversity_scores.max() - diversity_scores.min() + 1e-8
        )

        return diversity_scores


class ClusterBasedStrategy(BaseActiveLearningStrategy):
    """
    åŸºäºèšç±»çš„é‡‡æ ·ç­–ç•¥

    å…ˆå°†æ ·æœ¬èšç±»ï¼Œç„¶åä»æ¯ä¸ªèšç±»ä¸­é€‰æ‹©æœ€ä¸ç¡®å®šçš„æ ·æœ¬ã€‚
    è¿™ç¡®ä¿äº†é€‰æ‹©çš„æ ·æœ¬æ—¢æœ‰ä»£è¡¨æ€§åˆæœ‰æŒ‘æˆ˜æ€§ã€‚
    """

    def __init__(self, n_clusters: int = 10, cluster_method: str = "kmeans"):
        super().__init__("cluster_based")
        self.n_clusters = n_clusters
        self.cluster_method = cluster_method

    def score_samples(
        self, uncertainty_results: UncertaintyResults, feature_embeddings: Optional[np.ndarray] = None, **kwargs
    ) -> np.ndarray:
        """åŸºäºèšç±»çš„é‡‡æ ·è¯„åˆ†"""

        if feature_embeddings is None:
            logger.warning("No feature embeddings provided for cluster-based strategy")
            return uncertainty_results.uncertainty_scores

        # èšç±»
        if self.cluster_method == "kmeans":
            clusterer = KMeans(n_clusters=self.n_clusters, random_state=42)
            cluster_labels = clusterer.fit_predict(feature_embeddings)
        else:
            raise ValueError(f"Unknown cluster method: {self.cluster_method}")

        # ä¸ºæ¯ä¸ªèšç±»å†…çš„æ ·æœ¬è®¡ç®—ç›¸å¯¹åˆ†æ•°
        cluster_scores = np.zeros(len(uncertainty_results.sample_ids))
        uncertainty_scores = uncertainty_results.uncertainty_scores

        for cluster_id in range(self.n_clusters):
            cluster_mask = cluster_labels == cluster_id
            if np.sum(cluster_mask) == 0:
                continue

            cluster_uncertainties = uncertainty_scores[cluster_mask]
            # åœ¨èšç±»å†…æŒ‰ä¸ç¡®å®šæ€§æ’åº
            cluster_ranks = np.argsort(cluster_uncertainties)[::-1]  # é™åº

            # åˆ†é…åˆ†æ•°ï¼šèšç±»å†…æ’åé å‰çš„æ ·æœ¬å¾—åˆ†æ›´é«˜
            for i, sample_idx in enumerate(np.where(cluster_mask)[0]):
                rank_in_cluster = np.where(cluster_ranks == i)[0][0]
                cluster_scores[sample_idx] = 1.0 - (rank_in_cluster / len(cluster_ranks))

        return cluster_scores


class QueryByCommitteeStrategy(BaseActiveLearningStrategy):
    """
    Query-by-Committeeç­–ç•¥

    åˆ©ç”¨å¤šä¸ªæ¨¡å‹ä¹‹é—´çš„é¢„æµ‹å·®å¼‚æ¥é€‰æ‹©æ ·æœ¬ã€‚å½“å¤šä¸ªæ¨¡å‹å¯¹æŸä¸ªæ ·æœ¬
    çš„é¢„æµ‹åˆ†æ­§å¾ˆå¤§æ—¶ï¼Œè¯´æ˜è¿™ä¸ªæ ·æœ¬å¾ˆæœ‰å­¦ä¹ ä»·å€¼ã€‚
    """

    def __init__(self, committee_predictions: List[np.ndarray] = None):
        super().__init__("query_by_committee")
        self.committee_predictions = committee_predictions or []

    def set_committee_predictions(self, predictions: List[np.ndarray]):
        """è®¾ç½®å§”å‘˜ä¼šæ¨¡å‹çš„é¢„æµ‹ç»“æœ"""
        self.committee_predictions = predictions

    def score_samples(
        self, uncertainty_results: UncertaintyResults, feature_embeddings: Optional[np.ndarray] = None, **kwargs
    ) -> np.ndarray:
        """åŸºäºæ¨¡å‹é—´åˆ†æ­§ä¸ºæ ·æœ¬è¯„åˆ†"""

        if not self.committee_predictions:
            logger.warning("No committee predictions available, falling back to uncertainty")
            return uncertainty_results.uncertainty_scores

        n_samples = len(uncertainty_results.sample_ids)
        disagreement_scores = np.zeros(n_samples)

        # è®¡ç®—æ¨¡å‹é—´çš„å¹³å‡åˆ†æ­§
        for i in range(n_samples):
            sample_predictions = [pred[i] for pred in self.committee_predictions]

            # è®¡ç®—æ¨¡å‹é—´çš„KLæ•£åº¦å¹³å‡å€¼
            kl_divergences = []
            for j in range(len(sample_predictions)):
                for k in range(j + 1, len(sample_predictions)):
                    p = sample_predictions[j] + 1e-8  # é¿å…log(0)
                    q = sample_predictions[k] + 1e-8
                    kl_div = np.sum(p * np.log(p / q))
                    kl_divergences.append(kl_div)

            disagreement_scores[i] = np.mean(kl_divergences) if kl_divergences else 0.0

        # å½’ä¸€åŒ–
        if disagreement_scores.max() > disagreement_scores.min():
            disagreement_scores = (disagreement_scores - disagreement_scores.min()) / (
                disagreement_scores.max() - disagreement_scores.min()
            )

        return disagreement_scores


class HybridActiveLearningSelector:
    """
    æ··åˆä¸»åŠ¨å­¦ä¹ é€‰æ‹©å™¨

    ç»“åˆå¤šç§ä¸»åŠ¨å­¦ä¹ ç­–ç•¥çš„ä¼˜åŠ¿ï¼Œé€šè¿‡åŠ æƒç»„åˆä¸åŒç­–ç•¥çš„åˆ†æ•°
    æ¥é€‰æ‹©æœ€æœ‰ä»·å€¼çš„æ ·æœ¬ã€‚
    """

    def __init__(
        self, strategies: Dict[str, float] = None, feature_extractor: Callable = None, budget_per_iteration: int = 50
    ):
        """
        åˆå§‹åŒ–æ··åˆé€‰æ‹©å™¨

        Args:
            strategies: ç­–ç•¥æƒé‡å­—å…¸ï¼Œä¾‹å¦‚ {"uncertainty": 0.6, "diversity": 0.4}
            feature_extractor: ç‰¹å¾æå–å‡½æ•°
            budget_per_iteration: æ¯è½®è¿­ä»£çš„æ ‡æ³¨é¢„ç®—
        """
        # é»˜è®¤ç­–ç•¥æƒé‡
        default_strategies = {"uncertainty": 0.5, "diversity": 0.3, "cluster_based": 0.2}
        self.strategy_weights = strategies or default_strategies
        self.feature_extractor = feature_extractor
        self.budget_per_iteration = budget_per_iteration

        # åˆå§‹åŒ–ç­–ç•¥å®ä¾‹
        self.strategies = {
            "uncertainty": UncertaintyStrategy(),
            "diversity": DiversityStrategy(),
            "cluster_based": ClusterBasedStrategy(),
            "query_by_committee": QueryByCommitteeStrategy(),
        }

        logger.info(f"ğŸ¯ HybridActiveLearningSelector initialized with strategies: {self.strategy_weights}")

    def select_samples(
        self,
        uncertainty_results: UncertaintyResults,
        candidate_samples: List[PseudoLabelSample],
        feature_embeddings: Optional[np.ndarray] = None,
        committee_predictions: Optional[List[np.ndarray]] = None,
        budget: Optional[int] = None,
    ) -> ActiveLearningResults:
        """
        é€‰æ‹©æœ€æœ‰ä»·å€¼çš„æ ·æœ¬è¿›è¡Œä¸»åŠ¨å­¦ä¹ 

        Args:
            uncertainty_results: ä¸ç¡®å®šæ€§ä¼°è®¡ç»“æœ
            candidate_samples: å€™é€‰æ ·æœ¬åˆ—è¡¨ï¼ˆä½ç½®ä¿¡åº¦æ ·æœ¬ï¼‰
            feature_embeddings: ç‰¹å¾åµŒå…¥
            committee_predictions: å§”å‘˜ä¼šæ¨¡å‹é¢„æµ‹
            budget: æœ¬è½®æ ‡æ³¨é¢„ç®—

        Returns:
            ä¸»åŠ¨å­¦ä¹ é€‰æ‹©ç»“æœ
        """
        budget = budget or self.budget_per_iteration
        logger.info(f"ğŸ” Selecting {budget} samples from {len(candidate_samples)} candidates...")

        # æ„å»ºå€™é€‰æ ·æœ¬çš„æ˜ å°„
        candidate_indices = []
        candidate_id_to_idx = {}

        for i, sample in enumerate(candidate_samples):
            if sample.sample_id in uncertainty_results.sample_ids:
                idx = uncertainty_results.sample_ids.index(sample.sample_id)
                candidate_indices.append(idx)
                candidate_id_to_idx[sample.sample_id] = idx

        if not candidate_indices:
            logger.warning("No matching candidates found in uncertainty results")
            return ActiveLearningResults([], [], {}, {})

        # è¿‡æ»¤ç›¸å…³çš„ä¸ç¡®å®šæ€§ç»“æœå’Œç‰¹å¾
        filtered_uncertainty = self._filter_uncertainty_results(uncertainty_results, candidate_indices)
        filtered_features = feature_embeddings[candidate_indices] if feature_embeddings is not None else None

        # è®¡ç®—å„ç­–ç•¥çš„åˆ†æ•°
        strategy_scores = {}
        for strategy_name, weight in self.strategy_weights.items():
            if weight <= 0:
                continue

            strategy = self.strategies[strategy_name]

            # ç‰¹æ®Šå¤„ç†query_by_committee
            if strategy_name == "query_by_committee" and committee_predictions:
                filtered_committee_preds = [pred[candidate_indices] for pred in committee_predictions]
                strategy.set_committee_predictions(filtered_committee_preds)

            scores = strategy.score_samples(filtered_uncertainty, filtered_features)
            strategy_scores[strategy_name] = scores

            logger.debug(f"ğŸ“Š {strategy_name} scores: mean={np.mean(scores):.3f}, std={np.std(scores):.3f}")

        # åŠ æƒç»„åˆç­–ç•¥åˆ†æ•°
        combined_scores = self._combine_strategy_scores(strategy_scores, self.strategy_weights)

        # åˆ›å»ºæŸ¥è¯¢å¯¹è±¡
        queries = []
        for i, sample in enumerate(candidate_samples):
            if sample.sample_id not in candidate_id_to_idx:
                continue

            uncertainty_idx = candidate_id_to_idx[sample.sample_id]
            local_idx = candidate_indices.index(uncertainty_idx)

            query = ActiveLearningQuery(
                sample_id=sample.sample_id,
                uncertainty_score=float(filtered_uncertainty.uncertainty_scores[local_idx]),
                diversity_score=(
                    float(strategy_scores.get("diversity", [0])[local_idx]) if "diversity" in strategy_scores else 0.0
                ),
                expected_impact=float(combined_scores[local_idx]),
                combined_score=float(combined_scores[local_idx]),
                selection_reason=self._get_selection_reason(strategy_scores, local_idx),
            )
            queries.append(query)

        # æŒ‰ç»„åˆåˆ†æ•°æ’åºå¹¶é€‰æ‹©top-k
        queries.sort(key=lambda x: x.combined_score, reverse=True)
        selected_samples = queries[:budget]

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        selection_stats = self._compute_selection_statistics(queries, selected_samples)
        diversity_analysis = (
            self._analyze_diversity(selected_samples, filtered_features) if filtered_features is not None else {}
        )

        results = ActiveLearningResults(
            selected_samples=selected_samples,
            candidate_pool=queries,
            selection_statistics=selection_stats,
            diversity_analysis=diversity_analysis,
        )

        logger.info(f"âœ… Selected {len(selected_samples)} samples for annotation")
        logger.info(f"ğŸ“ˆ Average combined score: {np.mean([s.combined_score for s in selected_samples]):.3f}")

        return results

    def _filter_uncertainty_results(
        self, uncertainty_results: UncertaintyResults, indices: List[int]
    ) -> UncertaintyResults:
        """è¿‡æ»¤ä¸ç¡®å®šæ€§ç»“æœä»¥åŒ¹é…å€™é€‰æ ·æœ¬"""
        return UncertaintyResults(
            sample_ids=[uncertainty_results.sample_ids[i] for i in indices],
            predictions=uncertainty_results.predictions[indices],
            uncertainty_scores=uncertainty_results.uncertainty_scores[indices],
            epistemic_uncertainty=uncertainty_results.epistemic_uncertainty[indices],
            aleatoric_uncertainty=uncertainty_results.aleatoric_uncertainty[indices],
            prediction_entropy=uncertainty_results.prediction_entropy[indices],
            confidence_scores=uncertainty_results.confidence_scores[indices],
            calibrated_confidence=uncertainty_results.calibrated_confidence[indices],
        )

    def _combine_strategy_scores(self, strategy_scores: Dict[str, np.ndarray], weights: Dict[str, float]) -> np.ndarray:
        """åŠ æƒç»„åˆå¤šä¸ªç­–ç•¥çš„åˆ†æ•°"""
        if not strategy_scores:
            return np.array([])

        # å½’ä¸€åŒ–æ¯ä¸ªç­–ç•¥çš„åˆ†æ•°åˆ°0-1èŒƒå›´
        normalized_scores = {}
        for name, scores in strategy_scores.items():
            if len(scores) == 0:
                continue
            score_min, score_max = scores.min(), scores.max()
            if score_max > score_min:
                normalized_scores[name] = (scores - score_min) / (score_max - score_min)
            else:
                normalized_scores[name] = np.ones_like(scores)

        # åŠ æƒç»„åˆ
        n_samples = len(list(normalized_scores.values())[0])
        combined = np.zeros(n_samples)
        total_weight = 0

        for name, scores in normalized_scores.items():
            weight = weights.get(name, 0)
            combined += weight * scores
            total_weight += weight

        # å½’ä¸€åŒ–æƒé‡
        if total_weight > 0:
            combined /= total_weight

        return combined

    def _get_selection_reason(self, strategy_scores: Dict[str, np.ndarray], idx: int) -> str:
        """ç”Ÿæˆæ ·æœ¬é€‰æ‹©çš„åŸå› æè¿°"""
        reasons = []

        for strategy_name, scores in strategy_scores.items():
            if len(scores) <= idx:
                continue
            score = scores[idx]
            percentile = (scores <= score).mean() * 100

            if percentile >= 90:
                reasons.append(f"high_{strategy_name}")
            elif percentile >= 70:
                reasons.append(f"medium_{strategy_name}")

        return ", ".join(reasons) if reasons else "low_priority"

    def _compute_selection_statistics(
        self, all_queries: List[ActiveLearningQuery], selected: List[ActiveLearningQuery]
    ) -> Dict:
        """è®¡ç®—é€‰æ‹©ç»Ÿè®¡ä¿¡æ¯"""
        if not all_queries:
            return {}

        all_scores = [q.combined_score for q in all_queries]
        selected_scores = [q.combined_score for q in selected]

        stats = {
            "total_candidates": len(all_queries),
            "selected_count": len(selected),
            "selection_ratio": len(selected) / len(all_queries),
            "score_statistics": {
                "all_mean": np.mean(all_scores),
                "all_std": np.std(all_scores),
                "selected_mean": np.mean(selected_scores) if selected_scores else 0,
                "selected_min": np.min(selected_scores) if selected_scores else 0,
                "selected_max": np.max(selected_scores) if selected_scores else 0,
            },
            "strategy_contributions": self.strategy_weights.copy(),
        }

        return stats

    def _analyze_diversity(self, selected_samples: List[ActiveLearningQuery], feature_embeddings: np.ndarray) -> Dict:
        """åˆ†æé€‰æ‹©æ ·æœ¬çš„å¤šæ ·æ€§"""
        if len(selected_samples) < 2 or feature_embeddings is None:
            return {}

        # æå–é€‰æ‹©æ ·æœ¬çš„ç‰¹å¾
        selected_indices = list(range(len(selected_samples)))  # è¿™é‡Œéœ€è¦å®é™…çš„ç´¢å¼•æ˜ å°„

        # è®¡ç®—æ ·æœ¬é—´è·ç¦»
        if len(selected_indices) >= 2:
            selected_features = feature_embeddings[: len(selected_samples)]  # ç®€åŒ–å¤„ç†
            distances = euclidean_distances(selected_features)

            # å»é™¤å¯¹è§’çº¿ï¼ˆè‡ªèº«è·ç¦»ï¼‰
            mask = np.ones(distances.shape, dtype=bool)
            np.fill_diagonal(mask, False)
            inter_distances = distances[mask]

            diversity_analysis = {
                "mean_distance": float(np.mean(inter_distances)),
                "min_distance": float(np.min(inter_distances)),
                "max_distance": float(np.max(inter_distances)),
                "std_distance": float(np.std(inter_distances)),
                "diversity_score": float(np.mean(inter_distances)),  # å¹³å‡è·ç¦»ä½œä¸ºå¤šæ ·æ€§æŒ‡æ ‡
            }

            return diversity_analysis

        return {}

    def save_results(self, results: ActiveLearningResults, output_path: Path):
        """ä¿å­˜ä¸»åŠ¨å­¦ä¹ ç»“æœ"""
        output_path.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜å®Œæ•´ç»“æœ
        with open(output_path / "active_learning_results.json", "w") as f:
            json.dump(results.to_dict(), f, indent=2)

        # ä¿å­˜é€‰æ‹©çš„æ ·æœ¬IDåˆ—è¡¨ï¼ˆç”¨äºåç»­æ ‡æ³¨ï¼‰
        selected_ids = [s.sample_id for s in results.selected_samples]
        with open(output_path / "samples_for_annotation.txt", "w") as f:
            f.write("\n".join(selected_ids))

        # ä¿å­˜è¯¦ç»†çš„é€‰æ‹©ç»“æœCSV
        if results.selected_samples:
            df = pd.DataFrame([asdict(s) for s in results.selected_samples])
            df.to_csv(output_path / "selected_samples_detailed.csv", index=False)

        logger.info(f"ğŸ“ Active learning results saved to {output_path}")

    def create_visualization(self, results: ActiveLearningResults, output_path: Path):
        """åˆ›å»ºä¸»åŠ¨å­¦ä¹ é€‰æ‹©çš„å¯è§†åŒ–"""
        if not results.selected_samples:
            return

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. åˆ†æ•°åˆ†å¸ƒå¯¹æ¯”
        all_scores = [q.combined_score for q in results.candidate_pool]
        selected_scores = [q.combined_score for q in results.selected_samples]

        axes[0, 0].hist(all_scores, bins=30, alpha=0.5, label="All Candidates", color="lightblue")
        axes[0, 0].hist(selected_scores, bins=20, alpha=0.7, label="Selected", color="red")
        axes[0, 0].set_title("Score Distribution")
        axes[0, 0].set_xlabel("Combined Score")
        axes[0, 0].set_ylabel("Count")
        axes[0, 0].legend()

        # 2. ä¸ç¡®å®šæ€§vså¤šæ ·æ€§æ•£ç‚¹å›¾
        uncertainty_scores = [q.uncertainty_score for q in results.candidate_pool]
        diversity_scores = [q.diversity_score for q in results.candidate_pool]
        selected_uncertainty = [q.uncertainty_score for q in results.selected_samples]
        selected_diversity = [q.diversity_score for q in results.selected_samples]

        axes[0, 1].scatter(uncertainty_scores, diversity_scores, alpha=0.5, color="lightgray", label="Candidates")
        axes[0, 1].scatter(selected_uncertainty, selected_diversity, alpha=0.8, color="red", s=50, label="Selected")
        axes[0, 1].set_title("Uncertainty vs Diversity")
        axes[0, 1].set_xlabel("Uncertainty Score")
        axes[0, 1].set_ylabel("Diversity Score")
        axes[0, 1].legend()

        # 3. é€‰æ‹©åŸå› é¥¼å›¾
        reasons = [q.selection_reason for q in results.selected_samples]
        reason_counts = pd.Series(reasons).value_counts()

        if len(reason_counts) > 0:
            axes[1, 0].pie(reason_counts.values, labels=reason_counts.index, autopct="%1.1f%%")
            axes[1, 0].set_title("Selection Reasons")

        # 4. ç­–ç•¥è´¡çŒ®æƒé‡
        if results.selection_statistics.get("strategy_contributions"):
            strategies = list(results.selection_statistics["strategy_contributions"].keys())
            weights = list(results.selection_statistics["strategy_contributions"].values())

            axes[1, 1].bar(strategies, weights, color="skyblue")
            axes[1, 1].set_title("Strategy Weights")
            axes[1, 1].set_xlabel("Strategy")
            axes[1, 1].set_ylabel("Weight")
            axes[1, 1].tick_params(axis="x", rotation=45)

        plt.tight_layout()
        plt.savefig(output_path / "active_learning_visualization.png", dpi=300, bbox_inches="tight")
        plt.close()

        logger.info(f"ğŸ“Š Visualization saved to {output_path / 'active_learning_visualization.png'}")


def create_active_learning_selector(config: Dict) -> HybridActiveLearningSelector:
    """å·¥å‚å‡½æ•°ï¼šä»é…ç½®åˆ›å»ºä¸»åŠ¨å­¦ä¹ é€‰æ‹©å™¨"""
    return HybridActiveLearningSelector(
        strategies=config.get("strategies", None), budget_per_iteration=config.get("budget_per_iteration", 50)
    )
