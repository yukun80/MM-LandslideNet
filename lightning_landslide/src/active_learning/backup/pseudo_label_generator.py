# =============================================================================
# lightning_landslide/src/active_learning/pseudo_label_generator.py
# =============================================================================

"""
ä¼ªæ ‡ç­¾ç”Ÿæˆå™¨ - åŸºäºç½®ä¿¡åº¦å’Œä¸ç¡®å®šæ€§ç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾

è¿™ä¸ªæ¨¡å—å®ç°äº†æ™ºèƒ½çš„ä¼ªæ ‡ç­¾ç”Ÿæˆç­–ç•¥ï¼Œé€šè¿‡å¤šé‡è´¨é‡æ§åˆ¶æœºåˆ¶
ç¡®ä¿ç”Ÿæˆçš„ä¼ªæ ‡ç­¾ä¸ä¼šæ±¡æŸ“è®­ç»ƒæ•°æ®ã€‚

æ ¸å¿ƒç­–ç•¥ï¼š
1. ç½®ä¿¡åº¦ç­›é€‰ï¼šåªé€‰æ‹©é«˜ç½®ä¿¡åº¦çš„é¢„æµ‹
2. ä¸ç¡®å®šæ€§è¿‡æ»¤ï¼šæ’é™¤é«˜ä¸ç¡®å®šæ€§çš„æ ·æœ¬
3. ç±»åˆ«å¹³è¡¡ï¼šç¡®ä¿ä¼ªæ ‡ç­¾çš„ç±»åˆ«åˆ†å¸ƒåˆç†
4. ä¸€è‡´æ€§æ£€æŸ¥ï¼šå¤šæ¨¡å‹é¢„æµ‹ä¸€è‡´æ€§éªŒè¯
5. è´¨é‡è¯„ä¼°ï¼šåŠ¨æ€è°ƒæ•´ä¼ªæ ‡ç­¾è´¨é‡è¦æ±‚
"""

import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Optional, Union
from pathlib import Path
import json
from dataclasses import dataclass, asdict
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

from .uncertainty_estimator import UncertaintyResults

logger = logging.getLogger(__name__)


@dataclass
class PseudoLabelSample:
    """å•ä¸ªä¼ªæ ‡ç­¾æ ·æœ¬çš„ä¿¡æ¯"""

    sample_id: str
    predicted_label: int
    confidence: float
    uncertainty: float
    quality_score: float
    source_model: str = "single"


@dataclass
class PseudoLabelResults:
    """ä¼ªæ ‡ç­¾ç”Ÿæˆç»“æœ"""

    high_confidence_samples: List[PseudoLabelSample]
    low_confidence_samples: List[PseudoLabelSample]
    excluded_samples: List[PseudoLabelSample]
    statistics: Dict
    quality_distribution: Dict

    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä»¥ä¾¿ä¿å­˜"""
        return {
            "high_confidence_samples": [asdict(s) for s in self.high_confidence_samples],
            "low_confidence_samples": [asdict(s) for s in self.low_confidence_samples],
            "excluded_samples": [asdict(s) for s in self.excluded_samples],
            "statistics": self.statistics,
            "quality_distribution": self.quality_distribution,
        }


class AdaptiveThresholdScheduler:
    """
    è‡ªé€‚åº”é˜ˆå€¼è°ƒåº¦å™¨

    æ ¹æ®è®­ç»ƒè¿›å±•å’Œæ¨¡å‹æ€§èƒ½åŠ¨æ€è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œ
    åœ¨è®­ç»ƒæ—©æœŸä½¿ç”¨è¾ƒä½é˜ˆå€¼ï¼Œéšç€æ¨¡å‹æ”¹è¿›é€æ­¥æé«˜è¦æ±‚ã€‚
    """

    def __init__(self, initial_threshold: float = 0.8, final_threshold: float = 0.95, max_iterations: int = 5):
        self.initial_threshold = initial_threshold
        self.final_threshold = final_threshold
        self.max_iterations = max_iterations

    def get_threshold(
        self, current_iteration: int, validation_performance: float = None, pseudo_label_quality: float = None
    ) -> float:
        """è®¡ç®—å½“å‰è¿­ä»£çš„ç½®ä¿¡åº¦é˜ˆå€¼"""

        # åŸºç¡€çº¿æ€§è°ƒåº¦
        progress = min(current_iteration / self.max_iterations, 1.0)
        base_threshold = self.initial_threshold + (self.final_threshold - self.initial_threshold) * progress

        # æ ¹æ®éªŒè¯æ€§èƒ½è°ƒæ•´
        if validation_performance is not None:
            if validation_performance > 0.85:  # æ¨¡å‹è¡¨ç°å¥½æ—¶æé«˜è¦æ±‚
                base_threshold += 0.02
            elif validation_performance < 0.75:  # æ¨¡å‹è¡¨ç°å·®æ—¶é™ä½è¦æ±‚
                base_threshold -= 0.02

        # æ ¹æ®ä¼ªæ ‡ç­¾è´¨é‡è°ƒæ•´
        if pseudo_label_quality is not None:
            if pseudo_label_quality > 0.9:  # ä¼ªæ ‡ç­¾è´¨é‡é«˜æ—¶å¯ä»¥é€‚å½“é™ä½é˜ˆå€¼
                base_threshold -= 0.01
            elif pseudo_label_quality < 0.7:  # ä¼ªæ ‡ç­¾è´¨é‡ä½æ—¶æé«˜é˜ˆå€¼
                base_threshold += 0.03

        # ç¡®ä¿é˜ˆå€¼åœ¨åˆç†èŒƒå›´å†…
        return np.clip(base_threshold, 0.6, 0.98)


class ClassBalanceController:
    """
    ç±»åˆ«å¹³è¡¡æ§åˆ¶å™¨

    ç¡®ä¿ä¼ªæ ‡ç­¾çš„ç±»åˆ«åˆ†å¸ƒåˆç†ï¼Œé¿å…æŸä¸ªç±»åˆ«çš„ä¼ªæ ‡ç­¾è¿‡å¤š
    å¯¼è‡´æ¨¡å‹åå‘æŸä¸ªç±»åˆ«ã€‚
    """

    def __init__(self, original_class_distribution: Dict[int, int], balance_ratio: float = 2.0):
        """
        Args:
            original_class_distribution: åŸå§‹è®­ç»ƒé›†çš„ç±»åˆ«åˆ†å¸ƒ
            balance_ratio: å…è®¸çš„æœ€å¤§ç±»åˆ«ä¸å¹³è¡¡æ¯”ä¾‹
        """
        self.original_distribution = original_class_distribution
        self.balance_ratio = balance_ratio
        self.total_original = sum(original_class_distribution.values())

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç›®æ ‡æ¯”ä¾‹
        self.target_ratios = {cls: count / self.total_original for cls, count in original_class_distribution.items()}

        logger.info(f"ğŸ“Š Original class distribution: {original_class_distribution}")
        logger.info(f"ğŸ¯ Target ratios: {self.target_ratios}")

    def filter_balanced_samples(
        self, samples: List[PseudoLabelSample], max_samples_per_class: Optional[int] = None
    ) -> List[PseudoLabelSample]:
        """æ ¹æ®ç±»åˆ«å¹³è¡¡ç­–ç•¥ç­›é€‰æ ·æœ¬"""

        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡
        class_counts = Counter(s.predicted_label for s in samples)
        logger.info(f"ğŸ“ˆ Pseudo-label class distribution: {dict(class_counts)}")

        if max_samples_per_class is None:
            # æ ¹æ®åŸå§‹åˆ†å¸ƒè®¡ç®—æ¯ä¸ªç±»åˆ«çš„æœ€å¤§æ ·æœ¬æ•°
            max_samples_per_class = {
                cls: int(self.total_original * ratio * self.balance_ratio) for cls, ratio in self.target_ratios.items()
            }
        else:
            max_samples_per_class = {cls: max_samples_per_class for cls in self.target_ratios.keys()}

        # æŒ‰ç±»åˆ«åˆ†ç»„å¹¶æ’åº
        class_groups = {}
        for sample in samples:
            if sample.predicted_label not in class_groups:
                class_groups[sample.predicted_label] = []
            class_groups[sample.predicted_label].append(sample)

        # å¯¹æ¯ä¸ªç±»åˆ«æŒ‰è´¨é‡åˆ†æ•°æ’åºï¼Œé€‰æ‹©æœ€å¥½çš„æ ·æœ¬
        balanced_samples = []
        for cls, cls_samples in class_groups.items():
            # æŒ‰è´¨é‡åˆ†æ•°é™åºæ’åº
            cls_samples.sort(key=lambda x: x.quality_score, reverse=True)

            # é€‰æ‹©å‰Nä¸ªæœ€å¥½çš„æ ·æœ¬
            max_count = max_samples_per_class.get(cls, len(cls_samples))
            selected = cls_samples[:max_count]
            balanced_samples.extend(selected)

            logger.info(f"ğŸ·ï¸ Class {cls}: {len(cls_samples)} â†’ {len(selected)} samples selected")

        return balanced_samples


class PseudoLabelGenerator:
    """
    æ™ºèƒ½ä¼ªæ ‡ç­¾ç”Ÿæˆå™¨

    è¿™ä¸ªç±»æ•´åˆäº†å¤šç§è´¨é‡æ§åˆ¶ç­–ç•¥ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ä¼ªæ ‡ç­¾ã€‚
    è®¾è®¡ç†å¿µï¼šå®ç¼ºæ¯‹æ»¥ - åªé€‰æ‹©æœ€æœ‰æŠŠæ¡çš„æ ·æœ¬ä½œä¸ºä¼ªæ ‡ç­¾ã€‚
    """

    def __init__(
        self,
        confidence_threshold: float = 0.9,
        uncertainty_threshold: float = 0.1,
        use_adaptive_threshold: bool = True,
        use_class_balance: bool = True,
        quality_weight_config: Dict[str, float] = None,
    ):
        """
        åˆå§‹åŒ–ä¼ªæ ‡ç­¾ç”Ÿæˆå™¨

        Args:
            confidence_threshold: åŸºç¡€ç½®ä¿¡åº¦é˜ˆå€¼
            uncertainty_threshold: æœ€å¤§å…è®¸ä¸ç¡®å®šæ€§
            use_adaptive_threshold: æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼
            use_class_balance: æ˜¯å¦è¿›è¡Œç±»åˆ«å¹³è¡¡
            quality_weight_config: è´¨é‡åˆ†æ•°æƒé‡é…ç½®
        """
        self.base_confidence_threshold = confidence_threshold
        self.uncertainty_threshold = uncertainty_threshold
        self.use_adaptive_threshold = use_adaptive_threshold
        self.use_class_balance = use_class_balance

        # è´¨é‡åˆ†æ•°æƒé‡é…ç½®
        default_weights = {
            "confidence_weight": 0.4,
            "uncertainty_weight": 0.3,
            "consistency_weight": 0.2,
            "calibration_weight": 0.1,
        }
        self.quality_weights = quality_weight_config or default_weights

        # åˆå§‹åŒ–ç»„ä»¶
        if use_adaptive_threshold:
            self.threshold_scheduler = AdaptiveThresholdScheduler()

        self.class_balance_controller = None  # å°†åœ¨è®¾ç½®ç±»åˆ«åˆ†å¸ƒæ—¶åˆå§‹åŒ–

        logger.info("ğŸ·ï¸ PseudoLabelGenerator initialized")
        logger.info(f"ğŸ“ Base confidence threshold: {confidence_threshold}")
        logger.info(f"ğŸ”§ Quality weights: {self.quality_weights}")

    def set_class_distribution(self, class_distribution: Dict[int, int]):
        """è®¾ç½®åŸå§‹è®­ç»ƒé›†çš„ç±»åˆ«åˆ†å¸ƒ"""
        if self.use_class_balance:
            self.class_balance_controller = ClassBalanceController(class_distribution)

    def generate_pseudo_labels(
        self, uncertainty_results: UncertaintyResults, current_iteration: int = 0, validation_performance: float = None
    ) -> PseudoLabelResults:
        """
        ç”Ÿæˆä¼ªæ ‡ç­¾

        Args:
            uncertainty_results: ä¸ç¡®å®šæ€§ä¼°è®¡ç»“æœ
            current_iteration: å½“å‰è¿­ä»£è½®æ¬¡
            validation_performance: éªŒè¯é›†æ€§èƒ½

        Returns:
            ä¼ªæ ‡ç­¾ç”Ÿæˆç»“æœ
        """
        logger.info(f"ğŸ¯ Generating pseudo labels for iteration {current_iteration}...")

        # ç¡®å®šå½“å‰è¿­ä»£çš„ç½®ä¿¡åº¦é˜ˆå€¼
        if self.use_adaptive_threshold:
            current_threshold = self.threshold_scheduler.get_threshold(
                current_iteration=current_iteration, validation_performance=validation_performance
            )
        else:
            current_threshold = self.base_confidence_threshold

        logger.info(f"ğŸ“Š Using confidence threshold: {current_threshold:.3f}")

        # ç”Ÿæˆæ‰€æœ‰æ ·æœ¬çš„è´¨é‡åˆ†æ•°
        all_samples = self._evaluate_sample_quality(uncertainty_results)

        # æ ¹æ®é˜ˆå€¼åˆ†ç±»æ ·æœ¬
        high_confidence_samples = []
        low_confidence_samples = []
        excluded_samples = []

        for sample in all_samples:
            if sample.confidence >= current_threshold and sample.uncertainty <= self.uncertainty_threshold:
                high_confidence_samples.append(sample)
            elif sample.confidence >= 0.6:  # ä¸­ç­‰ç½®ä¿¡åº¦æ ·æœ¬ï¼Œå¯èƒ½ç”¨äºä¸»åŠ¨å­¦ä¹ 
                low_confidence_samples.append(sample)
            else:
                excluded_samples.append(sample)

        # ç±»åˆ«å¹³è¡¡å¤„ç†
        if self.use_class_balance and self.class_balance_controller:
            high_confidence_samples = self.class_balance_controller.filter_balanced_samples(high_confidence_samples)

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        statistics = self._compute_statistics(
            all_samples, high_confidence_samples, low_confidence_samples, excluded_samples
        )

        # è´¨é‡åˆ†å¸ƒåˆ†æ
        quality_distribution = self._analyze_quality_distribution(all_samples)

        results = PseudoLabelResults(
            high_confidence_samples=high_confidence_samples,
            low_confidence_samples=low_confidence_samples,
            excluded_samples=excluded_samples,
            statistics=statistics,
            quality_distribution=quality_distribution,
        )

        logger.info(f"âœ… Generated {len(high_confidence_samples)} high-quality pseudo labels")
        logger.info(f"ğŸ¤” Found {len(low_confidence_samples)} samples for potential active learning")
        logger.info(f"âŒ Excluded {len(excluded_samples)} low-quality samples")

        return results

    def _evaluate_sample_quality(self, uncertainty_results: UncertaintyResults) -> List[PseudoLabelSample]:
        """è¯„ä¼°æ¯ä¸ªæ ·æœ¬çš„è´¨é‡åˆ†æ•°"""
        samples = []

        for i, sample_id in enumerate(uncertainty_results.sample_ids):
            # è·å–é¢„æµ‹æ ‡ç­¾
            predicted_label = int(np.argmax(uncertainty_results.predictions[i]))

            # åŸºç¡€æŒ‡æ ‡
            confidence = uncertainty_results.confidence_scores[i]
            uncertainty = uncertainty_results.uncertainty_scores[i]
            calibrated_confidence = uncertainty_results.calibrated_confidence[i]

            # è®¡ç®—è´¨é‡åˆ†æ•°ï¼ˆå½’ä¸€åŒ–åˆ°0-1ï¼‰
            quality_score = self._compute_quality_score(
                confidence=confidence, uncertainty=uncertainty, calibrated_confidence=calibrated_confidence
            )

            sample = PseudoLabelSample(
                sample_id=sample_id,
                predicted_label=predicted_label,
                confidence=float(confidence),
                uncertainty=float(uncertainty),
                quality_score=float(quality_score),
            )

            samples.append(sample)

        return samples

    def _compute_quality_score(self, confidence: float, uncertainty: float, calibrated_confidence: float) -> float:
        """è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°"""

        # å½’ä¸€åŒ–æŒ‡æ ‡åˆ°0-1èŒƒå›´
        normalized_confidence = confidence  # å·²ç»æ˜¯0-1
        normalized_uncertainty = 1.0 - min(uncertainty, 1.0)  # è½¬æ¢ä¸ºç¡®å®šæ€§
        normalized_calibrated = calibrated_confidence  # å·²ç»æ˜¯0-1

        # åŠ æƒç»„åˆ
        quality_score = (
            self.quality_weights["confidence_weight"] * normalized_confidence
            + self.quality_weights["uncertainty_weight"] * normalized_uncertainty
            + self.quality_weights["calibration_weight"] * normalized_calibrated
        )

        # ä¸€è‡´æ€§åˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…å¯ä»¥ç”¨å¤šæ¨¡å‹ä¸€è‡´æ€§ï¼‰
        consistency_score = min(confidence, normalized_uncertainty)
        quality_score += self.quality_weights["consistency_weight"] * consistency_score

        return np.clip(quality_score, 0.0, 1.0)

    def _compute_statistics(self, all_samples, high_conf, low_conf, excluded) -> Dict:
        """è®¡ç®—è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        total = len(all_samples)

        stats = {
            "total_samples": total,
            "high_confidence_count": len(high_conf),
            "low_confidence_count": len(low_conf),
            "excluded_count": len(excluded),
            "high_confidence_ratio": len(high_conf) / total if total > 0 else 0,
            "low_confidence_ratio": len(low_conf) / total if total > 0 else 0,
            "excluded_ratio": len(excluded) / total if total > 0 else 0,
            "average_quality_score": np.mean([s.quality_score for s in all_samples]),
            "high_conf_avg_quality": np.mean([s.quality_score for s in high_conf]) if high_conf else 0,
            "class_distribution_high_conf": dict(Counter(s.predicted_label for s in high_conf)),
            "class_distribution_low_conf": dict(Counter(s.predicted_label for s in low_conf)),
        }

        return stats

    def _analyze_quality_distribution(self, samples: List[PseudoLabelSample]) -> Dict:
        """åˆ†æè´¨é‡åˆ†æ•°åˆ†å¸ƒ"""
        quality_scores = [s.quality_score for s in samples]

        if not quality_scores:
            return {}

        quality_stats = {
            "mean": np.mean(quality_scores),
            "std": np.std(quality_scores),
            "min": np.min(quality_scores),
            "max": np.max(quality_scores),
            "percentiles": {
                "25th": np.percentile(quality_scores, 25),
                "50th": np.percentile(quality_scores, 50),
                "75th": np.percentile(quality_scores, 75),
                "90th": np.percentile(quality_scores, 90),
                "95th": np.percentile(quality_scores, 95),
            },
        }

        return quality_stats

    def save_results(self, results: PseudoLabelResults, output_path: Path):
        """ä¿å­˜ä¼ªæ ‡ç­¾ç»“æœåˆ°æ–‡ä»¶"""
        output_path.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜JSONæ ¼å¼çš„ç»“æœ
        with open(output_path / "pseudo_label_results.json", "w") as f:
            json.dump(results.to_dict(), f, indent=2)

        # ä¿å­˜CSVæ ¼å¼çš„é«˜ç½®ä¿¡åº¦æ ·æœ¬
        if results.high_confidence_samples:
            df = pd.DataFrame([asdict(s) for s in results.high_confidence_samples])
            df.to_csv(output_path / "high_confidence_pseudo_labels.csv", index=False)

        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        with open(output_path / "pseudo_label_statistics.json", "w") as f:
            json.dump(results.statistics, f, indent=2)

        logger.info(f"ğŸ“ Pseudo label results saved to {output_path}")

    def create_visualization(self, results: PseudoLabelResults, output_path: Path):
        """åˆ›å»ºå¯è§†åŒ–æŠ¥å‘Š"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. æ ·æœ¬åˆ†å¸ƒé¥¼å›¾
        labels = ["High Confidence", "Low Confidence", "Excluded"]
        sizes = [
            len(results.high_confidence_samples),
            len(results.low_confidence_samples),
            len(results.excluded_samples),
        ]
        colors = ["#2ecc71", "#f39c12", "#e74c3c"]

        axes[0, 0].pie(sizes, labels=labels, colors=colors, autopct="%1.1f%%", startangle=90)
        axes[0, 0].set_title("Sample Distribution")

        # 2. è´¨é‡åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾
        all_quality_scores = (
            [s.quality_score for s in results.high_confidence_samples]
            + [s.quality_score for s in results.low_confidence_samples]
            + [s.quality_score for s in results.excluded_samples]
        )

        axes[0, 1].hist(all_quality_scores, bins=30, alpha=0.7, color="skyblue", edgecolor="black")
        axes[0, 1].set_title("Quality Score Distribution")
        axes[0, 1].set_xlabel("Quality Score")
        axes[0, 1].set_ylabel("Count")

        # 3. ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”
        high_conf_classes = [s.predicted_label for s in results.high_confidence_samples]
        class_counts = Counter(high_conf_classes)

        if class_counts:
            classes = list(class_counts.keys())
            counts = list(class_counts.values())
            axes[1, 0].bar(classes, counts, color="lightgreen", alpha=0.7)
            axes[1, 0].set_title("High Confidence Class Distribution")
            axes[1, 0].set_xlabel("Class")
            axes[1, 0].set_ylabel("Count")

        # 4. ç½®ä¿¡åº¦vsä¸ç¡®å®šæ€§æ•£ç‚¹å›¾
        confidences = [s.confidence for s in results.high_confidence_samples]
        uncertainties = [s.uncertainty for s in results.high_confidence_samples]

        if confidences and uncertainties:
            axes[1, 1].scatter(confidences, uncertainties, alpha=0.6, color="purple")
            axes[1, 1].set_title("Confidence vs Uncertainty")
            axes[1, 1].set_xlabel("Confidence")
            axes[1, 1].set_ylabel("Uncertainty")

        plt.tight_layout()
        plt.savefig(output_path / "pseudo_label_analysis.png", dpi=300, bbox_inches="tight")
        plt.close()

        logger.info(f"ğŸ“Š Visualization saved to {output_path / 'pseudo_label_analysis.png'}")


def create_pseudo_label_generator(config: Dict) -> PseudoLabelGenerator:
    """å·¥å‚å‡½æ•°ï¼šä»é…ç½®åˆ›å»ºä¼ªæ ‡ç­¾ç”Ÿæˆå™¨"""
    return PseudoLabelGenerator(
        confidence_threshold=config.get("confidence_threshold", 0.9),
        uncertainty_threshold=config.get("uncertainty_threshold", 0.1),
        use_adaptive_threshold=config.get("use_adaptive_threshold", True),
        use_class_balance=config.get("use_class_balance", True),
        quality_weight_config=config.get("quality_weights", None),
    )
