# =============================================================================
# lightning_landslide/src/active_learning/active_pseudo_trainer.py
# =============================================================================

"""
ä¸»åŠ¨å­¦ä¹ +ä¼ªæ ‡ç­¾èåˆè®­ç»ƒå™¨

è¿™æ˜¯æ•´ä¸ªä¸»åŠ¨å­¦ä¹ ç³»ç»Ÿçš„æ ¸å¿ƒåè°ƒå™¨ï¼Œå®ƒæ•´åˆäº†ï¼š
1. ä¸ç¡®å®šæ€§ä¼°è®¡
2. ä¼ªæ ‡ç­¾ç”Ÿæˆ
3. ä¸»åŠ¨å­¦ä¹ æ ·æœ¬é€‰æ‹©
4. è¿­ä»£è®­ç»ƒæµç¨‹
5. æ•°æ®ç®¡ç†

è®¾è®¡ç†å¿µï¼š"æ¸è¿›å¢å¼º" - é€šè¿‡å¤šè½®è¿­ä»£ï¼Œé€æ­¥æ”¹å–„æ¨¡å‹æ€§èƒ½ï¼Œ
æœ€å¤§åŒ–åˆ©ç”¨æ— æ ‡æ³¨æ•°æ®ï¼Œæœ€å°åŒ–äººå·¥æ ‡æ³¨æˆæœ¬ã€‚
"""

import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Optional, Union, Callable
from pathlib import Path
import json
import time
from datetime import datetime
import copy
from dataclasses import dataclass, asdict
import matplotlib.pyplot as plt
import seaborn as sns

import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger

from ..utils.instantiate import instantiate_from_config
from ..data.multimodal_datamodule import MultiModalDataModule
from ..data.multimodal_dataset import MultiModalDataset
from .uncertainty_estimator import create_uncertainty_estimator, UncertaintyResults
from .pseudo_label_generator import create_pseudo_label_generator, PseudoLabelResults
from .active_learning_selector import create_active_learning_selector, ActiveLearningResults

logger = logging.getLogger(__name__)


@dataclass
class IterationResults:
    """å•æ¬¡è¿­ä»£çš„ç»“æœ"""

    iteration: int
    model_performance: Dict[str, float]
    uncertainty_results: UncertaintyResults
    pseudo_label_results: PseudoLabelResults
    active_learning_results: ActiveLearningResults
    training_time: float
    total_training_samples: int
    pseudo_label_count: int
    new_annotations_count: int


@dataclass
class ActivePseudoTrainingResults:
    """å®Œæ•´çš„ä¸»åŠ¨+ä¼ªæ ‡ç­¾è®­ç»ƒç»“æœ"""

    iteration_results: List[IterationResults]
    final_model_path: str
    best_model_path: str
    performance_history: Dict[str, List[float]]
    data_usage_history: Dict[str, List[int]]
    total_training_time: float
    convergence_iteration: int

    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "iteration_results": [asdict(r) for r in self.iteration_results],
            "final_model_path": self.final_model_path,
            "best_model_path": self.best_model_path,
            "performance_history": self.performance_history,
            "data_usage_history": self.data_usage_history,
            "total_training_time": self.total_training_time,
            "convergence_iteration": self.convergence_iteration,
        }


class EnhancedDataManager:
    """
    å¢å¼ºçš„æ•°æ®ç®¡ç†å™¨

    ç®¡ç†ä¸»åŠ¨å­¦ä¹ è¿‡ç¨‹ä¸­çš„å¤æ‚æ•°æ®çŠ¶æ€ï¼š
    - åŸå§‹æ ‡æ³¨æ•°æ®
    - ä¼ªæ ‡ç­¾æ•°æ®
    - æ–°å¢æ ‡æ³¨æ•°æ®
    - æµ‹è¯•é›†æ•°æ®
    """

    def __init__(self, base_config: Dict, output_dir: Path):
        self.base_config = base_config
        self.output_dir = output_dir

        # æ•°æ®çŠ¶æ€ç®¡ç†
        self.original_train_data = None
        self.pseudo_labeled_data = []
        self.newly_annotated_data = []
        self.test_data = None

        # åˆ›å»ºæ•°æ®ç‰ˆæœ¬æ§åˆ¶ç›®å½•
        self.data_versions_dir = output_dir / "data_versions"
        self.data_versions_dir.mkdir(parents=True, exist_ok=True)

        logger.info("ğŸ“Š EnhancedDataManager initialized")

    def load_initial_data(self):
        """åŠ è½½åˆå§‹æ•°æ®"""
        logger.info("ğŸ“‚ Loading initial data...")

        # ä½¿ç”¨é…ç½®åˆ›å»ºåˆå§‹æ•°æ®æ¨¡å—
        self.base_datamodule = instantiate_from_config(self.base_config["data"])
        self.base_datamodule.setup("fit")

        # ä¿å­˜åŸå§‹æ•°æ®å¼•ç”¨
        self.original_train_data = self.base_datamodule.train_dataset

        # è·å–ç±»åˆ«åˆ†å¸ƒ
        self.original_class_distribution = self._get_class_distribution(self.original_train_data)

        logger.info(f"âœ… Loaded {len(self.original_train_data)} original training samples")
        logger.info(f"ğŸ“Š Class distribution: {self.original_class_distribution}")

    def create_enhanced_datamodule(self, iteration: int) -> MultiModalDataModule:
        """åˆ›å»ºåŒ…å«ä¼ªæ ‡ç­¾çš„å¢å¼ºæ•°æ®æ¨¡å—"""
        logger.info(f"ğŸ”§ Creating enhanced datamodule for iteration {iteration}...")

        # åˆå¹¶æ‰€æœ‰è®­ç»ƒæ•°æ®
        combined_dataset = self._combine_training_data()

        # åˆ›å»ºæ–°çš„æ•°æ®æ¨¡å—
        enhanced_config = copy.deepcopy(self.base_config["data"])
        enhanced_datamodule = instantiate_from_config(enhanced_config)

        # æ›¿æ¢è®­ç»ƒæ•°æ®é›†
        enhanced_datamodule.train_dataset = combined_dataset
        enhanced_datamodule.setup("fit")  # é‡æ–°è®¾ç½®éªŒè¯é›†åˆ†å‰²

        # ä¿å­˜æ•°æ®ç‰ˆæœ¬
        self._save_data_version(combined_dataset, iteration)

        logger.info(f"ğŸ“ˆ Enhanced dataset size: {len(combined_dataset)} samples")
        return enhanced_datamodule

    def add_pseudo_labels(self, pseudo_label_results: PseudoLabelResults, test_dataloader):
        """æ·»åŠ ä¼ªæ ‡ç­¾æ•°æ®"""
        logger.info(f"ğŸ·ï¸ Adding {len(pseudo_label_results.high_confidence_samples)} pseudo labels...")

        # åˆ›å»ºä¼ªæ ‡ç­¾æ•°æ®é›†
        pseudo_dataset = self._create_pseudo_dataset(pseudo_label_results, test_dataloader)
        self.pseudo_labeled_data.append(pseudo_dataset)

        logger.info(f"âœ… Added pseudo-labeled dataset with {len(pseudo_dataset)} samples")

    def add_new_annotations(self, annotated_samples: List[str], test_dataloader):
        """æ·»åŠ æ–°æ ‡æ³¨çš„æ•°æ®"""
        if not annotated_samples:
            return

        logger.info(f"ğŸ“ Adding {len(annotated_samples)} newly annotated samples...")

        # è¿™é‡Œå‡è®¾æœ‰ä¸€ä¸ªäººå·¥æ ‡æ³¨æ¥å£
        # åœ¨å®é™…å®ç°ä¸­ï¼Œå¯èƒ½éœ€è¦ä¸æ ‡æ³¨å·¥å…·é›†æˆ
        annotated_dataset = self._create_annotated_dataset(annotated_samples, test_dataloader)
        self.newly_annotated_data.append(annotated_dataset)

        logger.info(f"âœ… Added {len(annotated_dataset)} newly annotated samples")

    def get_data_statistics(self) -> Dict:
        """è·å–å½“å‰æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        original_count = len(self.original_train_data) if self.original_train_data else 0
        pseudo_count = sum(len(dataset) for dataset in self.pseudo_labeled_data)
        new_annotation_count = sum(len(dataset) for dataset in self.newly_annotated_data)

        return {
            "original_training_samples": original_count,
            "pseudo_labeled_samples": pseudo_count,
            "newly_annotated_samples": new_annotation_count,
            "total_training_samples": original_count + pseudo_count + new_annotation_count,
            "data_augmentation_ratio": (pseudo_count + new_annotation_count) / max(original_count, 1),
        }

    def _combine_training_data(self):
        """åˆå¹¶æ‰€æœ‰è®­ç»ƒæ•°æ®"""
        combined_data = []

        # æ·»åŠ åŸå§‹æ•°æ®
        if self.original_train_data:
            combined_data.extend(self._dataset_to_list(self.original_train_data))

        # æ·»åŠ ä¼ªæ ‡ç­¾æ•°æ®
        for pseudo_dataset in self.pseudo_labeled_data:
            combined_data.extend(self._dataset_to_list(pseudo_dataset))

        # æ·»åŠ æ–°æ ‡æ³¨æ•°æ®
        for new_dataset in self.newly_annotated_data:
            combined_data.extend(self._dataset_to_list(new_dataset))

        # åˆ›å»ºæ–°çš„æ•°æ®é›†
        return self._create_combined_dataset(combined_data)

    def _get_class_distribution(self, dataset) -> Dict[int, int]:
        """è·å–æ•°æ®é›†çš„ç±»åˆ«åˆ†å¸ƒ"""
        class_counts = {}
        for i in range(len(dataset)):
            _, label = dataset[i]
            label_int = int(label.item())
            class_counts[label_int] = class_counts.get(label_int, 0) + 1
        return class_counts

    def _create_pseudo_dataset(self, pseudo_results: PseudoLabelResults, test_dataloader):
        """ä»ä¼ªæ ‡ç­¾ç»“æœåˆ›å»ºæ•°æ®é›†"""
        # ç®€åŒ–å®ç° - å®é™…ä¸­éœ€è¦æ ¹æ®å…·ä½“çš„æ•°æ®é›†ç±»å‹æ¥å®ç°
        pseudo_samples = []

        # è¿™é‡Œéœ€è¦æ ¹æ®sample_idä»test_dataloaderä¸­æå–å¯¹åº”çš„æ•°æ®
        # å¹¶ç”¨ä¼ªæ ‡ç­¾æ›¿æ¢åŸå§‹æ ‡ç­¾
        # å…·ä½“å®ç°å–å†³äºæ•°æ®é›†çš„ç»“æ„

        return pseudo_samples  # è¿”å›ä¼ªæ ‡ç­¾æ•°æ®é›†

    def _dataset_to_list(self, dataset):
        """å°†æ•°æ®é›†è½¬æ¢ä¸ºåˆ—è¡¨"""
        return [dataset[i] for i in range(len(dataset))]

    def _create_combined_dataset(self, data_list):
        """ä»æ•°æ®åˆ—è¡¨åˆ›å»ºç»„åˆæ•°æ®é›†"""
        # ç®€åŒ–å®ç° - éœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†ç±»å‹å®ç°
        return data_list

    def _save_data_version(self, dataset, iteration: int):
        """ä¿å­˜æ•°æ®ç‰ˆæœ¬"""
        version_info = {
            "iteration": iteration,
            "timestamp": datetime.now().isoformat(),
            "dataset_size": len(dataset),
            "class_distribution": self._get_class_distribution(dataset),
        }

        with open(self.data_versions_dir / f"iteration_{iteration}_info.json", "w") as f:
            json.dump(version_info, f, indent=2)


class ActivePseudoTrainer:
    """
    ä¸»åŠ¨å­¦ä¹ +ä¼ªæ ‡ç­¾èåˆè®­ç»ƒå™¨

    è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒï¼Œåè°ƒæ‰€æœ‰ç»„ä»¶å®Œæˆè¿­ä»£è®­ç»ƒè¿‡ç¨‹ã€‚
    """

    def __init__(self, config: Dict, experiment_name: str = None, output_dir: str = None):
        """
        åˆå§‹åŒ–ä¸»åŠ¨ä¼ªæ ‡ç­¾è®­ç»ƒå™¨

        Args:
            config: å®Œæ•´é…ç½®ï¼ˆåŒ…å«model, data, trainer, active_pseudo_learningç­‰ï¼‰
            experiment_name: å®éªŒåç§°
            output_dir: è¾“å‡ºç›®å½•
        """
        self.config = config
        self.active_config = config.get("active_pseudo_learning", {})

        # å®éªŒé…ç½®
        self.experiment_name = experiment_name or config.get("experiment_name", f"active_pseudo_{int(time.time())}")
        self.output_dir = Path(output_dir) if output_dir else Path("outputs") / self.experiment_name
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # è®­ç»ƒé…ç½®
        self.max_iterations = self.active_config.get("max_iterations", 5)
        self.convergence_threshold = self.active_config.get("convergence_threshold", 0.01)
        self.min_improvement_iterations = self.active_config.get("min_improvement_iterations", 2)

        # åˆ›å»ºç»„ä»¶
        self._initialize_components()

        # æ•°æ®ç®¡ç†å™¨
        self.data_manager = EnhancedDataManager(config, self.output_dir)

        # ç»“æœè·Ÿè¸ª
        self.iteration_results = []
        self.performance_history = {"val_f1": [], "val_loss": [], "train_f1": []}
        self.data_usage_history = {"training_samples": [], "pseudo_labels": [], "new_annotations": []}

        logger.info(f"ğŸš€ ActivePseudoTrainer initialized: {self.experiment_name}")
        logger.info(f"ğŸ“ Output directory: {self.output_dir}")
        logger.info(f"ğŸ”„ Max iterations: {self.max_iterations}")

    def _initialize_components(self):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶"""
        # ä¸ç¡®å®šæ€§ä¼°è®¡å™¨
        uncertainty_config = self.active_config.get("uncertainty_estimation", {})
        self.uncertainty_estimator = create_uncertainty_estimator(
            method=uncertainty_config.get("method", "mc_dropout"), **uncertainty_config.get("params", {})
        )

        # ä¼ªæ ‡ç­¾ç”Ÿæˆå™¨
        pseudo_config = self.active_config.get("pseudo_labeling", {})
        self.pseudo_generator = create_pseudo_label_generator(pseudo_config)

        # ä¸»åŠ¨å­¦ä¹ é€‰æ‹©å™¨
        active_config = self.active_config.get("active_learning", {})
        self.active_selector = create_active_learning_selector(active_config)

        logger.info("ğŸ”§ All components initialized successfully")

    def run(self) -> ActivePseudoTrainingResults:
        """è¿è¡Œå®Œæ•´çš„ä¸»åŠ¨+ä¼ªæ ‡ç­¾å­¦ä¹ æµç¨‹"""
        logger.info("ğŸ¯ Starting Active + Pseudo Label Learning...")
        start_time = time.time()

        # åˆå§‹åŒ–æ•°æ®
        self.data_manager.load_initial_data()
        self.pseudo_generator.set_class_distribution(self.data_manager.original_class_distribution)

        # è®­ç»ƒåŸºçº¿æ¨¡å‹
        logger.info("ğŸ Training baseline model...")
        baseline_model = self._train_baseline_model()
        current_model = baseline_model
        best_performance = 0.0
        best_model_path = None
        no_improvement_count = 0

        # è¿­ä»£è®­ç»ƒå¾ªç¯
        for iteration in range(self.max_iterations):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ”„ ITERATION {iteration + 1}/{self.max_iterations}")
            logger.info(f"{'='*60}")

            iteration_start = time.time()

            # 1. åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œä¸ç¡®å®šæ€§ä¼°è®¡
            logger.info("ğŸ“Š Step 1: Uncertainty estimation on test set...")
            test_dataloader = self._create_test_dataloader()
            uncertainty_results = self.uncertainty_estimator.estimate_uncertainty(current_model, test_dataloader)

            # 2. ç”Ÿæˆä¼ªæ ‡ç­¾
            logger.info("ğŸ·ï¸ Step 2: Generating pseudo labels...")
            pseudo_results = self.pseudo_generator.generate_pseudo_labels(
                uncertainty_results=uncertainty_results,
                current_iteration=iteration,
                validation_performance=best_performance,
            )

            # 3. ä¸»åŠ¨å­¦ä¹ æ ·æœ¬é€‰æ‹©
            logger.info("ğŸ¯ Step 3: Active learning sample selection...")
            active_results = self.active_selector.select_samples(
                uncertainty_results=uncertainty_results,
                candidate_samples=pseudo_results.low_confidence_samples,
                budget=self.active_config.get("annotation_budget", 50),
            )

            # 4. æ•°æ®å¢å¼ºï¼ˆæ·»åŠ ä¼ªæ ‡ç­¾å’Œæ–°æ ‡æ³¨ï¼‰
            logger.info("ğŸ“ˆ Step 4: Data augmentation...")
            self.data_manager.add_pseudo_labels(pseudo_results, test_dataloader)

            # æ¨¡æ‹Ÿäººå·¥æ ‡æ³¨è¿‡ç¨‹ï¼ˆå®é™…ä¸­éœ€è¦äººå·¥ä»‹å…¥ï¼‰
            selected_ids = [s.sample_id for s in active_results.selected_samples]
            self.data_manager.add_new_annotations(selected_ids, test_dataloader)

            # 5. é‡æ–°è®­ç»ƒæ¨¡å‹
            logger.info("ğŸš€ Step 5: Retraining model with augmented data...")
            enhanced_datamodule = self.data_manager.create_enhanced_datamodule(iteration)
            new_model, performance = self._train_enhanced_model(enhanced_datamodule, iteration)

            # 6. è¯„ä¼°æ€§èƒ½æ”¹è¿›
            current_f1 = performance.get("val_f1", 0.0)
            improvement = current_f1 - best_performance

            logger.info(f"ğŸ“ˆ Performance: {current_f1:.4f} (improvement: {improvement:+.4f})")

            # æ›´æ–°æœ€ä½³æ¨¡å‹
            if current_f1 > best_performance:
                best_performance = current_f1
                best_model_path = self.output_dir / f"models/best_model_iter_{iteration}.ckpt"
                torch.save(new_model.state_dict(), best_model_path)
                no_improvement_count = 0
                logger.info(f"ğŸ† New best model saved: {best_model_path}")
            else:
                no_improvement_count += 1

            # è®°å½•è¿­ä»£ç»“æœ
            iteration_time = time.time() - iteration_start
            data_stats = self.data_manager.get_data_statistics()

            iter_result = IterationResults(
                iteration=iteration + 1,
                model_performance=performance,
                uncertainty_results=uncertainty_results,
                pseudo_label_results=pseudo_results,
                active_learning_results=active_results,
                training_time=iteration_time,
                total_training_samples=data_stats["total_training_samples"],
                pseudo_label_count=data_stats["pseudo_labeled_samples"],
                new_annotations_count=data_stats["newly_annotated_samples"],
            )

            self.iteration_results.append(iter_result)

            # æ›´æ–°å†å²è®°å½•
            self.performance_history["val_f1"].append(current_f1)
            self.performance_history["val_loss"].append(performance.get("val_loss", 0.0))
            self.performance_history["train_f1"].append(performance.get("train_f1", 0.0))

            self.data_usage_history["training_samples"].append(data_stats["total_training_samples"])
            self.data_usage_history["pseudo_labels"].append(data_stats["pseudo_labeled_samples"])
            self.data_usage_history["new_annotations"].append(data_stats["newly_annotated_samples"])

            # ä¿å­˜ä¸­é—´ç»“æœ
            self._save_iteration_results(iter_result, iteration)

            # æ”¶æ•›æ£€æŸ¥
            if improvement < self.convergence_threshold and no_improvement_count >= self.min_improvement_iterations:
                logger.info(f"ğŸ¯ Convergence reached after {iteration + 1} iterations")
                break

            current_model = new_model

        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time

        # åˆ›å»ºæœ€ç»ˆç»“æœ
        final_results = ActivePseudoTrainingResults(
            iteration_results=self.iteration_results,
            final_model_path=str(self.output_dir / f"models/final_model.ckpt"),
            best_model_path=str(best_model_path) if best_model_path else "",
            performance_history=self.performance_history,
            data_usage_history=self.data_usage_history,
            total_training_time=total_time,
            convergence_iteration=len(self.iteration_results),
        )

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        torch.save(current_model.state_dict(), final_results.final_model_path)

        # ä¿å­˜å®Œæ•´ç»“æœå’Œå¯è§†åŒ–
        self._save_final_results(final_results)
        self._create_training_visualization(final_results)

        logger.info(f"\nğŸ‰ Active + Pseudo Label Learning completed!")
        logger.info(f"â±ï¸ Total time: {total_time:.2f}s")
        logger.info(f"ğŸ† Best F1 score: {best_performance:.4f}")
        logger.info(f"ğŸ“ Results saved to: {self.output_dir}")

        return final_results

    def _train_baseline_model(self):
        """è®­ç»ƒåŸºçº¿æ¨¡å‹"""
        baseline_trainer = self._create_trainer("baseline")
        model = instantiate_from_config(self.config["model"])

        baseline_trainer.fit(model, self.data_manager.base_datamodule)

        # ä¿å­˜åŸºçº¿æ¨¡å‹
        baseline_path = self.output_dir / "models/baseline_model.ckpt"
        baseline_path.parent.mkdir(parents=True, exist_ok=True)
        baseline_trainer.save_checkpoint(str(baseline_path))

        return model

    def _train_enhanced_model(self, datamodule, iteration: int):
        """è®­ç»ƒå¢å¼ºæ•°æ®çš„æ¨¡å‹"""
        trainer = self._create_trainer(f"iteration_{iteration}")
        model = instantiate_from_config(self.config["model"])

        trainer.fit(model, datamodule)

        # è¯„ä¼°æ¨¡å‹æ€§èƒ½
        val_results = trainer.validate(model, datamodule, verbose=False)
        performance = val_results[0] if val_results else {}

        return model, performance

    def _create_trainer(self, name: str) -> pl.Trainer:
        """åˆ›å»ºLightningè®­ç»ƒå™¨"""
        trainer_config = self.config["trainer"]["params"].copy()

        # è®¾ç½®æ—¥å¿—å’Œå›è°ƒ
        logger_instance = TensorBoardLogger(save_dir=str(self.output_dir / "logs"), name=name, version="")

        callbacks = [
            ModelCheckpoint(
                dirpath=str(self.output_dir / "checkpoints" / name),
                filename="{epoch}-{val_f1:.4f}",
                monitor="val_f1",
                mode="max",
                save_top_k=1,
            ),
            EarlyStopping(monitor="val_f1", patience=10, mode="max"),
        ]

        trainer = pl.Trainer(**trainer_config)
        trainer.callbacks = callbacks
        trainer.logger = logger_instance

        return trainer

    def _create_test_dataloader(self):
        """åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨"""
        # ä½¿ç”¨åŸºç¡€æ•°æ®æ¨¡å—çš„æµ‹è¯•é›†
        self.data_manager.base_datamodule.setup("test")
        return self.data_manager.base_datamodule.test_dataloader()

    def _save_iteration_results(self, result: IterationResults, iteration: int):
        """ä¿å­˜å•æ¬¡è¿­ä»£ç»“æœ"""
        iter_dir = self.output_dir / f"iteration_{iteration}"
        iter_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜è¿­ä»£æ‘˜è¦
        with open(iter_dir / "iteration_summary.json", "w") as f:
            json.dump(asdict(result), f, indent=2, default=str)

        # ä¿å­˜å„ä¸ªç»„ä»¶çš„è¯¦ç»†ç»“æœ
        self.pseudo_generator.save_results(result.pseudo_label_results, iter_dir / "pseudo_labels")
        self.active_selector.save_results(result.active_learning_results, iter_dir / "active_learning")

        # åˆ›å»ºå¯è§†åŒ–
        self.pseudo_generator.create_visualization(result.pseudo_label_results, iter_dir / "pseudo_labels")
        self.active_selector.create_visualization(result.active_learning_results, iter_dir / "active_learning")

    def _save_final_results(self, results: ActivePseudoTrainingResults):
        """ä¿å­˜æœ€ç»ˆå®Œæ•´ç»“æœ"""
        with open(self.output_dir / "final_results.json", "w") as f:
            json.dump(results.to_dict(), f, indent=2, default=str)

        # ä¿å­˜æ€§èƒ½å†å²CSV
        history_df = pd.DataFrame(results.performance_history)
        history_df.to_csv(self.output_dir / "performance_history.csv", index=False)

        # ä¿å­˜æ•°æ®ä½¿ç”¨å†å²CSV
        data_usage_df = pd.DataFrame(results.data_usage_history)
        data_usage_df.to_csv(self.output_dir / "data_usage_history.csv", index=False)

    def _create_training_visualization(self, results: ActivePseudoTrainingResults):
        """åˆ›å»ºè®­ç»ƒè¿‡ç¨‹çš„å¯è§†åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        iterations = list(range(1, len(results.performance_history["val_f1"]) + 1))

        # 1. æ€§èƒ½æ”¹è¿›æ›²çº¿
        axes[0, 0].plot(iterations, results.performance_history["val_f1"], "b-o", label="Validation F1")
        axes[0, 0].plot(iterations, results.performance_history["train_f1"], "g-s", label="Training F1")
        axes[0, 0].set_title("Model Performance Over Iterations")
        axes[0, 0].set_xlabel("Iteration")
        axes[0, 0].set_ylabel("F1 Score")
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # 2. æ•°æ®å¢é•¿è¶‹åŠ¿
        axes[0, 1].plot(iterations, results.data_usage_history["training_samples"], "r-o", label="Total Training")
        axes[0, 1].plot(
            iterations, results.data_usage_history["pseudo_labels"], "orange", marker="s", label="Pseudo Labels"
        )
        axes[0, 1].plot(
            iterations, results.data_usage_history["new_annotations"], "purple", marker="^", label="New Annotations"
        )
        axes[0, 1].set_title("Data Usage Over Iterations")
        axes[0, 1].set_xlabel("Iteration")
        axes[0, 1].set_ylabel("Number of Samples")
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # 3. æŸå¤±æ›²çº¿
        axes[1, 0].plot(iterations, results.performance_history["val_loss"], "r-o", label="Validation Loss")
        axes[1, 0].set_title("Validation Loss Over Iterations")
        axes[1, 0].set_xlabel("Iteration")
        axes[1, 0].set_ylabel("Loss")
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

        # 4. æ•°æ®å¢å¼ºæ•ˆç‡
        original_samples = (
            results.data_usage_history["training_samples"][0] if results.data_usage_history["training_samples"] else 0
        )
        augmentation_ratios = [
            (total - original_samples) / original_samples * 100
            for total in results.data_usage_history["training_samples"]
        ]

        axes[1, 1].bar(iterations, augmentation_ratios, color="skyblue", alpha=0.7)
        axes[1, 1].set_title("Data Augmentation Ratio")
        axes[1, 1].set_xlabel("Iteration")
        axes[1, 1].set_ylabel("Augmentation %")
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.output_dir / "training_overview.png", dpi=300, bbox_inches="tight")
        plt.close()

        logger.info(f"ğŸ“Š Training visualization saved to {self.output_dir / 'training_overview.png'}")


def create_active_pseudo_trainer(config: Dict, **kwargs) -> ActivePseudoTrainer:
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºä¸»åŠ¨ä¼ªæ ‡ç­¾è®­ç»ƒå™¨"""
    return ActivePseudoTrainer(config, **kwargs)
