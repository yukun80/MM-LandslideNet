# =============================================================================
# src/utils/metrics.py - è‡ªå®šä¹‰æŒ‡æ ‡å’Œå›è°ƒ
# =============================================================================

"""
è‡ªå®šä¹‰æŒ‡æ ‡å’Œå›è°ƒå‡½æ•°

è¿™ä¸ªæ¨¡å—æä¾›äº†é¢å¤–çš„æŒ‡æ ‡è®¡ç®—å’Œå›è°ƒåŠŸèƒ½ï¼Œè¡¥å……Lightningå†…ç½®çš„åŠŸèƒ½ã€‚
ç‰¹åˆ«é’ˆå¯¹æ»‘å¡æ£€æµ‹ä»»åŠ¡çš„ç‰¹æ®Šéœ€æ±‚è¿›è¡Œäº†ä¼˜åŒ–ã€‚
"""

import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import Callback
import numpy as np
from typing import Dict, Any, Optional, List
import logging
from pathlib import Path
import json

logger = logging.getLogger(__name__)


class MetricsLogger(Callback):
    """
    è‡ªå®šä¹‰æŒ‡æ ‡æ—¥å¿—å›è°ƒ

    è¿™ä¸ªå›è°ƒæ‰©å±•äº†Lightningçš„æ—¥å¿—åŠŸèƒ½ï¼Œæä¾›ï¼š
    1. æ›´è¯¦ç»†çš„æŒ‡æ ‡è®°å½•
    2. å®æ—¶æ€§èƒ½ç›‘æ§
    3. è‡ªåŠ¨çš„æ¨¡å‹æ€§èƒ½åˆ†æ
    4. è®­ç»ƒè¿‡ç¨‹çš„ç»Ÿè®¡ä¿¡æ¯
    """

    def __init__(self, log_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–MetricsLogger

        Args:
            log_dir: è‡ªå®šä¹‰æ—¥å¿—ä¿å­˜ç›®å½•ã€‚å¦‚æœä¸ºNoneï¼Œä¼šå°è¯•ä½¿ç”¨trainer.log_dir
        """
        super().__init__()
        self.custom_log_dir = Path(log_dir) if log_dir else None
        self.metrics_history = []
        self.epoch_times = []
        self.best_metrics = {}

    def on_train_epoch_start(self, trainer, pl_module):
        """è®­ç»ƒepochå¼€å§‹æ—¶è®°å½•æ—¶é—´"""
        self.epoch_start_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None
        if self.epoch_start_time:
            self.epoch_start_time.record()

    def on_train_epoch_end(self, trainer, pl_module):
        """è®­ç»ƒepochç»“æŸæ—¶çš„å¤„ç†"""
        # è®°å½•epochè€—æ—¶
        if self.epoch_start_time and torch.cuda.is_available():
            epoch_end_time = torch.cuda.Event(enable_timing=True)
            epoch_end_time.record()
            torch.cuda.synchronize()
            epoch_time = self.epoch_start_time.elapsed_time(epoch_end_time) / 1000.0  # è½¬æ¢ä¸ºç§’
            self.epoch_times.append(epoch_time)

            # è®°å½•åˆ°æ—¥å¿—
            pl_module.log("epoch_time_sec", epoch_time, on_epoch=True, logger=True)

    def on_validation_epoch_end(self, trainer, pl_module):
        """éªŒè¯epochç»“æŸæ—¶çš„å¤„ç†"""
        # æ”¶é›†å½“å‰epochçš„æŒ‡æ ‡
        current_metrics = {
            "epoch": trainer.current_epoch,
            "train_loss": trainer.logged_metrics.get("train_loss_epoch", 0),
            "val_loss": trainer.logged_metrics.get("val_loss", 0),
            "val_f1": trainer.logged_metrics.get("val_f1", 0),
            "val_acc": trainer.logged_metrics.get("val_acc", 0),
            "val_auroc": trainer.logged_metrics.get("val_auroc", 0),
            "learning_rate": trainer.optimizers[0].param_groups[0]["lr"],
        }

        # å°†trainer.logged_metricsè½¬æ¢ä¸ºå­—å…¸
        for key, value in trainer.logged_metrics.items():
            if isinstance(value, torch.Tensor):
                current_metrics[key] = value.item()  # è½¬æ¢tensorä¸ºfloat
            elif hasattr(value, "item"):  # å¤„ç†numpyç­‰å…¶ä»–æ•°å€¼ç±»å‹
                current_metrics[key] = value.item()
            else:
                current_metrics[key] = value

        self.metrics_history.append(current_metrics)

        # æ›´æ–°æœ€ä½³æŒ‡æ ‡
        val_f1 = current_metrics["val_f1"]
        if isinstance(val_f1, torch.Tensor):
            val_f1 = val_f1.item()

        if "best_val_f1" not in self.best_metrics or val_f1 > self.best_metrics["best_val_f1"]:
            self.best_metrics.update(
                {
                    "best_val_f1": val_f1,
                    "best_val_f1_epoch": trainer.current_epoch,
                    "best_val_acc": current_metrics["val_acc"],
                    "best_val_auroc": current_metrics["val_auroc"],
                }
            )

        # æ¯10ä¸ªepochæ‰“å°ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
        if trainer.current_epoch % 10 == 0:
            self._log_detailed_metrics(trainer, current_metrics)

    def _get_save_directory(self) -> Path:
        """
        è·å–ä¿å­˜ç›®å½• - è‡ªå®šä¹‰è·¯å¾„é€‰æ‹©

        Returns:
            ç¡®å®šçš„ä¿å­˜ç›®å½•è·¯å¾„
        """
        target_dir = self.custom_log_dir
        logger.debug(f"Using custom log directory: {target_dir}")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        target_dir.mkdir(parents=True, exist_ok=True)
        return target_dir

    def _log_detailed_metrics(self, trainer, current_metrics):
        """è®°å½•è¯¦ç»†çš„æŒ‡æ ‡ä¿¡æ¯"""
        logger.info(f"\n{'='*100}")
        logger.info(f"Epoch {trainer.current_epoch} Summary:")
        logger.info(f"{'='*100}\n")
        logger.info(f"Train Loss: {current_metrics['train_loss']:.4f}")
        logger.info(f"Val Loss:   {current_metrics['val_loss']:.4f}")
        logger.info(f"Val F1:     {current_metrics['val_f1']:.4f}")
        logger.info(f"Val Acc:    {current_metrics['val_acc']:.4f}")
        logger.info(f"Val AUROC:  {current_metrics['val_auroc']:.4f}")
        logger.info(f"LR:         {current_metrics['learning_rate']:.6f}")

        if self.best_metrics:
            logger.info(f"\nBest Metrics So Far:")
            logger.info(
                f"Best Val F1: {self.best_metrics['best_val_f1']:.4f} (Epoch {self.best_metrics['best_val_f1_epoch']})"
            )

        if self.epoch_times:
            avg_time = np.mean(self.epoch_times[-10:])  # æœ€è¿‘10ä¸ªepochçš„å¹³å‡æ—¶é—´
            logger.info(f"Avg Epoch Time: {avg_time:.2f}s")

        logger.info(f"{'='*100}\n")

    def on_train_end(self, trainer, pl_module):
        """è®­ç»ƒç»“æŸæ—¶ä¿å­˜å®Œæ•´çš„æŒ‡æ ‡å†å² - ä½¿ç”¨æ™ºèƒ½è·¯å¾„é€‰æ‹©"""
        target_dir = self._get_save_directory()
        metrics_file = target_dir / "metrics_history.json"

        # è½¬æ¢tensorä¸ºfloat
        history_serializable = []
        for epoch_metrics in self.metrics_history:
            epoch_data = {}
            for key, value in epoch_metrics.items():
                if isinstance(value, torch.Tensor):
                    epoch_data[key] = value.item()
                else:
                    epoch_data[key] = value
            history_serializable.append(epoch_data)

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(metrics_file, "w") as f:
            json.dump(
                {
                    "metrics_history": history_serializable,
                    "best_metrics": self.best_metrics,
                    "total_epochs": trainer.current_epoch + 1,
                    "avg_epoch_time": np.mean(self.epoch_times) if self.epoch_times else 0,
                    "save_directory": str(target_dir),  # è®°å½•å®é™…ä¿å­˜è·¯å¾„
                },
                f,
                indent=2,
            )

        logger.info(f"ğŸ“ Metrics saved in directory: {target_dir}")
