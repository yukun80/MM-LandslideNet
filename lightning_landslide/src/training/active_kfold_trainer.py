# =============================================================================
# lightning_landslide/src/training/active_kfold_trainer.py
# =============================================================================

"""
ä¸»åŠ¨å­¦ä¹ +KæŠ˜äº¤å‰éªŒè¯èåˆè®­ç»ƒå™¨

è¿™ä¸ªç±»ç»“åˆäº†KæŠ˜äº¤å‰éªŒè¯çš„ç¨³å®šæ€§å’Œä¸»åŠ¨å­¦ä¹ çš„æ•°æ®æ•ˆç‡ä¼˜åŠ¿ã€‚
åœ¨æ¯ä¸ªfoldä¸­éƒ½åº”ç”¨ä¸»åŠ¨å­¦ä¹ ç­–ç•¥ï¼Œæœ€åèšåˆæ‰€æœ‰foldçš„ç»“æœã€‚

æ ¸å¿ƒç­–ç•¥ï¼š
1. å¯¹æ¯ä¸ªfoldéƒ½è¿›è¡Œä¸»åŠ¨å­¦ä¹ è®­ç»ƒ
2. ä½¿ç”¨ä¸€è‡´çš„ä¸»åŠ¨å­¦ä¹ ç­–ç•¥ç¡®ä¿å…¬å¹³æ¯”è¾ƒ
3. èšåˆæ‰€æœ‰foldçš„ä¸ç¡®å®šæ€§ä¼°è®¡
4. æä¾›æ›´robustçš„æ€§èƒ½è¯„ä¼°

è®¾è®¡æ€æƒ³ï¼š
"æ¯ä¸ªfoldéƒ½æ˜¯ä¸€ä¸ªå®Œæ•´çš„ä¸»åŠ¨å­¦ä¹ å®éªŒï¼Œæœ€ç»ˆç»“æœæ˜¯æ‰€æœ‰å®éªŒçš„ç»¼åˆ"
"""

import torch
import numpy as np
import pandas as pd
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import json
import time
from datetime import datetime
from dataclasses import dataclass, asdict
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

import pytorch_lightning as pl

from ..data.kfold_extension import create_kfold_wrapper
from ..utils.instantiate import instantiate_from_config
from .simple_kfold_trainer import SimpleKFoldTrainer

logger = logging.getLogger(__name__)


@dataclass
class ActiveKFoldResults:
    """ä¸»åŠ¨å­¦ä¹ +KæŠ˜äº¤å‰éªŒè¯çš„ç»“æœ"""

    aggregated_performance: Dict[str, float]
    cross_fold_analysis: Dict[str, Any]
    best_fold_index: int
    ensemble_model_paths: List[str]
    total_training_time: float
    data_efficiency_analysis: Dict[str, Any]

    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "fold_results": [result.to_dict() for result in self.fold_results],
            "aggregated_performance": self.aggregated_performance,
            "cross_fold_analysis": self.cross_fold_analysis,
            "best_fold_index": self.best_fold_index,
            "ensemble_model_paths": self.ensemble_model_paths,
            "total_training_time": self.total_training_time,
            "data_efficiency_analysis": self.data_efficiency_analysis,
        }


class ActiveKFoldTrainer:
    """
    ä¸»åŠ¨å­¦ä¹ +KæŠ˜äº¤å‰éªŒè¯èåˆè®­ç»ƒå™¨

    è¿™ä¸ªç±»æ˜¯SimpleKFoldTrainerå’ŒActivePseudoTrainerçš„é«˜çº§ç»„åˆï¼Œ
    åœ¨æ¯ä¸ªfoldä¸­éƒ½è¿è¡Œå®Œæ•´çš„ä¸»åŠ¨å­¦ä¹ æµç¨‹ã€‚
    """

    def __init__(self, config: Dict[str, Any], experiment_name: str = None, output_dir: str = None):
        """
        åˆå§‹åŒ–ä¸»åŠ¨KæŠ˜è®­ç»ƒå™¨

        Args:
            config: å®Œæ•´é…ç½®ï¼ˆåŒ…å«kfoldå’Œactive_pseudo_learningé…ç½®ï¼‰
            experiment_name: å®éªŒåç§°
            output_dir: è¾“å‡ºç›®å½•
        """
        self.config = config
        self.kfold_config = config.get("kfold", {})
        self.active_config = config.get("active_pseudo_learning", {})

        # KæŠ˜é…ç½®
        self.n_splits = self.kfold_config.get("n_splits", 5)
        self.stratified = self.kfold_config.get("stratified", True)
        self.seed = config.get("seed", 3407)

        # å®éªŒç®¡ç†
        self.experiment_name = experiment_name or config.get("experiment_name", f"active_kfold_{int(time.time())}")
        self.output_dir = Path(output_dir) if output_dir else Path("outputs") / self.experiment_name
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ç»“æœå­˜å‚¨
        self.fold_results = []

        logger.info(f"ğŸ”„ğŸ¯ ActiveKFoldTrainer initialized: {self.experiment_name}")
        logger.info(f"ğŸ“ Output directory: {self.output_dir}")
        logger.info(f"ğŸ² Using {self.n_splits}-fold cross-validation with active learning")

    def run(self) -> ActiveKFoldResults:
        """è¿è¡Œä¸»åŠ¨å­¦ä¹ +KæŠ˜äº¤å‰éªŒè¯"""
        logger.info("ğŸš€ Starting Active Learning + K-fold Cross-validation...")
        start_time = time.time()

        # åˆ›å»ºKæŠ˜æ•°æ®åŒ…è£…å™¨
        kfold_wrapper = self._create_kfold_wrapper()

        # ä¸ºæ¯ä¸ªfoldè¿è¡Œä¸»åŠ¨å­¦ä¹ 
        for fold_idx in range(self.n_splits):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ”„ FOLD {fold_idx + 1}/{self.n_splits}")
            logger.info(f"{'='*60}")

            fold_result = self._run_active_learning_for_fold(fold_idx, kfold_wrapper)
            self.fold_results.append(fold_result)

            # ä¿å­˜å•æŠ˜ç»“æœ
            self._save_fold_result(fold_result, fold_idx)

        # èšåˆæ‰€æœ‰foldçš„ç»“æœ
        total_time = time.time() - start_time
        final_results = self._aggregate_fold_results(total_time)

        # ä¿å­˜æœ€ç»ˆç»“æœå’Œå¯è§†åŒ–
        self._save_final_results(final_results)
        self._create_comprehensive_visualization(final_results)

        logger.info(f"\nğŸ‰ Active K-fold training completed!")
        logger.info(f"â±ï¸ Total time: {total_time:.2f}s")
        logger.info(f"ğŸ† Mean CV performance: {final_results.aggregated_performance.get('mean_val_f1', 0):.4f}")

        return final_results

    def _create_kfold_wrapper(self):
        """åˆ›å»ºKæŠ˜æ•°æ®åŒ…è£…å™¨"""
        return create_kfold_wrapper(
            base_datamodule_config=self.config["data"]["params"],
            n_splits=self.n_splits,
            stratified=self.stratified,
            seed=self.seed,
            output_dir=str(self.output_dir / "kfold_info"),
        )

    def _create_fold_config(self, fold_idx: int, kfold_wrapper) -> Dict:
        """ä¸ºç‰¹å®šfoldåˆ›å»ºé…ç½®"""
        fold_config = self.config.copy()

        # ä¿®æ”¹æ•°æ®é…ç½®ä»¥ä½¿ç”¨å½“å‰foldçš„æ•°æ®
        fold_datamodule = kfold_wrapper.get_fold_datamodule(fold_idx)

        # è¿™é‡Œéœ€è¦å°†fold_datamoduleè½¬æ¢ä¸ºé…ç½®æ ¼å¼
        # ç®€åŒ–å¤„ç†ï¼šç›´æ¥ä½¿ç”¨åŸé…ç½®ï¼Œåœ¨å®é™…æ•°æ®åŠ è½½æ—¶å¤„ç†foldåˆ†å‰²
        fold_config["fold_index"] = fold_idx
        fold_config["kfold_wrapper"] = kfold_wrapper

        return fold_config

    def _aggregate_fold_results(self, total_time: float) -> ActiveKFoldResults:
        """èšåˆæ‰€æœ‰foldçš„ç»“æœ"""
        logger.info("ğŸ“Š Aggregating results from all folds...")

        if not self.fold_results:
            raise ValueError("No fold results to aggregate")

        # è®¡ç®—èšåˆæ€§èƒ½æŒ‡æ ‡
        aggregated_performance = self._compute_aggregated_performance()

        # äº¤å‰foldåˆ†æ
        cross_fold_analysis = self._perform_cross_fold_analysis()

        # æ‰¾åˆ°æœ€ä½³fold
        best_fold_index = self._find_best_fold()

        # æ”¶é›†æ‰€æœ‰æœ€ä½³æ¨¡å‹è·¯å¾„
        ensemble_model_paths = [result.best_model_path for result in self.fold_results]

        # æ•°æ®æ•ˆç‡åˆ†æ
        data_efficiency_analysis = self._analyze_data_efficiency()

        return ActiveKFoldResults(
            fold_results=self.fold_results,
            aggregated_performance=aggregated_performance,
            cross_fold_analysis=cross_fold_analysis,
            best_fold_index=best_fold_index,
            ensemble_model_paths=ensemble_model_paths,
            total_training_time=total_time,
            data_efficiency_analysis=data_efficiency_analysis,
        )

    def _compute_aggregated_performance(self) -> Dict[str, float]:
        """è®¡ç®—èšåˆæ€§èƒ½æŒ‡æ ‡"""
        # æ”¶é›†æ‰€æœ‰foldçš„æœ€ç»ˆæ€§èƒ½
        final_performances = []
        best_performances = []

        for result in self.fold_results:
            if result.performance_history["val_f1"]:
                final_f1 = result.performance_history["val_f1"][-1]
                best_f1 = max(result.performance_history["val_f1"])

                final_performances.append(final_f1)
                best_performances.append(best_f1)

        if not final_performances:
            return {}

        final_performances = np.array(final_performances)
        best_performances = np.array(best_performances)

        return {
            "mean_final_f1": float(np.mean(final_performances)),
            "std_final_f1": float(np.std(final_performances)),
            "mean_best_f1": float(np.mean(best_performances)),
            "std_best_f1": float(np.std(best_performances)),
            "min_f1": float(np.min(best_performances)),
            "max_f1": float(np.max(best_performances)),
            "mean_val_f1": float(np.mean(best_performances)),  # ä¸»è¦æŒ‡æ ‡
            "std_cv_score": float(np.std(best_performances)),  # CVæ ‡å‡†å·®
        }

    def _perform_cross_fold_analysis(self) -> Dict[str, Any]:
        """æ‰§è¡Œäº¤å‰foldåˆ†æ"""
        analysis = {}

        # 1. æ”¶æ•›è¿­ä»£æ¬¡æ•°åˆ†æ
        convergence_iterations = [result.convergence_iteration for result in self.fold_results]
        analysis["convergence_analysis"] = {
            "mean_iterations": np.mean(convergence_iterations),
            "std_iterations": np.std(convergence_iterations),
            "min_iterations": np.min(convergence_iterations),
            "max_iterations": np.max(convergence_iterations),
        }

        # 2. æ•°æ®ä½¿ç”¨æ•ˆç‡åˆ†æ
        total_samples_used = []
        pseudo_labels_used = []

        for result in self.fold_results:
            if result.data_usage_history["training_samples"]:
                total_samples_used.append(result.data_usage_history["training_samples"][-1])
                pseudo_labels_used.append(result.data_usage_history["pseudo_labels"][-1])

        if total_samples_used:
            analysis["data_usage_consistency"] = {
                "mean_total_samples": np.mean(total_samples_used),
                "std_total_samples": np.std(total_samples_used),
                "mean_pseudo_labels": np.mean(pseudo_labels_used),
                "std_pseudo_labels": np.std(pseudo_labels_used),
                "pseudo_label_ratio": (
                    np.mean(pseudo_labels_used) / np.mean(total_samples_used) if np.mean(total_samples_used) > 0 else 0
                ),
            }

        # 3. æ€§èƒ½ç¨³å®šæ€§åˆ†æ
        if len(self.fold_results) > 1:
            fold_performances = [
                max(result.performance_history["val_f1"]) if result.performance_history["val_f1"] else 0
                for result in self.fold_results
            ]

            analysis["performance_stability"] = {
                "coefficient_of_variation": (
                    np.std(fold_performances) / np.mean(fold_performances) if np.mean(fold_performances) > 0 else 0
                ),
                "performance_range": np.max(fold_performances) - np.min(fold_performances),
                "consistency_score": (
                    1.0 - (np.std(fold_performances) / np.mean(fold_performances))
                    if np.mean(fold_performances) > 0
                    else 0
                ),
            }

        return analysis

    def _find_best_fold(self) -> int:
        """æ‰¾åˆ°è¡¨ç°æœ€å¥½çš„fold"""
        best_performance = -1
        best_fold_idx = 0

        for i, result in enumerate(self.fold_results):
            if result.performance_history["val_f1"]:
                fold_best = max(result.performance_history["val_f1"])
                if fold_best > best_performance:
                    best_performance = fold_best
                    best_fold_idx = i

        return best_fold_idx

    def _analyze_data_efficiency(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®ä½¿ç”¨æ•ˆç‡"""
        efficiency_analysis = {}

        # è®¡ç®—æ¯ä¸ªfoldçš„æ•°æ®æ•ˆç‡æŒ‡æ ‡
        fold_efficiencies = []

        for i, result in self.fold_results:
            if result.performance_history["val_f1"] and result.data_usage_history["training_samples"]:

                initial_performance = result.performance_history["val_f1"][0]
                final_performance = max(result.performance_history["val_f1"])
                initial_samples = result.data_usage_history["training_samples"][0]
                final_samples = result.data_usage_history["training_samples"][-1]

                # è®¡ç®—æ€§èƒ½æå‡ / æ•°æ®å¢é•¿æ¯”ç‡
                performance_gain = final_performance - initial_performance
                data_growth_ratio = final_samples / initial_samples if initial_samples > 0 else 1

                efficiency = performance_gain / (data_growth_ratio - 1) if data_growth_ratio > 1 else 0
                fold_efficiencies.append(efficiency)

        if fold_efficiencies:
            efficiency_analysis["fold_efficiencies"] = fold_efficiencies
            efficiency_analysis["mean_efficiency"] = np.mean(fold_efficiencies)
            efficiency_analysis["std_efficiency"] = np.std(fold_efficiencies)

        return efficiency_analysis

    def _save_final_results(self, results: ActiveKFoldResults):
        """ä¿å­˜æœ€ç»ˆèšåˆç»“æœ"""
        # ä¿å­˜å®Œæ•´ç»“æœ
        with open(self.output_dir / "active_kfold_results.json", "w") as f:
            json.dump(results.to_dict(), f, indent=2, default=str)

        # ä¿å­˜æ€§èƒ½æ‘˜è¦
        performance_summary = {
            "experiment_name": self.experiment_name,
            "n_splits": self.n_splits,
            "aggregated_performance": results.aggregated_performance,
            "best_fold_index": results.best_fold_index,
            "total_training_time": results.total_training_time,
            "timestamp": datetime.now().isoformat(),
        }

        with open(self.output_dir / "performance_summary.json", "w") as f:
            json.dump(performance_summary, f, indent=2)

        # ä¿å­˜CSVæ ¼å¼çš„æ€§èƒ½å†å²
        self._save_performance_history_csv(results)

        logger.info(f"ğŸ“ Final results saved to {self.output_dir}")

    def _save_performance_history_csv(self, results: ActiveKFoldResults):
        """ä¿å­˜æ€§èƒ½å†å²çš„CSVæ–‡ä»¶"""
        # æ”¶é›†æ‰€æœ‰foldçš„æ€§èƒ½å†å²
        all_history = []

        for fold_idx, fold_result in enumerate(results.fold_results):
            for iter_idx, val_f1 in enumerate(fold_result.performance_history["val_f1"]):
                all_history.append(
                    {
                        "fold": fold_idx,
                        "iteration": iter_idx + 1,
                        "val_f1": val_f1,
                        "train_f1": (
                            fold_result.performance_history["train_f1"][iter_idx]
                            if iter_idx < len(fold_result.performance_history["train_f1"])
                            else None
                        ),
                        "val_loss": (
                            fold_result.performance_history["val_loss"][iter_idx]
                            if iter_idx < len(fold_result.performance_history["val_loss"])
                            else None
                        ),
                        "training_samples": (
                            fold_result.data_usage_history["training_samples"][iter_idx]
                            if iter_idx < len(fold_result.data_usage_history["training_samples"])
                            else None
                        ),
                    }
                )

        df = pd.DataFrame(all_history)
        df.to_csv(self.output_dir / "performance_history.csv", index=False)

    def _create_comprehensive_visualization(self, results: ActiveKFoldResults):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–æŠ¥å‘Š"""
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        fig.suptitle(f"Active Learning + K-fold Cross-validation Results\n{self.experiment_name}", fontsize=16)

        # 1. æ¯ä¸ªfoldçš„æ€§èƒ½æ›²çº¿
        for fold_idx, fold_result in enumerate(results.fold_results):
            if fold_result.performance_history["val_f1"]:
                iterations = list(range(1, len(fold_result.performance_history["val_f1"]) + 1))
                axes[0, 0].plot(
                    iterations,
                    fold_result.performance_history["val_f1"],
                    label=f"Fold {fold_idx}",
                    marker="o",
                    alpha=0.7,
                )

        axes[0, 0].set_title("Validation F1 Score by Fold")
        axes[0, 0].set_xlabel("Iteration")
        axes[0, 0].set_ylabel("F1 Score")
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # 2. foldæ€§èƒ½åˆ†å¸ƒç®±çº¿å›¾
        fold_performances = [
            max(result.performance_history["val_f1"]) if result.performance_history["val_f1"] else 0
            for result in results.fold_results
        ]

        axes[0, 1].boxplot(fold_performances)
        axes[0, 1].scatter(range(1, len(fold_performances) + 1), fold_performances, color="red", alpha=0.7)
        axes[0, 1].set_title("Performance Distribution Across Folds")
        axes[0, 1].set_xlabel("Fold")
        axes[0, 1].set_ylabel("Best F1 Score")
        axes[0, 1].grid(True, alpha=0.3)

        # 3. æ•°æ®ä½¿ç”¨æ•ˆç‡
        for fold_idx, fold_result in enumerate(results.fold_results):
            if fold_result.data_usage_history["training_samples"]:
                iterations = list(range(1, len(fold_result.data_usage_history["training_samples"]) + 1))
                axes[0, 2].plot(
                    iterations,
                    fold_result.data_usage_history["training_samples"],
                    label=f"Fold {fold_idx}",
                    marker="s",
                    alpha=0.7,
                )

        axes[0, 2].set_title("Training Data Growth")
        axes[0, 2].set_xlabel("Iteration")
        axes[0, 2].set_ylabel("Total Training Samples")
        axes[0, 2].legend()
        axes[0, 2].grid(True, alpha=0.3)

        # 4. æ”¶æ•›è¿­ä»£æ¬¡æ•°
        convergence_iters = [result.convergence_iteration for result in results.fold_results]
        axes[1, 0].bar(range(len(convergence_iters)), convergence_iters, color="skyblue", alpha=0.7)
        axes[1, 0].set_title("Convergence Iterations by Fold")
        axes[1, 0].set_xlabel("Fold Index")
        axes[1, 0].set_ylabel("Iterations to Convergence")
        axes[1, 0].grid(True, alpha=0.3)

        # 5. ä¼ªæ ‡ç­¾ä½¿ç”¨ç»Ÿè®¡
        pseudo_label_counts = []
        for fold_result in results.fold_results:
            if fold_result.data_usage_history["pseudo_labels"]:
                pseudo_label_counts.append(fold_result.data_usage_history["pseudo_labels"][-1])
            else:
                pseudo_label_counts.append(0)

        axes[1, 1].bar(range(len(pseudo_label_counts)), pseudo_label_counts, color="orange", alpha=0.7)
        axes[1, 1].set_title("Pseudo Labels Used by Fold")
        axes[1, 1].set_xlabel("Fold Index")
        axes[1, 1].set_ylabel("Number of Pseudo Labels")
        axes[1, 1].grid(True, alpha=0.3)

        # 6. ç»¼åˆæ€§èƒ½æ‘˜è¦
        metrics = ["Mean F1", "Std F1", "Min F1", "Max F1"]
        values = [
            results.aggregated_performance.get("mean_best_f1", 0),
            results.aggregated_performance.get("std_best_f1", 0),
            results.aggregated_performance.get("min_f1", 0),
            results.aggregated_performance.get("max_f1", 0),
        ]

        bars = axes[1, 2].bar(metrics, values, color=["green", "orange", "red", "blue"], alpha=0.7)
        axes[1, 2].set_title("Aggregated Performance Metrics")
        axes[1, 2].set_ylabel("F1 Score")
        axes[1, 2].tick_params(axis="x", rotation=45)

        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
        for bar, value in zip(bars, values):
            axes[1, 2].text(
                bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.001, f"{value:.3f}", ha="center", va="bottom"
            )

        plt.tight_layout()
        plt.savefig(self.output_dir / "active_kfold_comprehensive_analysis.png", dpi=300, bbox_inches="tight")
        plt.close()

        logger.info(f"ğŸ“Š Comprehensive visualization saved")


def create_active_kfold_trainer(config: Dict, **kwargs) -> ActiveKFoldTrainer:
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºä¸»åŠ¨KæŠ˜è®­ç»ƒå™¨"""
    return ActiveKFoldTrainer(config, **kwargs)
