# =============================================================================
# lightning_landslide/src/training/kfold_trainer.py - KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒå™¨
# =============================================================================

"""
KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒå™¨ - Kaggleç«èµ›çº§åˆ«çš„æ¨¡å‹è®­ç»ƒç®¡ç†

è¿™ä¸ªæ¨¡å—æä¾›äº†å®Œæ•´çš„KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. è‡ªåŠ¨åŒ–çš„KæŠ˜è®­ç»ƒå¾ªç¯
2. OOF(Out-of-Fold)é¢„æµ‹ç”Ÿæˆå’Œç®¡ç†
3. æµ‹è¯•é›†é¢„æµ‹é›†æˆ
4. æ¨¡å‹æ€§èƒ½åˆ†æå’ŒæŠ¥å‘Š
5. æ¨¡å‹ä¿å­˜å’Œç‰ˆæœ¬ç®¡ç†
"""

import torch
import numpy as np
import pandas as pd
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import json
import time
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger
import warnings

warnings.filterwarnings("ignore")

from ..data.kfold_datamodule import KFoldDataModule, create_kfold_datamodule
from ..utils.instantiate import instantiate_from_config

logger = logging.getLogger(__name__)


class KFoldTrainer:
    """
    KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒå™¨

    ä½œä¸ºKaggleç«èµ›å¤§å¸ˆçš„æ ¸å¿ƒæ­¦å™¨ï¼Œè¿™ä¸ªç±»æä¾›äº†ï¼š
    1. å…¨è‡ªåŠ¨çš„KæŠ˜è®­ç»ƒæµç¨‹
    2. ä¸¥æ ¼çš„æ€§èƒ½ç›‘æ§å’Œæ—©åœ
    3. OOFé¢„æµ‹çš„ç”Ÿæˆå’ŒéªŒè¯
    4. æµ‹è¯•é›†é¢„æµ‹çš„é›†æˆ
    5. è¯¦ç»†çš„æ€§èƒ½åˆ†ææŠ¥å‘Š
    """

    def __init__(
        self,
        model_config: Dict[str, Any],
        data_config: Dict[str, Any],
        trainer_config: Dict[str, Any],
        # KæŠ˜é…ç½®
        n_splits: int = 5,
        stratified: bool = True,
        # è¾“å‡ºé…ç½®
        output_dir: str = "outputs/kfold_experiments",
        experiment_name: str = None,
        # æ€§èƒ½é…ç½®
        primary_metric: str = "f1",
        early_stopping_patience: int = 10,
        # å…¶ä»–é…ç½®
        seed: int = 3407,
        save_predictions: bool = True,
        save_models: bool = True,
        generate_oof: bool = True,
        **kwargs,
    ):
        """
        åˆå§‹åŒ–KæŠ˜è®­ç»ƒå™¨

        Args:
            model_config: æ¨¡å‹é…ç½®
            data_config: æ•°æ®é…ç½®
            trainer_config: è®­ç»ƒå™¨é…ç½®
            n_splits: KæŠ˜æ•°é‡
            experiment_name: å®éªŒåç§°
            primary_metric: ä¸»è¦è¯„ä¼°æŒ‡æ ‡
        """
        self.model_config = model_config
        self.data_config = data_config
        self.trainer_config = trainer_config

        # KæŠ˜é…ç½®
        self.n_splits = n_splits
        self.stratified = stratified

        # è¾“å‡ºé…ç½®
        self.output_dir = Path(output_dir)
        self.experiment_name = experiment_name or f"kfold_experiment_{int(time.time())}"
        self.experiment_dir = self.output_dir / self.experiment_name
        self.experiment_dir.mkdir(parents=True, exist_ok=True)

        # æ€§èƒ½é…ç½®
        self.primary_metric = primary_metric
        self.early_stopping_patience = early_stopping_patience

        # å…¶ä»–é…ç½®
        self.seed = seed
        self.save_predictions = save_predictions
        self.save_models = save_models
        self.generate_oof = generate_oof

        # ç»“æœå­˜å‚¨
        self.fold_results = []
        self.oof_predictions = None
        self.test_predictions = []
        self.fold_models = []

        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        self._setup_directories()

        logger.info(f"ğŸš€ KFoldTrainer initialized for {n_splits}-fold CV")
        logger.info(f"Experiment: {self.experiment_name}")
        logger.info(f"Output directory: {self.experiment_dir}")

    def _setup_directories(self) -> None:
        """è®¾ç½®è¾“å‡ºç›®å½•ç»“æ„"""
        dirs_to_create = [
            self.experiment_dir / "models",
            self.experiment_dir / "predictions",
            self.experiment_dir / "logs",
            self.experiment_dir / "plots",
            self.experiment_dir / "reports",
        ]

        for dir_path in dirs_to_create:
            dir_path.mkdir(parents=True, exist_ok=True)

    def train_kfold(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒ

        Returns:
            åŒ…å«æ‰€æœ‰æŠ˜ç»“æœçš„å­—å…¸
        """
        logger.info(f"ğŸ¯ Starting {self.n_splits}-fold cross validation training")

        # å‡†å¤‡OOFé¢„æµ‹æ•°ç»„
        if self.generate_oof:
            self._prepare_oof_arrays()

        start_time = time.time()

        # è®­ç»ƒæ¯ä¸€æŠ˜
        for fold in range(self.n_splits):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ”„ Training Fold {fold + 1}/{self.n_splits}")
            logger.info(f"{'='*60}")

            fold_start_time = time.time()

            # è®­ç»ƒå•æŠ˜
            fold_result = self._train_single_fold(fold)
            self.fold_results.append(fold_result)

            fold_time = time.time() - fold_start_time
            logger.info(f"âœ… Fold {fold + 1} completed in {fold_time:.2f}s")
            logger.info(
                f"ğŸ“Š Fold {fold + 1} {self.primary_metric}: {fold_result['val_metrics'][self.primary_metric]:.4f}"
            )

        total_time = time.time() - start_time

        # ç”Ÿæˆæœ€ç»ˆç»“æœ
        final_results = self._generate_final_results(total_time)

        # ä¿å­˜ç»“æœ
        self._save_results(final_results)

        # ç”ŸæˆæŠ¥å‘Š
        self._generate_reports()

        logger.info(f"ğŸ‰ K-Fold training completed in {total_time:.2f}s")
        logger.info(
            f"ğŸ“ˆ Mean {self.primary_metric}: {final_results['mean_cv_score']:.4f} Â± {final_results['std_cv_score']:.4f}"
        )

        return final_results

    def _prepare_oof_arrays(self) -> None:
        """å‡†å¤‡OOFé¢„æµ‹æ•°ç»„"""
        # åˆ›å»ºä¸´æ—¶æ•°æ®æ¨¡å—è·å–æ•°æ®é›†å¤§å°
        temp_dm = create_kfold_datamodule(self.data_config, 0)
        temp_dm.prepare_data()
        temp_dm.setup("fit")

        total_samples = len(temp_dm.full_dataset)

        # åˆå§‹åŒ–OOFæ•°ç»„
        self.oof_predictions = np.zeros(total_samples)
        self.oof_targets = np.zeros(total_samples)

        logger.info(f"ğŸ“‹ Prepared OOF arrays for {total_samples} samples")

    def _train_single_fold(self, fold: int) -> Dict[str, Any]:
        """
        è®­ç»ƒå•ä¸ªæŠ˜

        Args:
            fold: æŠ˜ç´¢å¼•

        Returns:
            å•æŠ˜è®­ç»ƒç»“æœ
        """
        # è®¾ç½®ç§å­ç¡®ä¿å¯é‡ç°æ€§
        pl.seed_everything(self.seed + fold)

        # åˆ›å»ºæ•°æ®æ¨¡å—
        datamodule = create_kfold_datamodule(self.data_config, fold)
        datamodule.prepare_data()
        datamodule.setup("fit")

        # åˆ›å»ºæ¨¡å‹
        model = instantiate_from_config(self.model_config)

        # è®¾ç½®å›è°ƒå‡½æ•°
        callbacks = self._create_callbacks(fold)

        # è®¾ç½®æ—¥å¿—è®°å½•å™¨
        logger_config = TensorBoardLogger(save_dir=str(self.experiment_dir / "logs"), name=f"fold_{fold}", version="")

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer_params = self.trainer_config.copy()
        trainer_params.update(
            {
                "callbacks": callbacks,
                "logger": logger_config,
                "deterministic": True,
            }
        )

        trainer = pl.Trainer(**trainer_params)

        # è®­ç»ƒæ¨¡å‹
        trainer.fit(model, datamodule)

        # éªŒè¯æ¨¡å‹
        val_results = trainer.validate(model, datamodule, verbose=False)
        val_metrics = val_results[0] if val_results else {}

        # ç”ŸæˆOOFé¢„æµ‹
        if self.generate_oof:
            self._generate_oof_predictions(model, datamodule, fold)

        # ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹
        test_predictions = self._generate_test_predictions(model, datamodule, fold)
        self.test_predictions.append(test_predictions)

        # ä¿å­˜æ¨¡å‹
        if self.save_models:
            model_path = self.experiment_dir / "models" / f"fold_{fold}_model.ckpt"
            trainer.save_checkpoint(str(model_path))
            self.fold_models.append(str(model_path))

        return {
            "fold": fold,
            "val_metrics": val_metrics,
            "model_path": str(model_path) if self.save_models else None,
            "test_predictions": test_predictions,
        }

    def _create_callbacks(self, fold: int) -> List[pl.Callback]:
        """åˆ›å»ºè®­ç»ƒå›è°ƒå‡½æ•°"""
        callbacks = []

        # æ¨¡å‹æ£€æŸ¥ç‚¹
        checkpoint_callback = ModelCheckpoint(
            dirpath=str(self.experiment_dir / "models"),
            filename=f"fold_{fold}_best_{{epoch}}_{{val_{self.primary_metric}:.4f}}",
            monitor=f"val_{self.primary_metric}",
            mode="max" if self.primary_metric in ["f1", "accuracy", "precision", "recall", "auroc"] else "min",
            save_top_k=1,
            save_last=True,
            verbose=True,
        )
        callbacks.append(checkpoint_callback)

        # æ—©åœ
        early_stopping = EarlyStopping(
            monitor=f"val_{self.primary_metric}",
            mode="max" if self.primary_metric in ["f1", "accuracy", "precision", "recall", "auroc"] else "min",
            patience=self.early_stopping_patience,
            verbose=True,
            strict=True,
        )
        callbacks.append(early_stopping)

        return callbacks

    def _generate_oof_predictions(self, model: pl.LightningModule, datamodule: KFoldDataModule, fold: int) -> None:
        """ç”ŸæˆOOFé¢„æµ‹"""
        model.eval()

        # è·å–éªŒè¯é›†ç´¢å¼•
        _, val_indices = datamodule.fold_indices[fold]

        # ç”Ÿæˆé¢„æµ‹
        predictions = []
        targets = []

        val_loader = datamodule.val_dataloader()
        with torch.no_grad():
            for batch in val_loader:
                if isinstance(batch, (list, tuple)) and len(batch) == 2:
                    x, y = batch
                    targets.extend(y.cpu().numpy())
                else:
                    x = batch

                # æ¨¡å‹é¢„æµ‹
                logits = model(x.to(model.device))
                probs = torch.sigmoid(logits).cpu().numpy()
                predictions.extend(probs.flatten())

        # å­˜å‚¨OOFé¢„æµ‹
        self.oof_predictions[val_indices] = predictions
        self.oof_targets[val_indices] = targets

    def _generate_test_predictions(
        self, model: pl.LightningModule, datamodule: KFoldDataModule, fold: int
    ) -> np.ndarray:
        """ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹"""
        model.eval()

        # è®¾ç½®æµ‹è¯•æ•°æ®
        datamodule.setup("test")
        test_loader = datamodule.test_dataloader()

        predictions = []
        with torch.no_grad():
            for batch in test_loader:
                if isinstance(batch, (list, tuple)):
                    x = batch[0]
                else:
                    x = batch

                logits = model(x.to(model.device))
                probs = torch.sigmoid(logits).cpu().numpy()
                predictions.extend(probs.flatten())

        return np.array(predictions)

    def _generate_final_results(self, total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆç»“æœæ‘˜è¦"""
        # è®¡ç®—CVåˆ†æ•°ç»Ÿè®¡
        cv_scores = [result["val_metrics"][self.primary_metric] for result in self.fold_results]
        mean_cv_score = np.mean(cv_scores)
        std_cv_score = np.std(cv_scores)

        # ç”Ÿæˆé›†æˆæµ‹è¯•é¢„æµ‹
        ensemble_predictions = np.mean(self.test_predictions, axis=0)

        # OOFæ€§èƒ½è¯„ä¼°
        oof_metrics = {}
        if self.generate_oof:
            oof_metrics = self._calculate_oof_metrics()

        # é›†æˆæ‰€æœ‰ä¿¡æ¯
        final_results = {
            "experiment_name": self.experiment_name,
            "n_splits": self.n_splits,
            "fold_results": self.fold_results,
            "cv_scores": cv_scores,
            "mean_cv_score": mean_cv_score,
            "std_cv_score": std_cv_score,
            "oof_metrics": oof_metrics,
            "ensemble_predictions": ensemble_predictions,
            "training_time": total_time,
            "timestamp": datetime.now().isoformat(),
            "config": {
                "model": self.model_config,
                "data": self.data_config,
                "trainer": self.trainer_config,
            },
        }

        return final_results

    def _calculate_oof_metrics(self) -> Dict[str, float]:
        """è®¡ç®—OOFé¢„æµ‹çš„æ€§èƒ½æŒ‡æ ‡"""
        if self.oof_predictions is None:
            return {}

        # äºŒå€¼åŒ–é¢„æµ‹
        oof_pred_binary = (self.oof_predictions > 0.5).astype(int)

        # è®¡ç®—å„ç§æŒ‡æ ‡
        f1 = f1_score(self.oof_targets, oof_pred_binary)
        auc = roc_auc_score(self.oof_targets, self.oof_predictions)

        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
        report = classification_report(self.oof_targets, oof_pred_binary, output_dict=True)

        oof_metrics = {
            "f1_score": f1,
            "auc_score": auc,
            "accuracy": report["accuracy"],
            "precision": report["macro avg"]["precision"],
            "recall": report["macro avg"]["recall"],
        }

        return oof_metrics

    def _save_results(self, results: Dict[str, Any]) -> None:
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        # ä¿å­˜ä¸»è¦ç»“æœ
        results_file = self.experiment_dir / "results.json"

        # å‡†å¤‡å¯åºåˆ—åŒ–çš„ç»“æœ
        serializable_results = results.copy()
        serializable_results["ensemble_predictions"] = results["ensemble_predictions"].tolist()

        with open(results_file, "w") as f:
            json.dump(serializable_results, f, indent=2)

        # ä¿å­˜OOFé¢„æµ‹
        if self.generate_oof and self.oof_predictions is not None:
            oof_df = pd.DataFrame({"oof_predictions": self.oof_predictions, "targets": self.oof_targets})
            oof_df.to_csv(self.experiment_dir / "oof_predictions.csv", index=False)

        # ä¿å­˜æµ‹è¯•é›†é¢„æµ‹
        if self.save_predictions:
            test_df = pd.DataFrame({"ensemble_prediction": results["ensemble_predictions"]})

            # æ·»åŠ æ¯æŠ˜çš„é¢„æµ‹
            for i, pred in enumerate(self.test_predictions):
                test_df[f"fold_{i}_prediction"] = pred

            test_df.to_csv(self.experiment_dir / "test_predictions.csv", index=False)

        logger.info(f"ğŸ“ Results saved to {self.experiment_dir}")

    def _generate_reports(self) -> None:
        """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
        # CVåˆ†æ•°åˆ†æå›¾
        self._plot_cv_scores()

        # OOFåˆ†æå›¾
        if self.generate_oof and self.oof_predictions is not None:
            self._plot_oof_analysis()

        # ç”Ÿæˆæ–‡å­—æŠ¥å‘Š
        self._generate_text_report()

    def _plot_cv_scores(self) -> None:
        """ç»˜åˆ¶CVåˆ†æ•°åˆ†æå›¾"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        # CVåˆ†æ•°åˆ†å¸ƒ
        cv_scores = [result["val_metrics"][self.primary_metric] for result in self.fold_results]
        folds = [f"Fold {i+1}" for i in range(self.n_splits)]

        ax1.bar(folds, cv_scores, color="skyblue", alpha=0.7)
        ax1.axhline(y=np.mean(cv_scores), color="red", linestyle="--", label=f"Mean: {np.mean(cv_scores):.4f}")
        ax1.set_title(f"{self.primary_metric.upper()} Score by Fold")
        ax1.set_ylabel(f"{self.primary_metric.upper()} Score")
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # CVåˆ†æ•°ç»Ÿè®¡
        ax2.boxplot(cv_scores, labels=[f"{self.primary_metric.upper()}"])
        ax2.set_title(f"CV {self.primary_metric.upper()} Distribution")
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.experiment_dir / "plots" / "cv_analysis.png", dpi=300, bbox_inches="tight")
        plt.close()

    def _plot_oof_analysis(self) -> None:
        """ç»˜åˆ¶OOFåˆ†æå›¾"""
        if self.oof_predictions is None:
            return

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

        # é¢„æµ‹åˆ†å¸ƒ
        ax1.hist(self.oof_predictions[self.oof_targets == 0], alpha=0.5, label="Negative", bins=50)
        ax1.hist(self.oof_predictions[self.oof_targets == 1], alpha=0.5, label="Positive", bins=50)
        ax1.set_title("OOF Prediction Distribution")
        ax1.set_xlabel("Prediction Probability")
        ax1.set_ylabel("Frequency")
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # ROCæ›²çº¿
        from sklearn.metrics import roc_curve

        fpr, tpr, _ = roc_curve(self.oof_targets, self.oof_predictions)
        ax2.plot(fpr, tpr, label=f"AUC = {roc_auc_score(self.oof_targets, self.oof_predictions):.4f}")
        ax2.plot([0, 1], [0, 1], "k--", alpha=0.5)
        ax2.set_title("ROC Curve")
        ax2.set_xlabel("False Positive Rate")
        ax2.set_ylabel("True Positive Rate")
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # æ··æ·†çŸ©é˜µ
        oof_pred_binary = (self.oof_predictions > 0.5).astype(int)
        cm = confusion_matrix(self.oof_targets, oof_pred_binary)
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax3)
        ax3.set_title("Confusion Matrix")
        ax3.set_xlabel("Predicted")
        ax3.set_ylabel("Actual")

        # é¢„æµ‹æ ¡å‡†å›¾
        from sklearn.calibration import calibration_curve

        fraction_of_positives, mean_predicted_value = calibration_curve(
            self.oof_targets, self.oof_predictions, n_bins=10
        )
        ax4.plot(mean_predicted_value, fraction_of_positives, "s-", label="Model")
        ax4.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
        ax4.set_title("Calibration Plot")
        ax4.set_xlabel("Mean Predicted Probability")
        ax4.set_ylabel("Fraction of Positives")
        ax4.legend()
        ax4.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.experiment_dir / "plots" / "oof_analysis.png", dpi=300, bbox_inches="tight")
        plt.close()

    def _generate_text_report(self) -> None:
        """ç”Ÿæˆæ–‡å­—æŠ¥å‘Š"""
        report_path = self.experiment_dir / "reports" / "experiment_report.md"

        cv_scores = [result["val_metrics"][self.primary_metric] for result in self.fold_results]
        mean_cv_score = np.mean(cv_scores)
        std_cv_score = np.std(cv_scores)

        report_content = f"""# K-Fold Cross Validation Report

## Experiment Information
- **Experiment Name**: {self.experiment_name}
- **Number of Folds**: {self.n_splits}
- **Primary Metric**: {self.primary_metric}
- **Stratified**: {self.stratified}
- **Seed**: {self.seed}

## Performance Summary
- **Mean CV Score**: {mean_cv_score:.4f} Â± {std_cv_score:.4f}
- **Best Fold**: Fold {np.argmax(cv_scores) + 1} ({max(cv_scores):.4f})
- **Worst Fold**: Fold {np.argmin(cv_scores) + 1} ({min(cv_scores):.4f})

## Individual Fold Results
"""

        for i, (score, result) in enumerate(zip(cv_scores, self.fold_results)):
            report_content += f"- **Fold {i+1}**: {score:.4f}\n"

        if self.generate_oof and self.oof_predictions is not None:
            oof_metrics = self._calculate_oof_metrics()
            report_content += f"""
## OOF Performance
- **F1 Score**: {oof_metrics.get('f1_score', 0):.4f}
- **AUC Score**: {oof_metrics.get('auc_score', 0):.4f}
- **Accuracy**: {oof_metrics.get('accuracy', 0):.4f}
- **Precision**: {oof_metrics.get('precision', 0):.4f}
- **Recall**: {oof_metrics.get('recall', 0):.4f}
"""

        report_content += f"""
## Configuration
```json
{json.dumps({
    'model': self.model_config,
    'data': {k: v for k, v in self.data_config.items() if k != 'transforms'},
    'trainer': self.trainer_config,
}, indent=2)}
```

## Files Generated
- `results.json`: Complete experiment results
- `oof_predictions.csv`: Out-of-fold predictions
- `test_predictions.csv`: Test set predictions
- `plots/cv_analysis.png`: Cross-validation analysis
- `plots/oof_analysis.png`: OOF prediction analysis
- `models/`: Trained model checkpoints
"""

        with open(report_path, "w") as f:
            f.write(report_content)

        logger.info(f"ğŸ“Š Report generated: {report_path}")

    def load_results(self, results_path: str) -> Dict[str, Any]:
        """åŠ è½½ä¹‹å‰çš„å®éªŒç»“æœ"""
        with open(results_path, "r") as f:
            return json.load(f)

    def predict_test(self, test_data_path: str = None) -> np.ndarray:
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•é›†é¢„æµ‹"""
        if not self.test_predictions:
            raise ValueError("No test predictions available. Run train_kfold() first.")

        return np.mean(self.test_predictions, axis=0)


# ä¾¿åˆ©å‡½æ•°
def run_kfold_experiment(config_path: str, n_splits: int = 5) -> Dict[str, Any]:
    """
    è¿è¡Œå®Œæ•´çš„KæŠ˜å®éªŒçš„ä¾¿åˆ©å‡½æ•°

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        n_splits: æŠ˜æ•°

    Returns:
        å®éªŒç»“æœ
    """
    from omegaconf import OmegaConf

    # åŠ è½½é…ç½®
    config = OmegaConf.load(config_path)

    # åˆ›å»ºKæŠ˜è®­ç»ƒå™¨
    trainer = KFoldTrainer(
        model_config=config.model,
        data_config=config.data.params,
        trainer_config=config.trainer.params,
        n_splits=n_splits,
        experiment_name=config.get("experiment_name", "kfold_experiment"),
    )

    # è¿è¡Œè®­ç»ƒ
    return trainer.train_kfold()


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO)

    print("âœ“ KFoldTrainer implementation completed!")
    print("Ready for Kaggle-level cross validation!")
