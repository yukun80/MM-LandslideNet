# =============================================================================
# lightning_landslide/src/models/classification_module.py - æ»‘å¡åˆ†ç±»è®­ç»ƒæ¨¡å—
# =============================================================================

"""
æ»‘å¡æ£€æµ‹çš„æ ¸å¿ƒLightningè®­ç»ƒæ¨¡å—

è¿™ä¸ªæ¨¡å—æ˜¯æ•´ä¸ªæ¡†æ¶çš„"å¤§è„‘"ï¼Œå®ƒç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚
è®¾è®¡å“²å­¦ä¸latent-diffusionç›¸ä¼¼ï¼š"ä¸€æ¬¡ç¼–å†™ï¼Œå¤„å¤„è¿è¡Œ"ï¼š

- ä»»ä½•ç»§æ‰¿BaseModelçš„æ¨¡å‹éƒ½èƒ½æ— ç¼æ¥å…¥
- æ”¯æŒå¤šç§æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç­–ç•¥
- è‡ªåŠ¨å¤„ç†è¯„ä¼°æŒ‡æ ‡å’Œæ—¥å¿—è®°å½•
- å†…ç½®æ¨¡å‹æ£€æŸ¥ç‚¹å’Œæ—©åœæœºåˆ¶

è¿™ä¸ªè®¾è®¡çš„å¨åŠ›åœ¨äºï¼šå½“æ‚¨æ·»åŠ æ–°çš„æ¨¡å‹ï¼ˆå¦‚InternImageã€EfficientNetï¼‰æ—¶ï¼Œ
ä¸éœ€è¦é‡å†™ä»»ä½•è®­ç»ƒé€»è¾‘ï¼Œåªéœ€è¦å®ç°BaseModelæ¥å£å³å¯ã€‚

æ•™å­¦è¦ç‚¹ï¼š
è¿™ä¸ªæ¨¡å—å±•ç¤ºäº†é¢å‘å¯¹è±¡è®¾è®¡çš„ç²¾é«“ï¼šé€šè¿‡æŠ½è±¡æ¥å£å®ç°å¤šæ€æ€§ã€‚
ä¸åŒçš„æ¨¡å‹æœ‰ä¸åŒçš„å†…éƒ¨å®ç°ï¼Œä½†å¯¹äºè®­ç»ƒæ¡†æ¶æ¥è¯´ï¼Œå®ƒä»¬éƒ½æ˜¯"æ¨¡å‹"ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
import torchmetrics
from typing import Dict, Any, Optional, List, Tuple, Callable
from omegaconf import DictConfig
import logging
import numpy as np
from pathlib import Path

from .base import BaseModel
from .optical_swin import OpticalSwinModel

logger = logging.getLogger(__name__)


class LandslideClassificationModule(pl.LightningModule):
    """
    æ»‘å¡æ£€æµ‹çš„æ ¸å¿ƒLightningæ¨¡å—

    è¿™ä¸ªç±»æ˜¯æ•´ä¸ªè®­ç»ƒæ¡†æ¶çš„æ ¸å¿ƒã€‚å®ƒæ¥å—ä»»ä½•ç¬¦åˆBaseModelæ¥å£çš„æ¨¡å‹ï¼Œ
    å¹¶ä¸ºå…¶æä¾›å®Œæ•´çš„è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ”¯æŒã€‚

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ¨¡å‹ç®¡ç†ï¼šåŠ¨æ€åˆ›å»ºå’Œé…ç½®ä¸åŒçš„æ¨¡å‹
    2. è®­ç»ƒå¾ªç¯ï¼šæ ‡å‡†åŒ–çš„è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æµç¨‹
    3. æŸå¤±è®¡ç®—ï¼šæ”¯æŒå¤šç§æŸå¤±å‡½æ•°
    4. æŒ‡æ ‡è¯„ä¼°ï¼šå…¨é¢çš„åˆ†ç±»æ€§èƒ½è¯„ä¼°
    5. ä¼˜åŒ–å™¨é…ç½®ï¼šçµæ´»çš„ä¼˜åŒ–ç­–ç•¥

    è®¾è®¡ä¼˜åŠ¿ï¼š
    - æ¨¡å‹æ— å…³ï¼šæ”¯æŒä»»ä½•BaseModelå­ç±»
    - é…ç½®é©±åŠ¨ï¼šé€šè¿‡YAMLæ–‡ä»¶æ§åˆ¶æ‰€æœ‰è¡Œä¸º
    - å¯æ‰©å±•ï¼šæ˜“äºæ·»åŠ æ–°çš„æŸå¤±å‡½æ•°å’ŒæŒ‡æ ‡
    - å¯å¤ç°ï¼šè‡ªåŠ¨ä¿å­˜é…ç½®å’Œéšæœºç§å­
    """

    def __init__(
        self,
        # æ¨¡å‹é…ç½®
        base_model: Dict[str, Any],
        # è®­ç»ƒé…ç½®
        loss_config: Dict[str, Any] = None,
        optimizer_config: Dict[str, Any] = None,
        scheduler_config: Dict[str, Any] = None,
        # è¯„ä¼°é…ç½®
        metrics_config: Dict[str, Any] = None,
        # å…¶ä»–é…ç½®
        classifier_config: Dict[str, Any] = None,
        **kwargs,
    ):
        """
        åˆå§‹åŒ–Lightningæ¨¡å—

        Args:
            base_model: åŸºç¡€æ¨¡å‹é…ç½®
            loss_config: æŸå¤±å‡½æ•°é…ç½®
            optimizer_config: ä¼˜åŒ–å™¨é…ç½®
            scheduler_config: å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
            metrics_config: è¯„ä¼°æŒ‡æ ‡é…ç½®
            classifier_config: åˆ†ç±»å¤´é…ç½®
        """
        super().__init__()

        # ä¿å­˜é…ç½®ç”¨äºæ£€æŸ¥ç‚¹æ¢å¤
        self.save_hyperparameters()

        # è§£æé…ç½®
        self.base_model_config = base_model
        self.loss_config = loss_config or {"type": "bce"}
        self.optimizer_config = optimizer_config or {"type": "adamw", "adamw_params": {"lr": 1e-4}}
        self.scheduler_config = scheduler_config
        self.metrics_config = metrics_config or {"primary_metric": "f1"}
        self.classifier_config = classifier_config or {"type": "simple"}

        # åˆ›å»ºéª¨å¹²æ¨¡å‹
        self.model = self._build_model()

        # åˆ›å»ºåˆ†ç±»å¤´
        self.classifier = self._build_classifier()

        # åˆ›å»ºæŸå¤±å‡½æ•°
        self.criterion = self._build_loss_function()

        # åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡
        self._setup_metrics()

        # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.training_step_outputs = []
        self.validation_step_outputs = []
        self.test_step_outputs = []

        logger.info("ğŸš€LandslideClassificationModule initialized" + "-" * 100)
        logger.info(f"Model: {self.model.__class__.__name__}")
        logger.info(f"Feature dim: {self.model.get_feature_dim()}")
        logger.info(f"Loss function: {self.criterion.__class__.__name__}")
        logger.info("-" * 100)

    def _build_model(self) -> BaseModel:
        """
        æ ¹æ®é…ç½®æ„å»ºéª¨å¹²æ¨¡å‹

        è¿™æ˜¯å·¥å‚æ–¹æ³•æ¨¡å¼çš„åº”ç”¨ã€‚æˆ‘ä»¬æ ¹æ®é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹ç±»å‹ï¼Œ
        åŠ¨æ€åˆ›å»ºå¯¹åº”çš„æ¨¡å‹å®ä¾‹ã€‚è¿™ç§è®¾è®¡è®©æ·»åŠ æ–°æ¨¡å‹å˜å¾—éå¸¸ç®€å•ã€‚

        Returns:
            é…ç½®å¥½çš„æ¨¡å‹å®ä¾‹
        """
        from ..utils.instantiate import instantiate_from_config

        logger.info("Building backbone model...")
        # åˆ›å»ºpytorchæ¨¡å‹
        model = instantiate_from_config(self.base_model_config)

        if not isinstance(model, BaseModel):
            raise TypeError(f"Model must inherit from BaseModel, got {type(model)}")

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        logger.info("ğŸ“¦model info:" + "-" * 100)
        model_info = model.get_model_info()
        for key, value in model_info.items():
            logger.info(f"  {key}: {value}")
        logger.info("-" * 100)

        return model

    def _build_classifier(self) -> nn.Module:
        """
        æ„å»ºåˆ†ç±»å¤´

        åˆ†ç±»å¤´æ˜¯è¿æ¥ç‰¹å¾æå–å™¨å’Œæœ€ç»ˆé¢„æµ‹çš„æ¡¥æ¢ã€‚
        æˆ‘ä»¬æ ¹æ®ä»»åŠ¡ç±»å‹å’Œé…ç½®å‚æ•°æ¥è®¾è®¡åˆ†ç±»å¤´ã€‚

        Returns:
            é…ç½®å¥½çš„åˆ†ç±»å¤´æ¨¡å—
        """
        feature_dim = self.model.get_feature_dim()
        num_classes = self.base_model_config["params"].get("num_classes", 1)
        dropout_rate = self.classifier_config.get("dropout_rate", 0.2)

        # æ”¯æŒå¤šç§åˆ†ç±»å¤´è®¾è®¡
        classifier_type = self.classifier_config.get("type", "simple")

        if classifier_type == "simple":
            # ç®€å•åˆ†ç±»å¤´ï¼šLayerNorm + Dropout + Linear
            classifier = nn.Sequential(
                nn.LayerNorm(feature_dim), nn.Dropout(dropout_rate), nn.Linear(feature_dim, num_classes)
            )
        elif classifier_type == "mlp":
            # MLPåˆ†ç±»å¤´ï¼šæ›´å¤æ‚çš„å¤šå±‚æ„ŸçŸ¥æœº
            hidden_dim = self.classifier_config.get("hidden_dim", feature_dim // 2)
            classifier = nn.Sequential(
                nn.LayerNorm(feature_dim),
                nn.Dropout(dropout_rate),
                nn.Linear(feature_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout_rate),
                nn.Linear(hidden_dim, num_classes),
            )
        elif classifier_type == "attention":
            # åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„åˆ†ç±»å¤´ï¼ˆå¯ä»¥è¿›ä¸€æ­¥å®ç°ï¼‰
            raise NotImplementedError("Attention classifier not implemented yet")
        else:
            raise ValueError(f"Unknown classifier type: {classifier_type}")

        logger.info(f"Built {classifier_type} classifier: {feature_dim} -> {num_classes}")
        return classifier

    def _build_loss_function(self) -> nn.Module:
        """
        æ„å»ºæŸå¤±å‡½æ•°

        æ”¯æŒå¤šç§æŸå¤±å‡½æ•°ï¼Œç‰¹åˆ«é’ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜æä¾›äº†å¤šç§è§£å†³æ–¹æ¡ˆã€‚
        è¿™ç§è®¾è®¡è®©æˆ‘ä»¬å¯ä»¥è½»æ¾å°è¯•ä¸åŒçš„æŸå¤±å‡½æ•°æ¥æ‰¾åˆ°æœ€é€‚åˆçš„æ–¹æ¡ˆã€‚

        Returns:
            é…ç½®å¥½çš„æŸå¤±å‡½æ•°
        """
        loss_type = self.loss_config["type"]

        if loss_type == "bce":
            # æ ‡å‡†äºŒå…ƒäº¤å‰ç†µæŸå¤±
            return nn.BCEWithLogitsLoss()

        elif loss_type == "weighted_bce":
            # åŠ æƒäºŒå…ƒäº¤å‰ç†µï¼Œå¤„ç†ç±»åˆ«ä¸å¹³è¡¡
            pos_weight = torch.tensor(self.loss_config.get("pos_weight", 1.0))
            return nn.BCEWithLogitsLoss(pos_weight=pos_weight)

        elif loss_type == "focal":
            # Focal Lossï¼Œä¸“é—¨å¤„ç†éš¾æ ·æœ¬å’Œç±»åˆ«ä¸å¹³è¡¡
            focal_params = self.loss_config.get("focal_params", {})
            return FocalLoss(alpha=focal_params.get("alpha", 1.0), gamma=focal_params.get("gamma", 2.0))

        elif loss_type == "dice":
            # Dice Lossï¼Œå¸¸ç”¨äºåˆ†å‰²ä»»åŠ¡ï¼Œä¹Ÿé€‚ç”¨äºä¸å¹³è¡¡åˆ†ç±»
            return DiceLoss(smooth=self.loss_config.get("smooth", 1.0))

        elif loss_type == "combined":
            # ç»„åˆæŸå¤±ï¼šå¯ä»¥ç»“åˆå¤šç§æŸå¤±å‡½æ•°çš„ä¼˜åŠ¿
            # ä¾‹å¦‚ï¼šBCE + Diceï¼Œæˆ–è€… Focal + Dice
            raise NotImplementedError("Combined loss not implemented yet")

        else:
            raise ValueError(f"Unknown loss type: {loss_type}")

    def _setup_metrics(self) -> None:
        """
        è®¾ç½®è¯„ä¼°æŒ‡æ ‡

        ä¸ºè®­ç»ƒå’ŒéªŒè¯è®¾ç½®å…¨é¢çš„åˆ†ç±»æ€§èƒ½è¯„ä¼°æŒ‡æ ‡ã€‚
        è¿™äº›æŒ‡æ ‡å¸®åŠ©æˆ‘ä»¬ä»å¤šä¸ªè§’åº¦è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

        åœ¨æ»‘å¡æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬ç‰¹åˆ«å…³æ³¨ï¼š
        - F1åˆ†æ•°ï¼šå¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡ï¼Œé€‚åˆä¸å¹³è¡¡æ•°æ®
        - AUROCï¼šæ¨¡å‹åŒºåˆ†èƒ½åŠ›çš„ç»¼åˆè¯„ä¼°
        - ç²¾ç¡®ç‡ï¼šé¢„æµ‹ä¸ºæ»‘å¡çš„å‡†ç¡®ç¨‹åº¦
        - å¬å›ç‡ï¼šçœŸå®æ»‘å¡è¢«æ£€æµ‹å‡ºçš„æ¯”ä¾‹
        """
        task_type = "binary"  # å‡è®¾æ˜¯äºŒåˆ†ç±»ä»»åŠ¡
        num_classes = 2

        # è®­ç»ƒæŒ‡æ ‡ï¼ˆè®¡ç®—å¼€é”€è¾ƒå°çš„æŒ‡æ ‡ï¼‰
        self.train_acc = torchmetrics.Accuracy(task=task_type, num_classes=num_classes)
        self.train_f1 = torchmetrics.F1Score(task=task_type, num_classes=num_classes)

        # éªŒè¯æŒ‡æ ‡ï¼ˆæ›´å…¨é¢çš„è¯„ä¼°ï¼‰
        self.val_acc = torchmetrics.Accuracy(task=task_type, num_classes=num_classes)
        self.val_f1 = torchmetrics.F1Score(task=task_type, num_classes=num_classes)
        self.val_precision = torchmetrics.Precision(task=task_type, num_classes=num_classes)
        self.val_recall = torchmetrics.Recall(task=task_type, num_classes=num_classes)
        self.val_auroc = torchmetrics.AUROC(task=task_type, num_classes=num_classes)

        # æµ‹è¯•æŒ‡æ ‡
        self.test_acc = torchmetrics.Accuracy(task=task_type, num_classes=num_classes)
        self.test_f1 = torchmetrics.F1Score(task=task_type, num_classes=num_classes)
        self.test_precision = torchmetrics.Precision(task=task_type, num_classes=num_classes)
        self.test_recall = torchmetrics.Recall(task=task_type, num_classes=num_classes)
        self.test_auroc = torchmetrics.AUROC(task=task_type, num_classes=num_classes)

        # æ··æ·†çŸ©é˜µï¼ˆç”¨äºè¯¦ç»†åˆ†æï¼‰
        self.val_confmat = torchmetrics.ConfusionMatrix(task=task_type, num_classes=num_classes)
        self.test_confmat = torchmetrics.ConfusionMatrix(task=task_type, num_classes=num_classes)

        logger.info(f"Setup metrics for {task_type} classification")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥å¼ é‡ [batch_size, channels, height, width]

        Returns:
            é¢„æµ‹logits [batch_size, num_classes]
        """
        # é€šè¿‡éª¨å¹²ç½‘ç»œæå–ç‰¹å¾
        features = self.model(x)

        # é€šè¿‡åˆ†ç±»å¤´å¾—åˆ°é¢„æµ‹
        logits = self.classifier(features)

        return logits

    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:
        """
        è®­ç»ƒæ­¥éª¤

        è¿™ä¸ªæ–¹æ³•å®šä¹‰äº†å•ä¸ªè®­ç»ƒæ‰¹æ¬¡çš„å¤„ç†é€»è¾‘ã€‚PyTorch Lightning
        ä¼šè‡ªåŠ¨è°ƒç”¨è¿™ä¸ªæ–¹æ³•ï¼Œå¹¶å¤„ç†æ¢¯åº¦è®¡ç®—ã€åå‘ä¼ æ’­ç­‰ç»†èŠ‚ã€‚

        Args:
            batch: è¾“å…¥æ‰¹æ¬¡ (x, y)
            batch_idx: æ‰¹æ¬¡ç´¢å¼•

        Returns:
            æŸå¤±å€¼
        """
        x, y = batch

        # å‰å‘ä¼ æ’­
        logits = self(x)

        # ç¡®ä¿æ ‡ç­¾ç»´åº¦æ­£ç¡®
        if logits.dim() == 2 and logits.size(1) == 1:
            logits = logits.squeeze(1)

        # è®¡ç®—æŸå¤±
        loss = self.criterion(logits, y)

        # è®¡ç®—é¢„æµ‹æ¦‚ç‡
        probs = torch.sigmoid(logits)
        preds = (probs > 0.5).float()

        # æ›´æ–°è®­ç»ƒæŒ‡æ ‡
        self.train_acc(preds, y)
        self.train_f1(preds, y)

        # è®°å½•æŒ‡æ ‡
        self.log("train_loss", loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log("train_acc", self.train_acc, on_step=False, on_epoch=True, prog_bar=True)
        self.log("train_f1", self.train_f1, on_step=False, on_epoch=True, prog_bar=True)

        return loss

    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> None:
        """
        éªŒè¯æ­¥éª¤

        éªŒè¯è¿‡ç¨‹ä¸éœ€è¦æ¢¯åº¦è®¡ç®—ï¼Œä¸»è¦ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æœªè§è¿‡æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚
        æˆ‘ä»¬ä½¿ç”¨æ›´å…¨é¢çš„æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹ã€‚

        Args:
            batch: è¾“å…¥æ‰¹æ¬¡ (x, y)
            batch_idx: æ‰¹æ¬¡ç´¢å¼•
        """
        x, y = batch

        # å‰å‘ä¼ æ’­
        logits = self(x)

        # ç¡®ä¿æ ‡ç­¾ç»´åº¦æ­£ç¡®
        if logits.dim() == 2 and logits.size(1) == 1:
            logits = logits.squeeze(1)

        # è®¡ç®—æŸå¤±
        loss = self.criterion(logits, y)

        # è®¡ç®—é¢„æµ‹æ¦‚ç‡å’Œé¢„æµ‹ç±»åˆ«
        probs = torch.sigmoid(logits)
        preds = (probs > 0.5).float()

        # æ›´æ–°éªŒè¯æŒ‡æ ‡
        self.val_acc(preds, y)
        self.val_f1(preds, y)
        self.val_precision(preds, y)
        self.val_recall(preds, y)
        self.val_auroc(probs, y.long())
        self.val_confmat(preds, y)

        # è®°å½•æŒ‡æ ‡
        self.log("val_loss", loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log("val_acc", self.val_acc, on_step=False, on_epoch=True, prog_bar=True)
        self.log("val_f1", self.val_f1, on_step=False, on_epoch=True, prog_bar=True)
        self.log("val_precision", self.val_precision, on_step=False, on_epoch=True)
        self.log("val_recall", self.val_recall, on_step=False, on_epoch=True)
        self.log("val_auroc", self.val_auroc, on_step=False, on_epoch=True)

    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> None:
        """
        æµ‹è¯•æ­¥éª¤

        æµ‹è¯•é˜¶æ®µæä¾›æœ€ç»ˆçš„æ¨¡å‹æ€§èƒ½è¯„ä¼°ï¼Œé€šå¸¸åœ¨æ¨¡å‹é€‰æ‹©å’Œç»“æœæŠ¥å‘Šä¸­ä½¿ç”¨ã€‚

        Args:
            batch: è¾“å…¥æ‰¹æ¬¡ (x, y)
            batch_idx: æ‰¹æ¬¡ç´¢å¼•
        """
        x, y = batch

        # å‰å‘ä¼ æ’­
        logits = self(x)

        # ç¡®ä¿æ ‡ç­¾ç»´åº¦æ­£ç¡®
        if logits.dim() == 2 and logits.size(1) == 1:
            logits = logits.squeeze(1)

        # è®¡ç®—æŸå¤±
        loss = self.criterion(logits, y)

        # è®¡ç®—é¢„æµ‹æ¦‚ç‡å’Œé¢„æµ‹ç±»åˆ«
        probs = torch.sigmoid(logits)
        preds = (probs > 0.5).float()

        # æ›´æ–°æµ‹è¯•æŒ‡æ ‡
        self.test_acc(preds, y)
        self.test_f1(preds, y)
        self.test_precision(preds, y)
        self.test_recall(preds, y)
        self.test_auroc(probs, y.long())
        self.test_confmat(preds, y)

        # è®°å½•æŒ‡æ ‡
        self.log("test_loss", loss, on_step=False, on_epoch=True)
        self.log("test_acc", self.test_acc, on_step=False, on_epoch=True)
        self.log("test_f1", self.test_f1, on_step=False, on_epoch=True)
        self.log("test_precision", self.test_precision, on_step=False, on_epoch=True)
        self.log("test_recall", self.test_recall, on_step=False, on_epoch=True)
        self.log("test_auroc", self.test_auroc, on_step=False, on_epoch=True)

    def predict_step(self, batch: torch.Tensor, batch_idx: int) -> Dict[str, torch.Tensor]:
        """
        é¢„æµ‹æ­¥éª¤

        ç”¨äºç”Ÿæˆé¢„æµ‹ç»“æœï¼Œé€šå¸¸ç”¨äºæ¨ç†å’Œç«èµ›æäº¤ã€‚

        Args:
            batch: è¾“å…¥æ‰¹æ¬¡
            batch_idx: æ‰¹æ¬¡ç´¢å¼•

        Returns:
            åŒ…å«é¢„æµ‹ç»“æœçš„å­—å…¸
        """
        x = batch if isinstance(batch, torch.Tensor) else batch[0]

        # å‰å‘ä¼ æ’­
        logits = self(x)

        # ç¡®ä¿logitsç»´åº¦æ­£ç¡®
        if logits.dim() == 2 and logits.size(1) == 1:
            logits = logits.squeeze(1)

        # è®¡ç®—é¢„æµ‹æ¦‚ç‡å’Œé¢„æµ‹ç±»åˆ«
        probs = torch.sigmoid(logits)
        preds = (probs > 0.5).float()

        return {"logits": logits, "probabilities": probs, "predictions": preds}

    def configure_optimizers(self) -> Dict[str, Any]:
        """
        é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨

        è¿™ä¸ªæ–¹æ³•å±•ç¤ºäº†å¦‚ä½•çµæ´»é…ç½®ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥ã€‚
        å®ƒæ”¯æŒå·®åˆ†å­¦ä¹ ç‡ã€å­¦ä¹ ç‡è°ƒåº¦ç­‰é«˜çº§æŠ€å·§ã€‚

        Returns:
            ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨é…ç½®
        """
        # è·å–æ¨¡å‹å‚æ•°
        if self.optimizer_config.get("differential_lr", {}).get("enable", False):
            # å·®åˆ†å­¦ä¹ ç‡ï¼šéª¨å¹²ç½‘ç»œä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œåˆ†ç±»å¤´ä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡
            optimizer_params = self._create_param_groups()
        else:
            # ç»Ÿä¸€å­¦ä¹ ç‡
            optimizer_params = self.parameters()

        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer_type = self.optimizer_config["type"]
        if optimizer_type == "adamw":
            optimizer = torch.optim.AdamW(optimizer_params, **self.optimizer_config["adamw_params"])
        elif optimizer_type == "adam":
            optimizer = torch.optim.Adam(optimizer_params, **self.optimizer_config["adam_params"])
        elif optimizer_type == "sgd":
            optimizer = torch.optim.SGD(optimizer_params, **self.optimizer_config["sgd_params"])
        else:
            raise ValueError(f"Unknown optimizer type: {optimizer_type}")

        # é…ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if self.scheduler_config is None:
            return optimizer

        scheduler_type = self.scheduler_config["type"]
        if scheduler_type == "cosine_with_warmup":
            from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts

            scheduler = CosineAnnealingWarmRestarts(optimizer, **self.scheduler_config["cosine_params"])
        elif scheduler_type == "step":
            from torch.optim.lr_scheduler import StepLR

            scheduler = StepLR(optimizer, **self.scheduler_config["step_params"])
        elif scheduler_type == "reduce_on_plateau":
            from torch.optim.lr_scheduler import ReduceLROnPlateau

            scheduler = ReduceLROnPlateau(optimizer, **self.scheduler_config["plateau_params"])
        else:
            raise ValueError(f"Unknown scheduler type: {scheduler_type}")

        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "monitor": self.scheduler_config.get("monitor", "val_loss"),
                "frequency": self.scheduler_config.get("frequency", 1),
            },
        }

    def _create_param_groups(self) -> List[Dict[str, Any]]:
        """
        åˆ›å»ºå·®åˆ†å­¦ä¹ ç‡å‚æ•°ç»„

        è¿™æ˜¯ä¸€ä¸ªé«˜çº§æŠ€å·§ï¼šå¯¹é¢„è®­ç»ƒçš„éª¨å¹²ç½‘ç»œä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œ
        å¯¹æ–°çš„åˆ†ç±»å¤´ä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡ã€‚è¿™æ ·å¯ä»¥åœ¨å¾®è°ƒæ—¶è·å¾—æ›´å¥½çš„æ•ˆæœã€‚

        Returns:
            å‚æ•°ç»„åˆ—è¡¨
        """
        differential_config = self.optimizer_config["differential_lr"]
        base_lr = self.optimizer_config["adamw_params"]["lr"]

        backbone_lr = base_lr * differential_config.get("backbone_lr_ratio", 0.1)
        classifier_lr = base_lr * differential_config.get("classifier_lr_ratio", 1.0)

        param_groups = [
            {"params": self.model.parameters(), "lr": backbone_lr, "name": "backbone"},
            {"params": self.classifier.parameters(), "lr": classifier_lr, "name": "classifier"},
        ]

        logger.info(f"Created parameter groups with differential learning rates:")
        logger.info(f"  Backbone LR: {backbone_lr}")
        logger.info(f"  Classifier LR: {classifier_lr}")

        return param_groups


class FocalLoss(nn.Module):
    """
    Focal Losså®ç° - ä¸“é—¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜

    Focal Lossæ˜¯Facebook AI Researchæå‡ºçš„æŸå¤±å‡½æ•°ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†
    æåº¦ä¸å¹³è¡¡çš„åˆ†ç±»é—®é¢˜ã€‚åœ¨é¥æ„Ÿæ»‘å¡æ£€æµ‹ä¸­ï¼Œè´Ÿæ ·æœ¬ï¼ˆéæ»‘å¡ï¼‰é€šå¸¸
    è¿œå¤šäºæ­£æ ·æœ¬ï¼ˆæ»‘å¡ï¼‰ï¼Œè¿™æ­£æ˜¯Focal Lossçš„ç”¨æ­¦ä¹‹åœ°ã€‚

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. é™ä½æ˜“åˆ†ç±»æ ·æœ¬çš„æƒé‡ï¼ˆè®©æ¨¡å‹ä¸“æ³¨äºå›°éš¾æ ·æœ¬ï¼‰
    2. å¢åŠ å°‘æ•°ç±»çš„æƒé‡ï¼ˆè§£å†³ç±»åˆ«ä¸å¹³è¡¡ï¼‰

    å…¬å¼ï¼šFL(p_t) = -Î±(1-p_t)^Î³ * log(p_t)
    å…¶ä¸­ï¼šp_tæ˜¯æ¨¡å‹å¯¹æ­£ç¡®ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡
    Î±æ§åˆ¶ç±»åˆ«æƒé‡ï¼ŒÎ³æ§åˆ¶å›°éš¾æ ·æœ¬çš„æƒé‡
    """

    def __init__(self, alpha: float = 1.0, gamma: float = 2.0, reduction: str = "mean"):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        targets = targets.float()  # å°†Longç±»å‹è½¬æ¢ä¸ºFloatç±»å‹

        # è®¡ç®—äºŒå…ƒäº¤å‰ç†µ
        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction="none")

        # è®¡ç®—é¢„æµ‹æ¦‚ç‡
        p_t = torch.exp(-bce_loss)

        # åº”ç”¨focal weight: (1 - p_t)^gamma
        focal_weight = (1 - p_t) ** self.gamma

        # åº”ç”¨ç±»åˆ«æƒé‡alpha
        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)

        # è®¡ç®—focal loss
        focal_loss = alpha_t * focal_weight * bce_loss

        if self.reduction == "mean":
            return focal_loss.mean()
        elif self.reduction == "sum":
            return focal_loss.sum()
        else:
            return focal_loss


class DiceLoss(nn.Module):
    """
    Dice Losså®ç° - é€‚ç”¨äºåˆ†å‰²ä»»åŠ¡çš„æŸå¤±å‡½æ•°

    Dice LossåŸºäºDiceç³»æ•°ï¼ˆä¹Ÿç§°ä¸ºF1åˆ†æ•°ï¼‰ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†
    åˆ†å‰²ä»»åŠ¡ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨æ»‘å¡æ£€æµ‹ä¸­ï¼Œå®ƒèƒ½å¤Ÿæ›´å¥½åœ°
    å…³æ³¨é¢„æµ‹åŒºåŸŸä¸çœŸå®åŒºåŸŸçš„é‡å ç¨‹åº¦ã€‚
    """

    def __init__(self, smooth: float = 1.0):
        super().__init__()
        self.smooth = smooth

    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        # åº”ç”¨sigmoidè·å¾—æ¦‚ç‡
        inputs = torch.sigmoid(inputs)

        # å±•å¹³å¼ é‡
        inputs = inputs.view(-1)
        targets = targets.view(-1)

        # è®¡ç®—äº¤é›†å’Œå¹¶é›†
        intersection = (inputs * targets).sum()
        dice_coeff = (2.0 * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)

        return 1 - dice_coeff
