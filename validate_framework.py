#!/usr/bin/env python3
"""
å®Œæ•´æ¡†æ¶åŠŸèƒ½æµ‹è¯•è„šæœ¬

è¿™ä¸ªè„šæœ¬æ˜¯æˆ‘ä»¬é‡æ„å·¥ä½œçš„"æœ€ç»ˆéªŒæ”¶æµ‹è¯•"ã€‚å®ƒéªŒè¯æ–°æ¡†æ¶çš„æ‰€æœ‰åŠŸèƒ½ï¼š
- è®­ç»ƒ (train)
- æµ‹è¯• (test)
- é¢„æµ‹ (predict)
- éªŒè¯ (validate)

å°±åƒæ˜¯ä¸€ä¸ªäº§å“å‡ºå‚å‰çš„"å…¨é¢è´¨æ£€"ï¼Œç¡®ä¿æ¯ä¸ªåŠŸèƒ½éƒ½å®Œç¾å·¥ä½œã€‚

æ•™å­¦è¦ç‚¹ï¼š
è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•ç³»ç»Ÿæ€§åœ°æµ‹è¯•ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶çš„å®Œæ•´åŠŸèƒ½ã€‚
å®ƒä¸ä»…æµ‹è¯•äº†å„ä¸ªä»»åŠ¡èƒ½å¦è¿è¡Œï¼Œè¿˜éªŒè¯äº†å®ƒä»¬çš„è¾“å‡ºæ˜¯å¦ç¬¦åˆé¢„æœŸã€‚
è¿™ç§å…¨é¢çš„æµ‹è¯•æ–¹æ³•æ˜¯è½¯ä»¶å·¥ç¨‹ä¸­çš„é‡è¦å®è·µã€‚
"""

import sys
import os
import subprocess
import tempfile
import yaml
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from datetime import datetime
import shutil

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(), logging.FileHandler("comprehensive_test.log")],
)
logger = logging.getLogger(__name__)


class ComprehensiveFrameworkTester:
    """
    å®Œæ•´æ¡†æ¶æµ‹è¯•å™¨

    è¿™ä¸ªç±»å°±åƒæ˜¯ä¸€ä¸ªä¸¥æ ¼çš„"è´¨é‡æ£€æŸ¥å‘˜"ï¼Œå®ƒä¼šç³»ç»Ÿæ€§åœ°æµ‹è¯•
    æˆ‘ä»¬æ¡†æ¶çš„æ¯ä¸€ä¸ªåŠŸèƒ½ï¼Œç¡®ä¿æ²¡æœ‰ä»»ä½•é—æ¼ã€‚

    æµ‹è¯•æµç¨‹ï¼š
    1. å‡†å¤‡æµ‹è¯•ç¯å¢ƒå’Œé…ç½®æ–‡ä»¶
    2. è¿è¡Œè®­ç»ƒä»»åŠ¡ï¼ˆäº§ç”Ÿæ£€æŸ¥ç‚¹ï¼‰
    3. åŸºäºè®­ç»ƒç»“æœè¿è¡Œæµ‹è¯•ä»»åŠ¡
    4. è¿è¡Œé¢„æµ‹ä»»åŠ¡
    5. è¿è¡ŒéªŒè¯ä»»åŠ¡
    6. éªŒè¯æ‰€æœ‰è¾“å‡ºæ–‡ä»¶
    7. æ¸…ç†æµ‹è¯•ç¯å¢ƒ
    """

    def __init__(self):
        self.project_root = Path(__file__).parent
        self.test_dir = self.project_root / "test_framework_outputs"
        self.temp_configs = []
        self.test_results = {}
        self.checkpoint_path = None

    def setup_test_environment(self):
        """
        è®¾ç½®æµ‹è¯•ç¯å¢ƒ

        åˆ›å»ºä¸´æ—¶ç›®å½•å’Œé…ç½®æ–‡ä»¶ï¼Œä¸ºæµ‹è¯•åšå‡†å¤‡ã€‚
        è¿™å°±åƒæ˜¯åœ¨å®éªŒå®¤ä¸­å‡†å¤‡æ‰€æœ‰éœ€è¦çš„å™¨æå’Œææ–™ã€‚
        """
        logger.info("ğŸ—ï¸  Setting up test environment...")

        # åˆ›å»ºæµ‹è¯•è¾“å‡ºç›®å½•
        if self.test_dir.exists():
            shutil.rmtree(self.test_dir)
        self.test_dir.mkdir(parents=True)

        # åˆ›å»ºå­ç›®å½•
        for subdir in ["checkpoints", "logs", "predictions", "test_results", "configs"]:
            (self.test_dir / subdir).mkdir(parents=True)

        logger.info(f"âœ… Test environment created at: {self.test_dir}")

    def create_test_configs(self) -> Dict[str, Path]:
        """
        åˆ›å»ºæ‰€æœ‰ä»»åŠ¡çš„æµ‹è¯•é…ç½®æ–‡ä»¶

        ä¸ºæ¯ä¸ªä»»åŠ¡ï¼ˆtrain/test/predict/validateï¼‰åˆ›å»ºä¸“é—¨çš„é…ç½®æ–‡ä»¶ã€‚
        è¿™äº›é…ç½®éƒ½ç»è¿‡ä¼˜åŒ–ï¼Œèƒ½å¤Ÿå¿«é€Ÿè¿è¡Œå¹¶äº§ç”Ÿå¯éªŒè¯çš„ç»“æœã€‚

        Returns:
            åŒ…å«æ‰€æœ‰é…ç½®æ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        logger.info("ğŸ“„ Creating test configuration files...")

        configs = {}

        # åŸºç¡€é…ç½®ï¼šç”¨äºè®­ç»ƒä»»åŠ¡
        train_config = {
            "experiment_name": "framework_test_train",
            "description": "Training task test for comprehensive framework validation",
            "seed": 42,
            "log_level": "WARNING",  # å‡å°‘æ—¥å¿—è¾“å‡º
            "model": {
                "target": "lightning_landslide.src.models.LandslideClassificationModule",
                "params": {
                    "base_model": {
                        "target": "lightning_landslide.src.models.optical_swin.OpticalSwinModel",
                        "params": {
                            "model_name": "swin_tiny_patch4_window7_224",
                            "input_channels": 5,
                            "num_classes": 1,
                            "dropout_rate": 0.1,
                            "pretrained": False,  # é¿å…ä¸‹è½½é¢„è®­ç»ƒæƒé‡
                        },
                    },
                    "loss_config": {"type": "bce"},
                    "optimizer_config": {"type": "adamw", "adamw_params": {"lr": 1e-3, "weight_decay": 1e-4}},
                    "scheduler_config": {"type": "constant"},
                    "metrics_config": {"primary_metric": "f1", "metrics": ["accuracy", "f1", "auroc"]},
                },
            },
            "data": {
                "target": "lightning_landslide.src.data.DummyDataModule",
                "params": {
                    "batch_size": 8,
                    "num_samples": 64,  # å°æ•°æ®é›†ï¼Œå¿«é€Ÿè®­ç»ƒ
                    "input_channels": 5,
                    "image_size": 64,
                    "num_workers": 0,
                },
            },
            "trainer": {
                "target": "pytorch_lightning.Trainer",
                "params": {
                    "max_epochs": 2,  # åªè®­ç»ƒ2ä¸ªepoch
                    "limit_train_batches": 4,  # æ¯ä¸ªepochåªè®­ç»ƒ4ä¸ªbatch
                    "limit_val_batches": 2,  # æ¯æ¬¡éªŒè¯åªç”¨2ä¸ªbatch
                    "enable_progress_bar": False,
                    "enable_model_summary": False,
                    "logger": False,
                    "accelerator": "cpu",  # å¼ºåˆ¶ä½¿ç”¨CPUï¼Œé¿å…GPUç›¸å…³é—®é¢˜
                    "devices": 1,
                    "fast_dev_run": False,
                    "check_val_every_n_epoch": 1,
                },
            },
            "callbacks": {
                "model_checkpoint": {
                    "target": "pytorch_lightning.callbacks.ModelCheckpoint",
                    "params": {
                        "dirpath": str(self.test_dir / "checkpoints"),
                        "filename": "test_model",
                        "monitor": "val_f1",
                        "mode": "max",
                        "save_top_k": 1,
                        "save_last": True,
                    },
                }
            },
            "outputs": {
                "checkpoint_dir": str(self.test_dir / "checkpoints"),
                "log_dir": str(self.test_dir / "logs"),
                "predictions_dir": str(self.test_dir / "predictions"),
                "figures_dir": str(self.test_dir / "figures"),
            },
        }

        # ä¿å­˜è®­ç»ƒé…ç½®
        train_config_path = self.test_dir / "configs" / "train_config.yaml"
        with open(train_config_path, "w") as f:
            yaml.dump(train_config, f, default_flow_style=False)
        configs["train"] = train_config_path
        self.temp_configs.append(train_config_path)

        # æµ‹è¯•é…ç½®ï¼šåŸºäºè®­ç»ƒé…ç½®ï¼Œä½†ç”¨äºæµ‹è¯•
        test_config = train_config.copy()
        test_config.update(
            {
                "experiment_name": "framework_test_test",
                "description": "Test task for comprehensive framework validation",
                "checkpoint_path": str(self.test_dir / "checkpoints" / "test_model.ckpt"),
            }
        )
        # ä¿®æ”¹traineré…ç½®ï¼Œä¸“é—¨ç”¨äºæµ‹è¯•
        test_config["trainer"]["params"].update(
            {"logger": False, "enable_checkpointing": False, "limit_test_batches": 2}
        )

        test_config_path = self.test_dir / "configs" / "test_config.yaml"
        with open(test_config_path, "w") as f:
            yaml.dump(test_config, f, default_flow_style=False)
        configs["test"] = test_config_path
        self.temp_configs.append(test_config_path)

        # é¢„æµ‹é…ç½®ï¼šç”¨äºæ¨ç†ä»»åŠ¡
        predict_config = train_config.copy()
        predict_config.update(
            {
                "experiment_name": "framework_test_predict",
                "description": "Prediction task for comprehensive framework validation",
                "checkpoint_path": str(self.test_dir / "checkpoints" / "test_model.ckpt"),
            }
        )
        predict_config["trainer"]["params"].update(
            {"logger": False, "enable_checkpointing": False, "limit_predict_batches": 2}
        )

        predict_config_path = self.test_dir / "configs" / "predict_config.yaml"
        with open(predict_config_path, "w") as f:
            yaml.dump(predict_config, f, default_flow_style=False)
        configs["predict"] = predict_config_path
        self.temp_configs.append(predict_config_path)

        # éªŒè¯é…ç½®ï¼šç”¨äºéªŒè¯ä»»åŠ¡
        validate_config = train_config.copy()
        validate_config.update(
            {
                "experiment_name": "framework_test_validate",
                "description": "Validation task for comprehensive framework validation",
                "checkpoint_path": str(self.test_dir / "checkpoints" / "test_model.ckpt"),
            }
        )
        validate_config["trainer"]["params"].update(
            {"logger": False, "enable_checkpointing": False, "limit_val_batches": 2}
        )

        validate_config_path = self.test_dir / "configs" / "validate_config.yaml"
        with open(validate_config_path, "w") as f:
            yaml.dump(validate_config, f, default_flow_style=False)
        configs["validate"] = validate_config_path
        self.temp_configs.append(validate_config_path)

        logger.info(f"âœ… Created {len(configs)} test configuration files")
        return configs

    def run_task_test(self, task: str, config_path: Path, timeout: int = 300) -> Tuple[bool, str, str]:
        """
        è¿è¡Œå•ä¸ªä»»åŠ¡çš„æµ‹è¯•

        Args:
            task: ä»»åŠ¡åç§° (train/test/predict/validate)
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            (success, stdout, stderr) å…ƒç»„
        """
        logger.info(f"ğŸ§ª Testing {task} task...")

        # æ„å»ºå‘½ä»¤
        cmd = [sys.executable, "main.py", task, str(config_path)]

        try:
            # è¿è¡Œå‘½ä»¤
            logger.info(f"  Running: {' '.join(cmd)}")
            result = subprocess.run(cmd, cwd=self.project_root, capture_output=True, text=True, timeout=timeout)

            success = result.returncode == 0

            if success:
                logger.info(f"âœ… {task} task completed successfully")
            else:
                logger.error(f"âŒ {task} task failed with return code {result.returncode}")
                logger.error(f"  Error output: {result.stderr[:500]}...")  # åªæ˜¾ç¤ºå‰500ä¸ªå­—ç¬¦

            return success, result.stdout, result.stderr

        except subprocess.TimeoutExpired:
            logger.error(f"âŒ {task} task timed out after {timeout} seconds")
            return False, "", f"Task timed out after {timeout} seconds"
        except Exception as e:
            logger.error(f"âŒ {task} task failed with exception: {e}")
            return False, "", str(e)

    def verify_outputs(self, task: str) -> bool:
        """
        éªŒè¯ä»»åŠ¡è¾“å‡ºæ–‡ä»¶

        æ£€æŸ¥æ¯ä¸ªä»»åŠ¡æ˜¯å¦äº§ç”Ÿäº†é¢„æœŸçš„è¾“å‡ºæ–‡ä»¶ã€‚
        è¿™å°±åƒæ˜¯æ£€æŸ¥å·¥å‚ç”Ÿäº§çº¿æ˜¯å¦äº§å‡ºäº†åˆæ ¼çš„äº§å“ã€‚

        Args:
            task: ä»»åŠ¡åç§°

        Returns:
            éªŒè¯æ˜¯å¦é€šè¿‡
        """
        logger.info(f"ğŸ” Verifying {task} task outputs...")

        verification_passed = True

        if task == "train":
            # è®­ç»ƒä»»åŠ¡åº”è¯¥äº§ç”Ÿæ£€æŸ¥ç‚¹æ–‡ä»¶
            checkpoint_dir = self.test_dir / "checkpoints"
            checkpoint_files = list(checkpoint_dir.glob("*.ckpt"))

            if checkpoint_files:
                self.checkpoint_path = checkpoint_files[0]
                logger.info(f"  âœ… Found checkpoint: {self.checkpoint_path}")
            else:
                logger.error("  âŒ No checkpoint files found")
                verification_passed = False

        elif task == "test":
            # æµ‹è¯•ä»»åŠ¡åº”è¯¥äº§ç”Ÿç»“æœæ–‡ä»¶
            results_dir = self.test_dir / "predictions"
            result_files = list(results_dir.glob("test_results_*.json"))

            if result_files:
                logger.info(f"  âœ… Found test results: {result_files[0]}")
                # éªŒè¯ç»“æœæ–‡ä»¶å†…å®¹
                try:
                    with open(result_files[0], "r") as f:
                        results = json.load(f)
                    if "test_results" in results:
                        logger.info("  âœ… Test results contain expected fields")
                    else:
                        logger.error("  âŒ Test results missing expected fields")
                        verification_passed = False
                except Exception as e:
                    logger.error(f"  âŒ Failed to read test results: {e}")
                    verification_passed = False
            else:
                logger.error("  âŒ No test result files found")
                verification_passed = False

        elif task == "predict":
            # é¢„æµ‹ä»»åŠ¡åº”è¯¥äº§ç”Ÿé¢„æµ‹æ–‡ä»¶
            predictions_dir = self.test_dir / "predictions"
            pred_files = list(predictions_dir.glob("predictions_*.json"))

            if pred_files:
                logger.info(f"  âœ… Found predictions: {pred_files[0]}")
                # éªŒè¯é¢„æµ‹æ–‡ä»¶å†…å®¹
                try:
                    with open(pred_files[0], "r") as f:
                        predictions = json.load(f)
                    if "predictions" in predictions and len(predictions["predictions"]) > 0:
                        logger.info(f"  âœ… Found {len(predictions['predictions'])} predictions")
                    else:
                        logger.error("  âŒ Predictions file is empty or malformed")
                        verification_passed = False
                except Exception as e:
                    logger.error(f"  âŒ Failed to read predictions: {e}")
                    verification_passed = False
            else:
                logger.error("  âŒ No prediction files found")
                verification_passed = False

        elif task == "validate":
            # éªŒè¯ä»»åŠ¡åº”è¯¥äº§ç”ŸéªŒè¯ç»“æœæ–‡ä»¶
            results_dir = self.test_dir / "predictions"
            val_files = list(results_dir.glob("validation_results_*.json"))

            if val_files:
                logger.info(f"  âœ… Found validation results: {val_files[0]}")
            else:
                logger.error("  âŒ No validation result files found")
                verification_passed = False

        return verification_passed

    def run_comprehensive_test(self) -> bool:
        """
        è¿è¡Œå®Œæ•´çš„æ¡†æ¶æµ‹è¯•

        è¿™æ˜¯æµ‹è¯•çš„ä¸»å‡½æ•°ï¼Œå®ƒæŒ‰ç…§æ­£ç¡®çš„é¡ºåºæ‰§è¡Œæ‰€æœ‰æµ‹è¯•ï¼Œ
        å¹¶ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Šã€‚

        Returns:
            æ‰€æœ‰æµ‹è¯•æ˜¯å¦éƒ½é€šè¿‡
        """
        logger.info("ğŸš€ Starting comprehensive framework test")
        logger.info("=" * 70)

        try:
            # 1. è®¾ç½®æµ‹è¯•ç¯å¢ƒ
            self.setup_test_environment()

            # 2. åˆ›å»ºæµ‹è¯•é…ç½®
            configs = self.create_test_configs()

            # 3. æŒ‰é¡ºåºè¿è¡Œæ‰€æœ‰ä»»åŠ¡æµ‹è¯•
            test_sequence = [
                ("train", configs["train"]),
                ("test", configs["test"]),
                ("predict", configs["predict"]),
                ("validate", configs["validate"]),
            ]

            all_tests_passed = True

            for task, config_path in test_sequence:
                logger.info(f"\n{'='*20} Testing {task.upper()} Task {'='*20}")

                # è¿è¡Œä»»åŠ¡
                success, stdout, stderr = self.run_task_test(task, config_path)

                # è®°å½•ç»“æœ
                self.test_results[task] = {
                    "success": success,
                    "stdout_length": len(stdout),
                    "stderr_length": len(stderr),
                    "has_errors": len(stderr) > 0,
                }

                if success:
                    # éªŒè¯è¾“å‡º
                    output_valid = self.verify_outputs(task)
                    self.test_results[task]["output_valid"] = output_valid

                    if not output_valid:
                        all_tests_passed = False
                else:
                    all_tests_passed = False
                    self.test_results[task]["output_valid"] = False

                # å¦‚æœè®­ç»ƒå¤±è´¥ï¼Œåç»­æµ‹è¯•æ— æ³•è¿›è¡Œ
                if task == "train" and not success:
                    logger.error("âŒ Training failed, skipping remaining tests")
                    break

            # 4. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            self.generate_test_report(all_tests_passed)

            return all_tests_passed

        except Exception as e:
            logger.error(f"ğŸ’¥ Comprehensive test failed with exception: {e}")
            return False
        finally:
            # æ¸…ç†æµ‹è¯•ç¯å¢ƒï¼ˆå¯é€‰ï¼‰
            # self.cleanup()
            pass

    def generate_test_report(self, all_passed: bool):
        """
        ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š

        Args:
            all_passed: æ˜¯å¦æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡
        """
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“Š COMPREHENSIVE TEST REPORT")
        logger.info("=" * 70)

        # æ€»ä½“çŠ¶æ€
        overall_status = "âœ… PASSED" if all_passed else "âŒ FAILED"
        logger.info(f"Overall Status: {overall_status}")
        logger.info(f"Test Environment: {self.test_dir}")
        logger.info(f"Test Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # å„ä»»åŠ¡è¯¦ç»†ç»“æœ
        logger.info("\nğŸ“‹ Task Results:")
        for task, results in self.test_results.items():
            status = "âœ… PASSED" if results["success"] and results.get("output_valid", True) else "âŒ FAILED"
            logger.info(f"  {task.upper()}: {status}")

            if not results["success"]:
                logger.info(f"    - Execution failed")
            elif not results.get("output_valid", True):
                logger.info(f"    - Output validation failed")
            else:
                logger.info(f"    - All checks passed")

        # è¾“å‡ºæ–‡ä»¶ç»Ÿè®¡
        logger.info(f"\nğŸ“ Generated Files:")
        for file_path in self.test_dir.rglob("*"):
            if file_path.is_file() and file_path.suffix in [".ckpt", ".json", ".csv", ".yaml"]:
                size_mb = file_path.stat().st_size / (1024 * 1024)
                logger.info(f"  {file_path.relative_to(self.test_dir)}: {size_mb:.2f} MB")

        # ç»“è®ºå’Œå»ºè®®
        logger.info(f"\nğŸ¯ Conclusion:")
        if all_passed:
            logger.info("ğŸ‰ All tests passed! Your framework is ready for production use.")
            logger.info("ğŸ’¡ You can now safely use all four task modes:")
            logger.info("   - python main.py train config.yaml")
            logger.info("   - python main.py test config.yaml")
            logger.info("   - python main.py predict config.yaml")
            logger.info("   - python main.py validate config.yaml")
        else:
            logger.info("âš ï¸  Some tests failed. Please review the errors above.")
            logger.info("ğŸ› ï¸  Fix the failing components before using the framework.")

        logger.info("=" * 70)

    def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        logger.info("ğŸ§¹ Cleaning up test environment...")

        try:
            if self.test_dir.exists():
                shutil.rmtree(self.test_dir)
                logger.info("âœ… Test directory cleaned up")
        except Exception as e:
            logger.warning(f"âš ï¸  Failed to cleanup test directory: {e}")


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„æ¡†æ¶æµ‹è¯•"""
    print("ğŸ” MM-LandslideNet Comprehensive Framework Test")
    print("=" * 70)
    print("This test validates all four task modes of the new framework:")
    print("- Training (train)")
    print("- Testing (test)")
    print("- Prediction (predict)")
    print("- Validation (validate)")
    print("=" * 70)

    tester = ComprehensiveFrameworkTester()
    success = tester.run_comprehensive_test()

    if success:
        print("\nğŸ‰ SUCCESS: All framework components are working correctly!")
        print("ğŸš€ Your MM-LandslideNet framework is ready for real experiments!")
        return 0
    else:
        print("\nğŸ’¥ FAILURE: Some framework components need attention.")
        print("ğŸ› ï¸  Please review the test output above and fix any issues.")
        return 1


if __name__ == "__main__":
    exit(main())
