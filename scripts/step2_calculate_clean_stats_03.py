#!/usr/bin/env python3
"""
æ¸…æ´æ•°æ®é›†ç»Ÿè®¡è®¡ç®—å™¨ (RGBè¿‡æ»¤)
åœ¨åŸºäºRGBå…‰å­¦é€šé“è¯„ä¼°æ’é™¤ä½è´¨é‡å›¾åƒåï¼Œ
è®¡ç®—æ¸…æ´æ•°æ®é›†çš„é€šé“ç»Ÿè®¡(å‡å€¼/æ ‡å‡†å·®)å’Œç±»åˆ«å¹³è¡¡ã€‚
"""

import os
import sys
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from configs.config import Config


class RGBFilteredDatasetAnalyzer:
    """
    ä½¿ç”¨åŸºäºRGBçš„è´¨é‡è¿‡æ»¤è®¡ç®—æ¸…æ´æ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯
    """

    def __init__(self, config):
        """ä½¿ç”¨é¡¹ç›®é…ç½®åˆå§‹åŒ–"""
        self.config = config

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.config.create_dirs()

        # åŠ è½½è®­ç»ƒæ ‡ç­¾
        self.train_df = pd.read_csv(self.config.TRAIN_CSV)
        print(f"å·²åŠ è½½è®­ç»ƒå…ƒæ•°æ®: {len(self.train_df)} ä¸ªæ ·æœ¬")

        # å®šä¹‰é€šé“ç»„ä»¥ä¾¿ç»„ç»‡ç»Ÿè®¡
        self.channel_groups = {
            "optical": {
                "channels": [0, 1, 2, 3],  # çº¢ã€ç»¿ã€è“ã€è¿‘çº¢å¤–
                "name": "Sentinel-2 å…‰å­¦",
                "description": "çº¢ã€ç»¿ã€è“ã€è¿‘çº¢å¤–",
            },
            "sar_descending": {
                "channels": [4, 5],  # VV, VH ä¸‹è¡Œ
                "name": "SAR ä¸‹è¡Œ",
                "description": "VV, VH ä¸‹è¡Œé€šé“",
            },
            "sar_desc_diff": {
                "channels": [6, 7],  # Diff VV, Diff VH ä¸‹è¡Œ
                "name": "SAR ä¸‹è¡Œå·®åˆ†",
                "description": "VV, VH ä¸‹è¡Œå·®åˆ†",
            },
            "sar_ascending": {
                "channels": [8, 9],  # VV, VH ä¸Šè¡Œ
                "name": "SAR ä¸Šè¡Œ",
                "description": "VV, VH ä¸Šè¡Œé€šé“",
            },
            "sar_asc_diff": {
                "channels": [10, 11],  # Diff VV, Diff VH ä¸Šè¡Œ
                "name": "SAR ä¸Šè¡Œå·®åˆ†",
                "description": "VV, VH ä¸Šè¡Œå·®åˆ†",
            },
        }

        print(f"ğŸ“Š å·²å®šä¹‰é€šé“ç»„:")
        for group_name, group_info in self.channel_groups.items():
            print(f"   {group_info['name']}: é€šé“ {group_info['channels']}")

    def load_rgb_exclusion_list(self):
        """
        ä»JSONæ–‡ä»¶åŠ è½½åŸºäºRGBçš„æ’é™¤åˆ—è¡¨
        """
        exclusion_file = self.config.DATASET_ROOT / "data_check" / "exclude_ids.json"

        if not exclusion_file.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°RGBæ’é™¤åˆ—è¡¨: {exclusion_file}")

        with open(exclusion_file, "r", encoding="utf-8") as f:
            exclusion_data = json.load(f)

        excluded_ids = set(exclusion_data["excluded_image_ids"])
        print(f"ğŸ“‹ å·²åŠ è½½åŸºäºRGBçš„æ’é™¤åˆ—è¡¨: {len(excluded_ids)} å¼ å›¾åƒéœ€è¦æ’é™¤")
        print(f"   RGBé˜ˆå€¼: {exclusion_data['threshold']:.4f}")
        print(f"   é˜ˆå€¼æŒ‡æ ‡: {exclusion_data.get('threshold_metric', 'rgb_std_mean')}")
        print(f"   æ’é™¤æ–¹æ³•: {exclusion_data.get('threshold_method', 'åŸºäºRGB')}")
        print(f"   æ’é™¤ç™¾åˆ†æ¯”: {exclusion_data['excluded_percentage']:.1f}%")

        return excluded_ids, exclusion_data

    def get_clean_dataset_info(self, excluded_ids):
        """
        è·å–RGBè¿‡æ»¤åæ¸…æ´æ•°æ®é›†çš„ä¿¡æ¯
        """
        # è¿‡æ»¤æ’é™¤çš„å›¾åƒ
        clean_mask = ~self.train_df["ID"].isin(excluded_ids)
        clean_df = self.train_df[clean_mask].copy()

        print(f"\nğŸ§¹ RGBè¿‡æ»¤åçš„æ¸…æ´æ•°æ®é›†ä¿¡æ¯:")
        print(f"   åŸå§‹æ•°æ®é›†: {len(self.train_df)} å¼ å›¾åƒ")
        print(f"   RGBæ’é™¤çš„å›¾åƒ: {len(excluded_ids)} å¼ å›¾åƒ")
        print(f"   æ¸…æ´æ•°æ®é›†: {len(clean_df)} å¼ å›¾åƒ")
        print(f"   ä¿ç•™ç‡: {len(clean_df)/len(self.train_df)*100:.1f}%")

        # æ¸…æ´æ•°æ®é›†ä¸­çš„ç±»åˆ«åˆ†å¸ƒ
        clean_class_counts = pd.Series(clean_df["label"]).value_counts().sort_index()
        print(f"\n   æ¸…æ´æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ:")
        print(f"   ç±»åˆ«0 (éæ»‘å¡): {clean_class_counts[0]} ({clean_class_counts[0]/len(clean_df)*100:.1f}%)")
        print(f"   ç±»åˆ«1 (æ»‘å¡): {clean_class_counts[1]} ({clean_class_counts[1]/len(clean_df)*100:.1f}%)")

        # ä¸åŸå§‹åˆ†å¸ƒæ¯”è¾ƒ
        original_class_counts = pd.Series(self.train_df["label"]).value_counts().sort_index()
        print(f"\n   åŸå§‹vs RGBæ¸…æ´ç±»åˆ«ä¿ç•™æƒ…å†µ:")
        for class_label in [0, 1]:
            original_count = original_class_counts[class_label]
            clean_count = clean_class_counts[class_label]
            retention_rate = clean_count / original_count * 100
            print(f"   ç±»åˆ«{class_label}: {clean_count}/{original_count} ({retention_rate:.1f}% ä¿ç•™)")

        return clean_df

    def calculate_channel_statistics_by_group(self, clean_df, excluded_ids):
        """
        åˆ†åˆ«è®¡ç®—æ¯ä¸ªé€šé“ç»„çš„ç»Ÿè®¡ä¿¡æ¯
        """
        print(f"\nğŸ“Š åœ¨RGBè¿‡æ»¤æ•°æ®é›†ä¸ŠæŒ‰ç»„è®¡ç®—é€šé“ç»Ÿè®¡...")
        print(f"æ­£åœ¨å¤„ç† {len(clean_df)} å¼ æ¸…æ´å›¾åƒ...")

        # æŒ‰é€šé“ç»„åˆå§‹åŒ–æ•°æ®æ”¶é›†
        group_data = {}
        for group_name, group_info in self.channel_groups.items():
            group_data[group_name] = {channel: [] for channel in group_info["channels"]}

        processed_count = 0
        failed_loads = []

        for idx, row in tqdm(clean_df.iterrows(), total=len(clean_df), desc="å¤„ç†RGBæ¸…æ´å›¾åƒ"):

            image_id = row["ID"]

            # å¦‚æœåœ¨æ’é™¤åˆ—è¡¨ä¸­åˆ™è·³è¿‡(åŒé‡æ£€æŸ¥)
            if image_id in excluded_ids:
                continue

            image_path = self.config.TRAIN_DATA_DIR / f"{image_id}.npy"

            try:
                # åŠ è½½å›¾åƒæ•°æ®
                if not image_path.exists():
                    print(f"è­¦å‘Š: æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶: {image_path}")
                    failed_loads.append(image_id)
                    continue

                image_data = np.load(image_path)

                # éªŒè¯å½¢çŠ¶
                if image_data.shape != (self.config.IMG_HEIGHT, self.config.IMG_WIDTH, self.config.IMG_CHANNELS):
                    print(f"è­¦å‘Š: {image_id} çš„å½¢çŠ¶æ„å¤–: {image_data.shape}")
                    failed_loads.append(image_id)
                    continue

                # ä¸ºæ¯ä¸ªé€šé“ç»„æ”¶é›†æ•°æ®
                for group_name, group_info in self.channel_groups.items():
                    for channel in group_info["channels"]:
                        channel_data = image_data[:, :, channel].flatten()
                        group_data[group_name][channel].extend(channel_data)

                processed_count += 1

            except Exception as e:
                print(f"å¤„ç† {image_id} æ—¶å‡ºé”™: {str(e)}")
                failed_loads.append(image_id)
                continue

        print(f"âœ… æˆåŠŸå¤„ç†äº† {processed_count} å¼ RGBæ¸…æ´å›¾åƒ")
        if failed_loads:
            print(f"âŒ å¤±è´¥åŠ è½½ {len(failed_loads)} å¼ å›¾åƒ")

        return group_data, processed_count

    def compute_group_statistics(self, group_data):
        """
        è®¡ç®—æ¯ä¸ªé€šé“ç»„çš„ç»Ÿè®¡ä¿¡æ¯
        """
        print("ğŸ“ˆ è®¡ç®—é€šé“ç»„ç»Ÿè®¡...")

        group_stats = {}

        for group_name, group_info in self.channel_groups.items():
            print(f"\nğŸ” æ­£åœ¨å¤„ç† {group_info['name']} é€šé“...")

            group_stats[group_name] = {
                "name": group_info["name"],
                "description": group_info["description"],
                "channels": {},
            }

            for channel in group_info["channels"]:
                if len(group_data[group_name][channel]) > 0:
                    channel_values = np.array(group_data[group_name][channel])

                    # è®¡ç®—ç»¼åˆç»Ÿè®¡
                    mean_val = float(np.mean(channel_values))
                    std_val = float(np.std(channel_values))
                    min_val = float(np.min(channel_values))
                    max_val = float(np.max(channel_values))
                    median_val = float(np.median(channel_values))
                    q25_val = float(np.percentile(channel_values, 25))
                    q75_val = float(np.percentile(channel_values, 75))

                    group_stats[group_name]["channels"][f"channel_{channel}"] = {
                        "channel_index": channel,
                        "name": self.config.CHANNEL_DESCRIPTIONS[channel],
                        "mean": mean_val,
                        "std": std_val,
                        "min": min_val,
                        "max": max_val,
                        "median": median_val,
                        "q25": q25_val,
                        "q75": q75_val,
                        "pixel_count": len(channel_values),
                    }

                    print(
                        f"   é€šé“ {channel:2d} ({self.config.CHANNEL_DESCRIPTIONS[channel]:25s}): "
                        f"å‡å€¼={mean_val:8.3f}, æ ‡å‡†å·®={std_val:8.3f}"
                    )
                else:
                    print(f"è­¦å‘Š: é€šé“ {channel} æ²¡æœ‰æ•°æ®")

        return group_stats

    def save_final_statistics(self, group_stats, clean_df, excluded_ids, exclusion_data, processed_count):
        """
        ä¿å­˜RGBè¿‡æ»¤æ•°æ®é›†çš„ç»¼åˆç»Ÿè®¡ä¿¡æ¯
        """
        print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆRGBè¿‡æ»¤ç»Ÿè®¡...")

        # å‡†å¤‡ç»¼åˆç»Ÿè®¡
        final_stats = {
            "dataset_info": {
                "original_image_count": len(self.train_df),
                "excluded_image_count": len(excluded_ids),
                "clean_image_count": len(clean_df),
                "processed_image_count": processed_count,
                "retention_rate": len(clean_df) / len(self.train_df),
                "exclusion_threshold": exclusion_data["threshold"],
                "exclusion_method": exclusion_data.get("threshold_method", "rgb_å…‰å­¦é€šé“_ç¬¬5ç™¾åˆ†ä½æ•°"),
                "exclusion_metric": exclusion_data.get("threshold_metric", "rgb_std_mean"),
                "filtering_approach": "RGB_å…‰å­¦é€šé“_ä»…é™",
            },
            "class_distribution": {
                "original": {
                    "class_0": int(self.train_df[self.train_df["label"] == 0].shape[0]),
                    "class_1": int(self.train_df[self.train_df["label"] == 1].shape[0]),
                },
                "clean": {
                    "class_0": int(clean_df[clean_df["label"] == 0].shape[0]),
                    "class_1": int(clean_df[clean_df["label"] == 1].shape[0]),
                },
            },
            "channel_statistics_by_group": group_stats,
            "data_specifications": {
                "image_height": self.config.IMG_HEIGHT,
                "image_width": self.config.IMG_WIDTH,
                "num_channels": self.config.IMG_CHANNELS,
                "channel_descriptions": self.config.CHANNEL_DESCRIPTIONS,
                "channel_groups": {
                    name: {"name": info["name"], "description": info["description"], "channels": info["channels"]}
                    for name, info in self.channel_groups.items()
                },
            },
            "quality_assessment_info": {
                "rgb_quality_filtering": True,
                "rgb_channels_used": [0, 1, 2],  # çº¢ã€ç»¿ã€è“
                "sar_channels_excluded_from_quality": [4, 5, 6, 7, 8, 9, 10, 11],
                "quality_metrics": ["rgb_std_mean", "rgb_contrast", "rgb_brightness"],
                "threshold_percentile": exclusion_data.get("statistics", {}).get("threshold_percentile", 5.0),
            },
            "processing_info": {
                "script_version": "2.0_RGBè¿‡æ»¤",
                "processing_date": pd.Timestamp.now().isoformat(),
                "config_file": "configs/config.py",
            },
        }

        # è®¡ç®—ç±»åˆ«å¹³è¡¡æŒ‡æ ‡
        clean_class_counts = pd.Series(clean_df["label"]).value_counts().sort_index()
        class_balance = {
            "class_0_count": int(clean_class_counts[0]),
            "class_1_count": int(clean_class_counts[1]),
            "class_0_percentage": float(clean_class_counts[0] / len(clean_df)),
            "class_1_percentage": float(clean_class_counts[1] / len(clean_df)),
            "imbalance_ratio": float(clean_class_counts[0] / clean_class_counts[1]),
            "minority_class": 1 if clean_class_counts[1] < clean_class_counts[0] else 0,
        }

        final_stats["class_balance"] = class_balance

        # ä¿å­˜åˆ°JSON
        output_file = self.config.DATASET_ROOT / "data_check" / "channel_stats.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(final_stats, f, indent=2, ensure_ascii=False)

        print(f"âœ… æœ€ç»ˆRGBè¿‡æ»¤ç»Ÿè®¡å·²ä¿å­˜åˆ°: {output_file}")

        # æ‰“å°ç»¼åˆæ‘˜è¦
        print(f"\nğŸ“‹ æœ€ç»ˆRGBè¿‡æ»¤æ•°æ®é›†æ‘˜è¦:")
        print(f"   ğŸ“¸ è¿‡æ»¤æ–¹æ³•: ä»…RGBå…‰å­¦é€šé“ (é€šé“0-2)")
        print(f"   ğŸ§¹ æ¸…æ´å›¾åƒ: {len(clean_df)} (ä¿ç•™ç‡: {len(clean_df)/len(self.train_df)*100:.1f}%)")
        print(
            f"   âš–ï¸  ç±»åˆ«å¹³è¡¡: {class_balance['class_0_count']} : {class_balance['class_1_count']} "
            f"(æ¯”ä¾‹: {class_balance['imbalance_ratio']:.2f})"
        )
        print(f"   ğŸ“Š å¤„ç†çš„é€šé“ç»„: {len(group_stats)}")
        print(
            f"   ğŸ¯ æ€»å¤„ç†åƒç´ : {processed_count * self.config.IMG_HEIGHT * self.config.IMG_WIDTH * self.config.IMG_CHANNELS:,}"
        )

        print(f"\nğŸ“ˆ é€šé“ç»„ç»Ÿè®¡æ‘˜è¦:")
        for group_name, group_info in group_stats.items():
            channel_count = len(group_info["channels"])
            print(f"   {group_info['name']:20s}: {channel_count} ä¸ªé€šé“å·²å¤„ç†")

        return output_file


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("ğŸš€ é˜¶æ®µ1 - æ­¥éª¤3: RGBè¿‡æ»¤çš„æ¸…æ´æ•°æ®é›†ç»Ÿè®¡")
    print("=" * 70)
    print("ğŸ“¸ ä½¿ç”¨åŸºäºRGBçš„è´¨é‡è¯„ä¼°è¿›è¡Œæ•°æ®è¿‡æ»¤")
    print("ğŸš« SARé€šé“å·²ä»è´¨é‡è¯„ä¼°ä¸­æ’é™¤(ä½†åŒ…å«åœ¨ç»Ÿè®¡ä¸­)")
    print("=" * 70)

    # åˆå§‹åŒ–é…ç½®
    config = Config()

    # æ£€æŸ¥è®­ç»ƒæ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
    if not config.TRAIN_DATA_DIR.exists():
        print(f"âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®ç›®å½•: {config.TRAIN_DATA_DIR}")
        print("è¯·ç¡®ä¿è®­ç»ƒæ•°æ®(.npyæ–‡ä»¶)å¯ç”¨ã€‚")
        return False

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = RGBFilteredDatasetAnalyzer(config)

    try:
        # åŠ è½½åŸºäºRGBçš„æ’é™¤åˆ—è¡¨
        excluded_ids, exclusion_data = analyzer.load_rgb_exclusion_list()

        # è·å–æ¸…æ´æ•°æ®é›†ä¿¡æ¯
        clean_df = analyzer.get_clean_dataset_info(excluded_ids)

        # æŒ‰ç»„è®¡ç®—é€šé“ç»Ÿè®¡
        group_data, processed_count = analyzer.calculate_channel_statistics_by_group(clean_df, excluded_ids)

        # è®¡ç®—ç»„ç»Ÿè®¡
        group_stats = analyzer.compute_group_statistics(group_data)

        if not group_stats:
            print("âŒ æ²¡æœ‰è®¡ç®—é€šé“ç»Ÿè®¡!")
            return False

        # ä¿å­˜æœ€ç»ˆç»Ÿè®¡
        output_file = analyzer.save_final_statistics(
            group_stats, clean_df, excluded_ids, exclusion_data, processed_count
        )

        print("\nğŸ‰ æ­¥éª¤3å®Œæˆ!")
        print(f"ğŸ“Š ç¡®å®šçš„RGBè¿‡æ»¤é€šé“ç»Ÿè®¡: {output_file}")
        print("ğŸ¯ é˜¶æ®µ1 åŸºäºRGBçš„æ•°æ®æ¸…ç†å®Œæˆ!")
        print("\nâœ¨ ç›¸æ¯”ä»¥å‰æ–¹æ³•çš„å…³é”®æ”¹è¿›:")
        print("   â€¢ ä»…RGBè´¨é‡è¯„ä¼°(æ›´å¯é )")
        print("   â€¢ æŒ‰é€šé“ç±»å‹åˆ†åˆ«ç»Ÿè®¡")
        print("   â€¢ åœ¨å…‰å­¦è´¨é‡è¿‡æ»¤æ—¶ä¿ç•™äº†SARæ•°æ®")

        return True

    except Exception as e:
        print(f"âŒ RGBè¿‡æ»¤ç»Ÿè®¡è®¡ç®—å‡ºé”™: {str(e)}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    if not success:
        sys.exit(1)
