"""
æ•°æ®æ¸…æ´—è„šæœ¬ - step3_clean_csv_data_01.py
åŸºäºquality assessmentç»“æœï¼Œç›´æ¥ä»CSVæ–‡ä»¶ä¸­ç§»é™¤ä½è´¨é‡æ•°æ®

éµå¾ªä¸‰ä¸ªåŸåˆ™ï¼š
1. æœ€å°æ”¹åŠ¨åŸåˆ™ï¼šç”Ÿæˆæ¸…æ´çš„CSVæ–‡ä»¶ï¼Œç®€åŒ–åç»­è®­ç»ƒæµç¨‹
2. å•ä¸€èŒè´£åŸåˆ™ï¼šä¸“æ³¨äºæ•°æ®æ¸…æ´—ï¼Œä¸æ¶‰åŠå…¶ä»–åŠŸèƒ½
3. æ¸è¿›å¢å¼ºåŸåˆ™ï¼šåŸºäºç°æœ‰quality assessmentæµç¨‹çš„è¾“å‡ºè¿›è¡Œæ¸…æ´—

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/step3_clean_csv_data_01.py \
        --exclude_json dataset/data_check/exclude_ids.json \
        --input_csv dataset/train.csv \
        --output_csv dataset/train_cleaned.csv
        
    # æˆ–ä½¿ç”¨é»˜è®¤è·¯å¾„
    python scripts/step3_clean_csv_data_01.py
"""

import os
import sys
import json
import argparse
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import List, Tuple, Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

try:
    from configs.config import Config

    HAVE_CONFIG = True
except ImportError:
    HAVE_CONFIG = False
    print("âš ï¸  Warning: configs.config not found, using manual paths")


class CSVDataCleaner:
    """
    CSVæ•°æ®æ¸…æ´—å™¨

    èŒè´£ï¼š
    - è¯»å–è´¨é‡è¯„ä¼°äº§ç”Ÿçš„æ’é™¤åˆ—è¡¨
    - ä»CSVæ–‡ä»¶ä¸­ç§»é™¤å¯¹åº”çš„è¡Œ
    - ç”Ÿæˆæ¸…æ´çš„CSVæ–‡ä»¶å¹¶è®°å½•æ¸…æ´—ä¿¡æ¯
    """

    def __init__(self):
        """åˆå§‹åŒ–æ¸…æ´—å™¨"""
        self.cleaning_stats = {
            "original_count": 0,
            "excluded_count": 0,
            "cleaned_count": 0,
            "excluded_by_class": {},
            "cleaned_by_class": {},
            "cleaning_timestamp": datetime.now().isoformat(),
        }

    def load_exclude_ids(self, exclude_json_path: Path) -> Tuple[List[str], Dict[str, Any]]:
        """
        åŠ è½½éœ€è¦æ’é™¤çš„æ ·æœ¬IDåˆ—è¡¨

        Args:
            exclude_json_path: æ’é™¤åˆ—è¡¨JSONæ–‡ä»¶è·¯å¾„

        Returns:
            Tuple[List[str], Dict]: (æ’é™¤çš„IDåˆ—è¡¨, æ’é™¤ä¿¡æ¯å…ƒæ•°æ®)
        """
        if not exclude_json_path.exists():
            raise FileNotFoundError(f"æ’é™¤åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {exclude_json_path}")

        print(f"ğŸ“– æ­£åœ¨åŠ è½½æ’é™¤åˆ—è¡¨: {exclude_json_path}")

        with open(exclude_json_path, "r", encoding="utf-8") as f:
            exclude_data = json.load(f)

        exclude_ids = exclude_data.get("excluded_image_ids", [])

        if not exclude_ids:
            print("âš ï¸  è­¦å‘Š: æ’é™¤åˆ—è¡¨ä¸ºç©º")
            return [], exclude_data

        print(f"ğŸ“‹ åŠ è½½äº† {len(exclude_ids)} ä¸ªéœ€è¦æ’é™¤çš„æ ·æœ¬ID")

        # æ‰“å°æ’é™¤ä¿¡æ¯
        if "threshold" in exclude_data:
            print(f"   ğŸ¯ æ’é™¤é˜ˆå€¼: {exclude_data['threshold']:.4f}")
        if "threshold_method" in exclude_data:
            print(f"   ğŸ“ æ’é™¤æ–¹æ³•: {exclude_data['threshold_method']}")
        if "excluded_percentage" in exclude_data:
            print(f"   ğŸ“Š æ’é™¤æ¯”ä¾‹: {exclude_data['excluded_percentage']:.1f}%")

        return exclude_ids, exclude_data

    def load_csv_data(self, csv_path: Path) -> pd.DataFrame:
        """
        åŠ è½½CSVæ•°æ®æ–‡ä»¶

        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„

        Returns:
            pd.DataFrame: åŠ è½½çš„æ•°æ®
        """
        if not csv_path.exists():
            raise FileNotFoundError(f"CSVæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")

        print(f"ğŸ“Š æ­£åœ¨åŠ è½½CSVæ•°æ®: {csv_path}")

        df = pd.read_csv(csv_path)

        print(f"   ğŸ“ˆ åŸå§‹æ•°æ®: {len(df)} è¡Œ")

        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_columns = ["ID"]
        missing_cols = [col for col in required_columns if col not in df.columns]
        if missing_cols:
            raise ValueError(f"CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}")

        # æ£€æŸ¥æ˜¯å¦æœ‰labelåˆ—ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
        has_labels = "label" in df.columns
        if has_labels:
            class_counts = df["label"].value_counts().sort_index()
            print(f"   ğŸ“‹ ç±»åˆ«åˆ†å¸ƒ: {dict(class_counts)}")
        else:
            print("   â„¹ï¸  æœªæ‰¾åˆ°labelåˆ—ï¼Œè·³è¿‡ç±»åˆ«ç»Ÿè®¡")

        self.cleaning_stats["original_count"] = len(df)
        if has_labels:
            self.cleaning_stats["original_by_class"] = df["label"].value_counts().to_dict()

        return df

    def clean_data(self, df: pd.DataFrame, exclude_ids: List[str]) -> pd.DataFrame:
        """
        æ¸…æ´—æ•°æ®ï¼Œç§»é™¤æ’é™¤åˆ—è¡¨ä¸­çš„æ ·æœ¬

        Args:
            df: åŸå§‹æ•°æ®DataFrame
            exclude_ids: éœ€è¦æ’é™¤çš„æ ·æœ¬IDåˆ—è¡¨

        Returns:
            pd.DataFrame: æ¸…æ´—åçš„æ•°æ®
        """
        print(f"\nğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")

        if not exclude_ids:
            print("   â„¹ï¸  æ’é™¤åˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›åŸå§‹æ•°æ®")
            return df.copy()

        # æ£€æŸ¥æœ‰å¤šå°‘éœ€è¦æ’é™¤çš„IDå®é™…å­˜åœ¨äºæ•°æ®ä¸­
        exclude_set = set(exclude_ids)
        existing_ids = set(df["ID"].values)
        actual_exclude_ids = exclude_set.intersection(existing_ids)
        missing_exclude_ids = exclude_set - existing_ids

        print(f"   ğŸ“‹ æ’é™¤åˆ—è¡¨ä¸­çš„ID: {len(exclude_ids)}")
        print(f"   âœ… å®é™…å­˜åœ¨çš„ID: {len(actual_exclude_ids)}")
        if missing_exclude_ids:
            print(f"   âš ï¸  ä¸å­˜åœ¨çš„ID: {len(missing_exclude_ids)}")
            if len(missing_exclude_ids) <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"      {list(missing_exclude_ids)[:5]}")

        # æ‰§è¡Œè¿‡æ»¤
        mask = ~df["ID"].isin(exclude_ids)
        cleaned_df = df[mask].copy().reset_index(drop=True)

        # ç»Ÿè®¡æ¸…æ´—ç»“æœ
        excluded_count = len(df) - len(cleaned_df)
        self.cleaning_stats["excluded_count"] = excluded_count
        self.cleaning_stats["cleaned_count"] = len(cleaned_df)

        print(f"   ğŸ“¤ ç§»é™¤æ ·æœ¬: {excluded_count}")
        print(f"   ğŸ“¥ ä¿ç•™æ ·æœ¬: {len(cleaned_df)}")
        print(f"   ğŸ“Š ä¿ç•™ç‡: {len(cleaned_df)/len(df)*100:.1f}%")

        # å¦‚æœæœ‰æ ‡ç­¾ï¼Œç»Ÿè®¡ç±»åˆ«ä¿¡æ¯
        if "label" in df.columns:
            excluded_df = df[~mask]
            excluded_class_counts = excluded_df["label"].value_counts().to_dict()
            cleaned_class_counts = cleaned_df["label"].value_counts().to_dict()

            self.cleaning_stats["excluded_by_class"] = excluded_class_counts
            self.cleaning_stats["cleaned_by_class"] = cleaned_class_counts

            print(f"   ğŸ“‹ ç§»é™¤çš„ç±»åˆ«åˆ†å¸ƒ: {excluded_class_counts}")
            print(f"   ğŸ“‹ ä¿ç•™çš„ç±»åˆ«åˆ†å¸ƒ: {cleaned_class_counts}")

        return cleaned_df

    def save_cleaned_data(self, cleaned_df: pd.DataFrame, output_path: Path, exclude_data: Dict[str, Any]) -> Path:
        """
        ä¿å­˜æ¸…æ´—åçš„æ•°æ®å’Œæ¸…æ´—æŠ¥å‘Š

        Args:
            cleaned_df: æ¸…æ´—åçš„æ•°æ®
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            exclude_data: æ’é™¤ä¿¡æ¯å…ƒæ•°æ®

        Returns:
            Path: æ¸…æ´—æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        print(f"\nğŸ’¾ ä¿å­˜æ¸…æ´—åçš„æ•°æ®...")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜æ¸…æ´—åçš„CSV
        cleaned_df.to_csv(output_path, index=False)
        print(f"   âœ… æ¸…æ´æ•°æ®å·²ä¿å­˜: {output_path}")

        return output_path

    def clean_csv_data(self, exclude_json_path: Path, input_csv_path: Path, output_csv_path: Path) -> Tuple[Path, Path]:
        """
        æ‰§è¡Œå®Œæ•´çš„CSVæ•°æ®æ¸…æ´—æµç¨‹

        Args:
            exclude_json_path: æ’é™¤åˆ—è¡¨JSONæ–‡ä»¶è·¯å¾„
            input_csv_path: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
            output_csv_path: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„

        Returns:
            Tuple[Path, Path]: (æ¸…æ´CSVæ–‡ä»¶è·¯å¾„, æ¸…æ´—æŠ¥å‘Šè·¯å¾„)
        """
        print("ğŸš€ å¼€å§‹CSVæ•°æ®æ¸…æ´—æµç¨‹")
        print("=" * 60)

        # æ­¥éª¤1: åŠ è½½æ’é™¤åˆ—è¡¨
        exclude_ids, exclude_data = self.load_exclude_ids(exclude_json_path)

        # æ­¥éª¤2: åŠ è½½CSVæ•°æ®
        df = self.load_csv_data(input_csv_path)

        # æ­¥éª¤3: æ¸…æ´—æ•°æ®
        cleaned_df = self.clean_data(df, exclude_ids)

        # æ­¥éª¤4: ä¿å­˜ç»“æœ
        self.save_cleaned_data(cleaned_df, output_csv_path, exclude_data)

        print("\nğŸ‰ CSVæ•°æ®æ¸…æ´—å®Œæˆ!")
        print(
            f"ğŸ“Š å¤„ç†æ‘˜è¦: {self.cleaning_stats['original_count']} â†’ {self.cleaning_stats['cleaned_count']} "
            f"(ç§»é™¤äº† {self.cleaning_stats['excluded_count']} ä¸ªæ ·æœ¬)"
        )

        return output_csv_path


def create_default_paths() -> Tuple[Path, Path, Path]:
    """
    åˆ›å»ºé»˜è®¤çš„æ–‡ä»¶è·¯å¾„ï¼ˆåŸºäºé¡¹ç›®é…ç½®æˆ–åˆç†æ¨æ–­ï¼‰

    Returns:
        Tuple[Path, Path, Path]: (exclude_json, input_csv, output_csv)
    """
    if HAVE_CONFIG:
        # ä½¿ç”¨é¡¹ç›®é…ç½®
        config = Config()
        exclude_json = config.DATASET_ROOT / "data_check" / "exclude_ids.json"
        input_csv = config.TRAIN_CSV
        output_csv = input_csv.parent / f"{input_csv.stem}_cleaned{input_csv.suffix}"
    else:
        # ä½¿ç”¨é»˜è®¤è·¯å¾„æ¨æ–­
        project_root = Path(__file__).parent.parent
        exclude_json = project_root / "dataset" / "data_check" / "exclude_ids.json"
        input_csv = project_root / "dataset" / "train.csv"
        output_csv = project_root / "dataset" / "train_cleaned.csv"

    return exclude_json, input_csv, output_csv


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="CSVæ•°æ®æ¸…æ´—å·¥å…· - ç§»é™¤ä½è´¨é‡æ ·æœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
    # ä½¿ç”¨é»˜è®¤è·¯å¾„
    python scripts/step3_clean_csv_data_01.py
    
    # æŒ‡å®šè‡ªå®šä¹‰è·¯å¾„
    python scripts/step3_clean_csv_data_01.py \
        --exclude_json dataset/data_check/exclude_ids.json \
        --input_csv dataset/list/Train.csv \
        --output_csv dataset/Train.csv
        """,
    )

    parser.add_argument("--exclude_json", type=Path, help="æ’é™¤åˆ—è¡¨JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--input_csv", type=Path, help="è¾“å…¥CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_csv", type=Path, help="è¾“å‡ºæ¸…æ´CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶è¦†ç›–è¾“å‡ºæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰")

    args = parser.parse_args()

    # ç¡®å®šæ–‡ä»¶è·¯å¾„
    if args.exclude_json and args.input_csv and args.output_csv:
        exclude_json = args.exclude_json
        input_csv = args.input_csv
        output_csv = args.output_csv
        print("ğŸ“‚ ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ–‡ä»¶è·¯å¾„")
    else:
        exclude_json, input_csv, output_csv = create_default_paths()
        print("ğŸ“‚ ä½¿ç”¨é»˜è®¤æ–‡ä»¶è·¯å¾„")

    print(f"   ğŸ” æ’é™¤åˆ—è¡¨: {exclude_json}")
    print(f"   ğŸ“Š è¾“å…¥CSV: {input_csv}")
    print(f"   ğŸ’¾ è¾“å‡ºCSV: {output_csv}")

    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if output_csv.exists() and not args.force:
        print(f"\nâš ï¸  è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨: {output_csv}")
        print("ä½¿ç”¨ --force å‚æ•°å¼ºåˆ¶è¦†ç›–ï¼Œæˆ–æŒ‡å®šä¸åŒçš„è¾“å‡ºè·¯å¾„")
        return False

    try:
        # æ‰§è¡Œæ¸…æ´—
        cleaner = CSVDataCleaner()
        cleaned_csv = cleaner.clean_csv_data(exclude_json, input_csv, output_csv)

        print(f"\nâœ… æ•°æ®æ¸…æ´—æˆåŠŸå®Œæˆ!")
        print(f"ğŸ“„ æ¸…æ´æ•°æ®: {cleaned_csv}")
        print(f"\nğŸ’¡ åç»­æ­¥éª¤:")
        print(f"   1. æ›´æ–°è®­ç»ƒé…ç½®ä¸­çš„CSVè·¯å¾„ä¸º: {cleaned_csv}")
        print(f"   2. ç§»é™¤æˆ–æ³¨é‡Šæ‰ exclude_ids_file å‚æ•°")
        print(f"   3. äº«å—ç®€åŒ–çš„è®­ç»ƒæµç¨‹! ğŸš€")

        return True

    except Exception as e:
        print(f"\nâŒ æ•°æ®æ¸…æ´—å¤±è´¥: {str(e)}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
