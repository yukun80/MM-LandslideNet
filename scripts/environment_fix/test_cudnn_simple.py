#!/usr/bin/env python3
"""
ç®€å•çš„cuDNNåŠŸèƒ½æµ‹è¯•è„šæœ¬
"""

import torch
import torch.nn as nn
import torch.backends.cudnn as cudnn
import sys


def test_cudnn():
    """æµ‹è¯•cuDNNåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•cuDNNåŠŸèƒ½...")
    print("=" * 40)

    # åŸºæœ¬ä¿¡æ¯
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"cuDNNç‰ˆæœ¬: {cudnn.version()}")
    print(f"cuDNNå¯ç”¨: {cudnn.enabled}")
    print(f"cuDNNç¡®å®šæ€§: {cudnn.deterministic}")

    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return False

    # è·å–GPUä¿¡æ¯
    print(f"GPUè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
    print(f"å½“å‰GPU: {torch.cuda.current_device()}")
    print(f"GPUåç§°: {torch.cuda.get_device_name()}")

    try:
        device = torch.device("cuda")

        # æµ‹è¯•1: ç®€å•å·ç§¯
        print("\nğŸ” æµ‹è¯•1: ç®€å•å·ç§¯æ“ä½œ")
        x = torch.randn(1, 3, 224, 224, device=device)
        conv = nn.Conv2d(3, 64, 3, padding=1).to(device)

        with torch.no_grad():
            y = conv(x)
            print(f"âœ… å·ç§¯æˆåŠŸ: {x.shape} -> {y.shape}")

        # æµ‹è¯•2: åå‘ä¼ æ’­
        print("\nğŸ” æµ‹è¯•2: åå‘ä¼ æ’­")
        x.requires_grad_(True)
        y = conv(x)
        loss = y.sum()
        loss.backward()
        print(f"âœ… åå‘ä¼ æ’­æˆåŠŸ: æ¢¯åº¦å½¢çŠ¶ {x.grad.shape}")

        # æµ‹è¯•3: æ‰¹é‡å½’ä¸€åŒ–
        print("\nğŸ” æµ‹è¯•3: æ‰¹é‡å½’ä¸€åŒ–")
        bn = nn.BatchNorm2d(64).to(device)
        y_bn = bn(y)
        print(f"âœ… æ‰¹é‡å½’ä¸€åŒ–æˆåŠŸ: {y.shape} -> {y_bn.shape}")

        # æµ‹è¯•4: æ¿€æ´»å‡½æ•°
        print("\nğŸ” æµ‹è¯•4: æ¿€æ´»å‡½æ•°")
        relu = nn.ReLU().to(device)
        y_relu = relu(y_bn)
        print(f"âœ… æ¿€æ´»å‡½æ•°æˆåŠŸ: {y_bn.shape} -> {y_relu.shape}")

        # æµ‹è¯•5: æ± åŒ–æ“ä½œ
        print("\nğŸ” æµ‹è¯•5: æ± åŒ–æ“ä½œ")
        pool = nn.MaxPool2d(2, 2).to(device)
        y_pool = pool(y_relu)
        print(f"âœ… æ± åŒ–æˆåŠŸ: {y_relu.shape} -> {y_pool.shape}")

        # æµ‹è¯•6: å†…å­˜ä½¿ç”¨
        print("\nğŸ” æµ‹è¯•6: GPUå†…å­˜ä½¿ç”¨")
        memory_allocated = torch.cuda.memory_allocated(device) / 1024**2
        memory_cached = torch.cuda.memory_reserved(device) / 1024**2
        print(f"âœ… GPUå†…å­˜ - å·²åˆ†é…: {memory_allocated:.1f}MB, å·²ç¼“å­˜: {memory_cached:.1f}MB")

        return True

    except Exception as e:
        print(f"âŒ cuDNNæµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_cudnn()

    if success:
        print("\nğŸ‰ cuDNNåŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        print("ç¯å¢ƒé…ç½®æˆåŠŸï¼Œå¯ä»¥è¿è¡Œè®­ç»ƒè„šæœ¬")
    else:
        print("\nâŒ cuDNNåŠŸèƒ½æµ‹è¯•å¤±è´¥")
        print("è¯·æ£€æŸ¥CUDAå’ŒcuDNNé…ç½®")
        sys.exit(1)
