"""
MM-LandslideNet ç»Ÿä¸€é¡¹ç›®å…¥å£ç‚¹

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ¨¡å‹è®­ç»ƒ (train)
2. æ¨¡å‹æµ‹è¯• (test)
3. æ¨¡å‹æ¨ç† (predict)
4. æ€§èƒ½è¯„ä¼° (evaluate)
5. æ•°æ®åˆ†æ (analyze)
6. æ¨¡å‹è½¬æ¢ (convert)

ä½¿ç”¨ç¤ºä¾‹ï¼š
1. è®­ç»ƒæ¨¡å‹ï¼špython main.py train --config-path configs/experiment --config-name optical_baseline
2. æµ‹è¯•æ¨¡å‹ï¼špython main.py test --checkpoint path/to/model.ckpt
3. æ‰¹é‡æ¨ç†ï¼špython main.py predict --checkpoint path/to/model.ckpt --input-dir test_data/
4. å¿«é€Ÿå¼€å§‹ï¼špython main.py train --preset quick_test

è®¾è®¡å“²å­¦ï¼š
"ä¸€ä¸ªå…¥å£ï¼Œå¤šç§å¯èƒ½" - é€šè¿‡ç»Ÿä¸€çš„æ¥å£ï¼Œè®©å¤æ‚çš„æ·±åº¦å­¦ä¹ å·¥ä½œæµå˜å¾—ç®€å•æ˜“ç”¨ã€‚
"""

import sys
import argparse
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# æŠ‘åˆ¶ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", ".*does not have many workers.*")
warnings.filterwarnings("ignore", ".*The dataloader.*")

import torch
import pytorch_lightning as pl
from omegaconf import DictConfig, OmegaConf
import hydra
from hydra import compose, initialize_config_dir

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from lightning_landslide.src.utils.logging_utils import setup_logging, get_project_logger

logger = get_project_logger(__name__)


class TaskRunner:
    """
    ä»»åŠ¡æ‰§è¡Œå™¨åŸºç±»

    è¿™ä¸ªç±»å®šä¹‰äº†æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå™¨çš„é€šç”¨æ¥å£ã€‚æ¯ç§å…·ä½“çš„ä»»åŠ¡
    ï¼ˆè®­ç»ƒã€æµ‹è¯•ã€æ¨ç†ç­‰ï¼‰éƒ½ä¼šç»§æ‰¿è¿™ä¸ªåŸºç±»ï¼Œå®ç°ç»Ÿä¸€çš„
    æ‰§è¡Œæ¨¡å¼ã€‚

    è¿™ç§è®¾è®¡è®©main.pyèƒ½å¤Ÿä»¥ç›¸åŒçš„æ–¹å¼å¤„ç†ä¸åŒç±»å‹çš„ä»»åŠ¡ï¼Œ
    åŒæ—¶ä¸ºæ¯ç§ä»»åŠ¡æä¾›äº†è¶³å¤Ÿçš„å®šåˆ¶ç©ºé—´ã€‚
    """

    def __init__(self, args: argparse.Namespace):
        self.args = args
        self.setup_environment()

    def setup_environment(self):
        """è®¾ç½®æ‰§è¡Œç¯å¢ƒ"""
        # è®¾ç½®æ—¥å¿—
        log_level = logging.DEBUG if self.args.verbose else logging.INFO
        setup_logging(level=log_level)

        # è®¾ç½®éšæœºç§å­ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if hasattr(self.args, "seed") and self.args.seed is not None:
            pl.seed_everything(self.args.seed, workers=True)
            logger.info(f"Set random seed to {self.args.seed}")

    def run(self) -> Dict[str, Any]:
        """æ‰§è¡Œä»»åŠ¡çš„ä¸»æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°"""
        raise NotImplementedError("Subclasses must implement run method")

    def load_config(self, config_path: str = None, config_name: str = None) -> DictConfig:
        """
        åŠ è½½é…ç½®æ–‡ä»¶

        è¿™ä¸ªæ–¹æ³•æä¾›äº†çµæ´»çš„é…ç½®åŠ è½½æœºåˆ¶ï¼Œæ”¯æŒå¤šç§é…ç½®æ¥æºï¼š
        1. å‘½ä»¤è¡ŒæŒ‡å®šçš„é…ç½®æ–‡ä»¶
        2. é¢„è®¾çš„é…ç½®æ¨¡æ¿
        3. æ£€æŸ¥ç‚¹ä¸­ä¿å­˜çš„é…ç½®
        """
        if config_path and config_name:
            # ä»æŒ‡å®šè·¯å¾„åŠ è½½é…ç½®
            with initialize_config_dir(config_dir=str(Path(config_path).absolute())):
                cfg = compose(config_name=config_name)
        elif hasattr(self.args, "preset") and self.args.preset:
            # ä½¿ç”¨é¢„è®¾é…ç½®
            cfg = self._load_preset_config(self.args.preset)
        else:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            cfg = self._load_default_config()

        # åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–
        if hasattr(self.args, "overrides") and self.args.overrides:
            for override in self.args.overrides:
                self._apply_override(cfg, override)

        return cfg

    def _load_preset_config(self, preset_name: str) -> DictConfig:
        """åŠ è½½é¢„è®¾é…ç½®"""
        preset_configs = {
            "quick_test": {
                "experiment": {"name": "quick_test", "description": "Quick test run"},
                "model": {"type": "optical_swin", "backbone_name": "swin_tiny_patch4_window7_224"},
                "data": {"batch_size": 16, "val_split": 0.3},
                "training": {"max_epochs": 10, "optimizer": {"lr": 2e-4}},
                "compute": {"precision": "16-mixed"},
            },
            "full_multimodal": {
                "experiment": {"name": "full_multimodal", "description": "Full 13-channel training"},
                "model": {"type": "optical_swin", "input_channels": 13},
                "data": {"usage_mode": "full_multimodal", "batch_size": 32},
                "training": {"max_epochs": 50},
            },
            "high_performance": {
                "experiment": {"name": "high_performance", "description": "High-performance training"},
                "model": {"type": "optical_swin", "backbone_name": "swin_base_patch4_window7_224"},
                "data": {"batch_size": 64, "use_weighted_sampling": True},
                "training": {"max_epochs": 100, "optimizer": {"layer_wise_lr": True}},
                "compute": {"precision": "16-mixed", "accumulate_grad_batches": 2},
            },
        }

        if preset_name not in preset_configs:
            raise ValueError(f"Unknown preset: {preset_name}. Available: {list(preset_configs.keys())}")

        return OmegaConf.create(preset_configs[preset_name])

    def _load_default_config(self) -> DictConfig:
        """åŠ è½½é»˜è®¤é…ç½®"""
        default_config_path = project_root / "configs" / "experiment" / "optical_baseline.yaml"
        if default_config_path.exists():
            return OmegaConf.load(default_config_path)
        else:
            # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›æœ€å°é…ç½®
            logger.warning("Default config not found, using minimal configuration")
            return self._load_preset_config("quick_test")

    def _apply_override(self, cfg: DictConfig, override: str):
        """åº”ç”¨é…ç½®è¦†ç›–"""
        try:
            key, value = override.split("=", 1)
            # å°è¯•å°†å€¼è½¬æ¢ä¸ºé€‚å½“çš„ç±»å‹
            try:
                value = eval(value)  # å°è¯•è§£æä¸ºPythonå¯¹è±¡
            except:
                pass  # ä¿æŒä¸ºå­—ç¬¦ä¸²

            OmegaConf.set(cfg, key, value)
            logger.info(f"Applied override: {key}={value}")
        except Exception as e:
            logger.warning(f"Failed to apply override '{override}': {e}")


class TrainTaskRunner(TaskRunner):
    """
    è®­ç»ƒä»»åŠ¡æ‰§è¡Œå™¨

    è¿™ä¸ªç±»ä¸“é—¨å¤„ç†æ¨¡å‹è®­ç»ƒä»»åŠ¡ã€‚å®ƒæ•´åˆäº†æˆ‘ä»¬ä¹‹å‰æ„å»ºçš„
    è®­ç»ƒæ¡†æ¶ï¼ŒåŒæ—¶æä¾›äº†æ›´çµæ´»çš„é…ç½®å’Œæ‰§è¡Œé€‰é¡¹ã€‚
    """

    def run(self) -> Dict[str, Any]:
        """æ‰§è¡Œè®­ç»ƒä»»åŠ¡"""
        logger.info("ğŸš€ Starting training task")

        # åŠ è½½é…ç½®
        cfg = self.load_config(
            config_path=getattr(self.args, "config_path", None), config_name=getattr(self.args, "config_name", None)
        )

        # å¯¼å…¥è®­ç»ƒæ¨¡å—ï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…ä¸å¿…è¦çš„ä¾èµ–ï¼‰
        from lightning_landslide.src.models import LandslideClassificationModule
        from lightning_landslide.src.data import MultiModalDataModule
        from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
        from pytorch_lightning.loggers import TensorBoardLogger

        # åˆ›å»ºæ•°æ®æ¨¡å—
        logger.info("Creating data module...")
        data_module = MultiModalDataModule(cfg)

        # åˆ›å»ºæ¨¡å‹
        logger.info("Creating model...")
        model = LandslideClassificationModule(cfg)

        # åˆ›å»ºå›è°ƒå‡½æ•°
        callbacks = self._create_callbacks(cfg)

        # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
        loggers = self._create_loggers(cfg)

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = pl.Trainer(
            max_epochs=cfg.training.max_epochs,
            accelerator=cfg.compute.accelerator,
            devices=cfg.compute.devices,
            precision=cfg.compute.precision,
            callbacks=callbacks,
            logger=loggers,
            deterministic=cfg.reproducibility.deterministic,
            log_every_n_steps=cfg.logging.log_every_n_steps,
        )

        # å¼€å§‹è®­ç»ƒ
        trainer.fit(model, data_module)

        # è¿”å›è®­ç»ƒç»“æœ
        return {
            "status": "success",
            "best_model_path": trainer.checkpoint_callback.best_model_path,
            "best_model_score": trainer.checkpoint_callback.best_model_score.item(),
            "final_epoch": trainer.current_epoch,
        }

    def _create_callbacks(self, cfg: DictConfig) -> List[pl.Callback]:
        """åˆ›å»ºè®­ç»ƒå›è°ƒå‡½æ•°"""
        callbacks = []

        # æ¨¡å‹æ£€æŸ¥ç‚¹
        checkpoint_callback = ModelCheckpoint(
            monitor=cfg.callbacks.model_checkpoint.monitor,
            mode=cfg.callbacks.model_checkpoint.mode,
            save_top_k=cfg.callbacks.model_checkpoint.save_top_k,
            filename=cfg.callbacks.model_checkpoint.filename,
            save_last=True,
        )
        callbacks.append(checkpoint_callback)

        # æ—©åœ
        if cfg.callbacks.early_stopping.enable:
            early_stopping = EarlyStopping(
                monitor=cfg.callbacks.early_stopping.monitor,
                patience=cfg.callbacks.early_stopping.patience,
                mode=cfg.callbacks.early_stopping.mode,
            )
            callbacks.append(early_stopping)

        # å­¦ä¹ ç‡ç›‘æ§
        lr_monitor = LearningRateMonitor(logging_interval="epoch")
        callbacks.append(lr_monitor)

        return callbacks

    def _create_loggers(self, cfg: DictConfig) -> List[pl.LightningLoggerBase]:
        """åˆ›å»ºæ—¥å¿—è®°å½•å™¨"""
        loggers = []

        # TensorBoardæ—¥å¿—
        if cfg.logging.tensorboard.enable:
            tb_logger = TensorBoardLogger(
                save_dir=cfg.logging.save_dir, name=cfg.logging.name, version=cfg.experiment.name
            )
            loggers.append(tb_logger)

        return loggers


class TestTaskRunner(TaskRunner):
    """
    æµ‹è¯•ä»»åŠ¡æ‰§è¡Œå™¨

    è´Ÿè´£å¯¹è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•è¯„ä¼°ã€‚
    """

    def run(self) -> Dict[str, Any]:
        """æ‰§è¡Œæµ‹è¯•ä»»åŠ¡"""
        logger.info("ğŸ§ª Starting test task")

        if not hasattr(self.args, "checkpoint") or not self.args.checkpoint:
            raise ValueError("Test task requires --checkpoint argument")

        checkpoint_path = Path(self.args.checkpoint)
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

        # ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹å’Œé…ç½®
        from src.models import LandslideClassificationModule
        from src.data import MultiModalDataModule

        logger.info(f"Loading model from {checkpoint_path}")
        model = LandslideClassificationModule.load_from_checkpoint(checkpoint_path)

        # åˆ›å»ºæ•°æ®æ¨¡å—ï¼ˆä½¿ç”¨ä¿å­˜çš„é…ç½®æˆ–æ–°é…ç½®ï¼‰
        if hasattr(self.args, "config_path") and self.args.config_path:
            cfg = self.load_config(self.args.config_path, self.args.config_name)
        else:
            # å°è¯•ä»æ£€æŸ¥ç‚¹ä¸­æ¢å¤é…ç½®
            cfg = self._extract_config_from_checkpoint(checkpoint_path)

        data_module = MultiModalDataModule(cfg)

        # åˆ›å»ºæµ‹è¯•å™¨
        trainer = pl.Trainer(
            accelerator="gpu" if torch.cuda.is_available() else "cpu", devices=1, logger=False  # æµ‹è¯•æ—¶ä¸éœ€è¦æ—¥å¿—
        )

        # æ‰§è¡Œæµ‹è¯•
        test_results = trainer.test(model, data_module)

        logger.info("âœ… Test completed")
        return {"status": "success", "test_results": test_results, "checkpoint_path": str(checkpoint_path)}

    def _extract_config_from_checkpoint(self, checkpoint_path: Path) -> DictConfig:
        """ä»æ£€æŸ¥ç‚¹ä¸­æå–é…ç½®ä¿¡æ¯"""
        checkpoint = torch.load(checkpoint_path, map_location="cpu")

        if "hyper_parameters" in checkpoint:
            # Lightningè‡ªåŠ¨ä¿å­˜çš„è¶…å‚æ•°
            return OmegaConf.create(checkpoint["hyper_parameters"])
        else:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            logger.warning("No configuration found in checkpoint, using default")
            return self._load_default_config()


class PredictTaskRunner(TaskRunner):
    """
    æ¨ç†ä»»åŠ¡æ‰§è¡Œå™¨

    å¤„ç†æ‰¹é‡æ¨ç†ä»»åŠ¡ï¼Œç”Ÿæˆé¢„æµ‹ç»“æœã€‚
    """

    def run(self) -> Dict[str, Any]:
        """æ‰§è¡Œæ¨ç†ä»»åŠ¡"""
        logger.info("ğŸ”® Starting prediction task")

        if not hasattr(self.args, "checkpoint") or not self.args.checkpoint:
            raise ValueError("Predict task requires --checkpoint argument")

        if not hasattr(self.args, "input_dir") or not self.args.input_dir:
            raise ValueError("Predict task requires --input-dir argument")

        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from src.models import LandslideClassificationModule

        # åŠ è½½æ¨¡å‹
        logger.info(f"Loading model from {self.args.checkpoint}")
        model = LandslideClassificationModule.load_from_checkpoint(self.args.checkpoint)
        model.eval()

        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = Path(getattr(self.args, "output_dir", "predictions"))
        output_dir.mkdir(parents=True, exist_ok=True)

        # æ‰§è¡Œæ‰¹é‡æ¨ç†
        predictions = self._run_batch_inference(model, self.args.input_dir, output_dir)

        logger.info(f"âœ… Prediction completed. Results saved to {output_dir}")
        return {"status": "success", "predictions_count": len(predictions), "output_dir": str(output_dir)}

    def _run_batch_inference(self, model, input_dir: str, output_dir: Path) -> List[Dict]:
        """æ‰§è¡Œæ‰¹é‡æ¨ç†"""
        input_path = Path(input_dir)
        predictions = []

        # è·å–æ‰€æœ‰è¾“å…¥æ–‡ä»¶
        data_files = list(input_path.glob("*.npy"))
        logger.info(f"Found {len(data_files)} files for prediction")

        # é€æ–‡ä»¶è¿›è¡Œæ¨ç†
        for data_file in data_files:
            try:
                # åŠ è½½æ•°æ®
                data = torch.from_numpy(np.load(data_file)).float().unsqueeze(0)

                # æ‰§è¡Œæ¨ç†
                with torch.no_grad():
                    logits = model(data)
                    prob = torch.sigmoid(logits).item()
                    pred = int(prob > 0.5)

                # è®°å½•ç»“æœ
                result = {"file_id": data_file.stem, "probability": prob, "prediction": pred}
                predictions.append(result)

            except Exception as e:
                logger.error(f"Failed to process {data_file}: {e}")

        # ä¿å­˜ç»“æœ
        self._save_predictions(predictions, output_dir)
        return predictions

    def _save_predictions(self, predictions: List[Dict], output_dir: Path):
        """ä¿å­˜é¢„æµ‹ç»“æœ"""
        import pandas as pd

        # åˆ›å»ºDataFrame
        df = pd.DataFrame(predictions)

        # ä¿å­˜ä¸ºCSV
        csv_path = output_dir / "predictions.csv"
        df.to_csv(csv_path, index=False)

        # åˆ›å»ºæäº¤æ ¼å¼æ–‡ä»¶
        submission_df = df[["file_id", "prediction"]].copy()
        submission_df.columns = ["ID", "label"]
        submission_path = output_dir / "submission.csv"
        submission_df.to_csv(submission_path, index=False)

        logger.info(f"Saved predictions to {csv_path}")
        logger.info(f"Saved submission format to {submission_path}")


def create_parser() -> argparse.ArgumentParser:
    """
    åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨

    è¿™ä¸ªå‡½æ•°å®šä¹‰äº†ç»Ÿä¸€å…¥å£ç‚¹çš„å®Œæ•´å‘½ä»¤è¡Œæ¥å£ã€‚
    è®¾è®¡ä¸Šå‚è€ƒäº†gitç­‰æˆåŠŸå·¥å…·çš„å­å‘½ä»¤æ¨¡å¼ã€‚
    """
    parser = argparse.ArgumentParser(
        description="MM-LandslideNet: Multi-modal Landslide Detection Framework",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # åŸºç¡€è®­ç»ƒ
  python main.py train --config-path configs/experiment --config-name optical_baseline
  
  # ä½¿ç”¨é¢„è®¾å¿«é€Ÿè®­ç»ƒ
  python main.py train --preset quick_test
  
  # è®­ç»ƒæ—¶è¦†ç›–å‚æ•°
  python main.py train --preset quick_test --override training.max_epochs=20 --override data.batch_size=32
  
  # æµ‹è¯•æ¨¡å‹
  python main.py test --checkpoint experiments/optical_baseline/checkpoints/best.ckpt
  
  # æ‰¹é‡æ¨ç†
  python main.py predict --checkpoint best.ckpt --input-dir test_data/ --output-dir results/
  
  # å¿«é€Ÿå¸®åŠ©
  python main.py --help
  python main.py train --help
        """,
    )

    # å…¨å±€å‚æ•°
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose logging")
    parser.add_argument("--seed", type=int, help="Random seed for reproducibility")

    # åˆ›å»ºå­å‘½ä»¤
    subparsers = parser.add_subparsers(dest="task", help="Task to execute")

    # è®­ç»ƒå­å‘½ä»¤
    train_parser = subparsers.add_parser("train", help="Train a model")
    train_parser.add_argument("--config-path", type=str, help="Path to config directory")
    train_parser.add_argument("--config-name", type=str, help="Config file name (without .yaml)")
    train_parser.add_argument(
        "--preset",
        type=str,
        choices=["quick_test", "full_multimodal", "high_performance"],
        help="Use predefined configuration preset",
    )
    train_parser.add_argument(
        "--override", action="append", dest="overrides", help="Override config parameters (e.g., training.lr=0.01)"
    )

    # æµ‹è¯•å­å‘½ä»¤
    test_parser = subparsers.add_parser("test", help="Test a trained model")
    test_parser.add_argument("--checkpoint", required=True, help="Path to model checkpoint")
    test_parser.add_argument("--config-path", type=str, help="Override config path")
    test_parser.add_argument("--config-name", type=str, help="Override config name")

    # æ¨ç†å­å‘½ä»¤
    predict_parser = subparsers.add_parser("predict", help="Run batch prediction")
    predict_parser.add_argument("--checkpoint", required=True, help="Path to model checkpoint")
    predict_parser.add_argument("--input-dir", required=True, help="Directory containing input data")
    predict_parser.add_argument("--output-dir", default="predictions", help="Output directory for results")

    return parser


def main():
    """
    ä¸»å‡½æ•°ï¼šé¡¹ç›®çš„ç»Ÿä¸€å…¥å£ç‚¹

    è¿™ä¸ªå‡½æ•°å®ç°äº†æ•´ä¸ªæ¡†æ¶çš„æ ¸å¿ƒè°ƒåº¦é€»è¾‘ã€‚å®ƒè§£æç”¨æˆ·çš„å‘½ä»¤ï¼Œ
    åˆ›å»ºç›¸åº”çš„ä»»åŠ¡æ‰§è¡Œå™¨ï¼Œç„¶åè¿è¡Œä»»åŠ¡å¹¶å¤„ç†ç»“æœã€‚
    """
    parser = create_parser()
    args = parser.parse_args()

    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»åŠ¡ï¼Œæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    if args.task is None:
        parser.print_help()
        sys.exit(1)

    # åˆ›å»ºä»»åŠ¡æ‰§è¡Œå™¨
    task_runners = {"train": TrainTaskRunner, "test": TestTaskRunner, "predict": PredictTaskRunner}

    if args.task not in task_runners:
        logger.error(f"Unknown task: {args.task}")
        sys.exit(1)

    # æ‰§è¡Œä»»åŠ¡
    try:
        runner = task_runners[args.task](args)
        results = runner.run()

        # æ˜¾ç¤ºæ‰§è¡Œç»“æœ
        print("\n" + "=" * 60)
        print(f"ğŸ‰ Task '{args.task}' completed successfully!")
        print("=" * 60)

        for key, value in results.items():
            if key != "status":
                print(f"{key}: {value}")

        print("=" * 60 + "\n")

    except Exception as e:
        logger.error(f"Task '{args.task}' failed: {str(e)}")
        if args.verbose:
            import traceback

            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
