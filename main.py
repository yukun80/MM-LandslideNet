#!/usr/bin/env python3
"""
MM-LandslideNet ç»Ÿä¸€é¡¹ç›®å…¥å£ç‚¹ (é‡æ„ç‰ˆ)

è¿™æ˜¯å‚è€ƒlatent-diffusionè®¾è®¡çš„æ–°ç‰ˆæœ¬å…¥å£ç‚¹ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯"é…ç½®é©±åŠ¨ä¸€åˆ‡"ï¼š
- æ‰€æœ‰ç»„ä»¶éƒ½é€šè¿‡é…ç½®æ–‡ä»¶åˆ›å»º
- æ”¯æŒå¤šç§ä»»åŠ¡ï¼ˆè®­ç»ƒã€æµ‹è¯•ã€æ¨ç†ç­‰ï¼‰
- æç®€çš„ä»£ç é€»è¾‘ï¼Œæœ€å¤§çš„çµæ´»æ€§

è®¾è®¡å“²å­¦ï¼š
"è®©é…ç½®æ–‡ä»¶æˆä¸ºå”¯ä¸€çš„å˜åŒ–ç‚¹" - æ·»åŠ æ–°æ¨¡å‹ã€æ–°æ•°æ®é›†æˆ–æ–°è®­ç»ƒç­–ç•¥æ—¶ï¼Œ
åªéœ€è¦ç¼–å†™é…ç½®æ–‡ä»¶ï¼Œæ— éœ€ä¿®æ”¹ä»»ä½•Pythonä»£ç ã€‚

æ•™å­¦è¦ç‚¹ï¼š
å¯¹æ¯”æ‚¨åŸæ¥çš„main.pyï¼Œæ–°ç‰ˆæœ¬çš„æ ¸å¿ƒæ”¹è¿›æ˜¯ç”¨"é…ç½®é©±åŠ¨"æ›¿ä»£äº†"ä»£ç é©±åŠ¨"ã€‚
è¿™ç§è®¾è®¡è®©æ¡†æ¶å…·å¤‡äº†ç±»ä¼¼latent-diffusionçš„å¼ºå¤§çµæ´»æ€§ã€‚
"""

import os
import sys
import argparse
import logging
from pathlib import Path
from typing import Dict, Any, Optional
import warnings
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# æŠ‘åˆ¶ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", ".*does not have many workers.*")
warnings.filterwarnings("ignore", ".*The dataloader.*")

import torch
import pytorch_lightning as pl
from omegaconf import DictConfig, OmegaConf

# å¯¼å…¥æˆ‘ä»¬çš„æ ¸å¿ƒå·¥å…·
from lightning_landslide.src.utils.instantiate import instantiate_from_config, validate_config_structure
from lightning_landslide.src.utils.logging_utils import setup_logging, get_project_logger

logger = get_project_logger(__name__)


class ExperimentRunner:
    """
    å®éªŒè¿è¡Œå™¨

    è¿™ä¸ªç±»è´Ÿè´£åè°ƒæ•´ä¸ªå®éªŒçš„æ‰§è¡Œæµç¨‹ã€‚å®ƒå°±åƒä¸€ä¸ªæŒ‡æŒ¥å®¶ï¼Œ
    æ ¹æ®é…ç½®æ–‡ä»¶çš„"ä¹è°±"æ¥æŒ‡æŒ¥å„ä¸ªç»„ä»¶ååŒå·¥ä½œã€‚

    ä¸æ‚¨åŸæ¥çš„TaskRunnerç›¸æ¯”ï¼Œè¿™ä¸ªç‰ˆæœ¬æ›´åŠ ä¸“æ³¨å’Œç®€åŒ–ï¼š
    - åªæœ‰ä¸€ä¸ªæ ¸å¿ƒèŒè´£ï¼šè¿è¡Œå®éªŒ
    - æ‰€æœ‰çš„å¤æ‚æ€§éƒ½å°è£…åœ¨é…ç½®æ–‡ä»¶ä¸­
    - ä»£ç é€»è¾‘å˜å¾—æå…¶ç®€æ´
    """

    def __init__(self, config_path: str, task: str = "train"):
        """
        åˆå§‹åŒ–å®éªŒè¿è¡Œå™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            task: è¦æ‰§è¡Œçš„ä»»åŠ¡ç±»å‹ï¼ˆtrain/test/predictç­‰ï¼‰
        """
        self.config_path = Path(config_path)
        self.task = task
        self.config = self._load_config()
        self._setup_environment()

    def _load_config(self) -> DictConfig:
        """
        åŠ è½½å’ŒéªŒè¯é…ç½®æ–‡ä»¶

        è¿™é‡Œæˆ‘ä»¬åšä¸¤ä»¶äº‹ï¼š
        1. åŠ è½½YAMLé…ç½®æ–‡ä»¶
        2. éªŒè¯é…ç½®çš„åŸºæœ¬ç»“æ„
        """
        if not self.config_path.exists():
            raise FileNotFoundError(f"Config file not found: {self.config_path}")

        logger.info(f"Loading config from: {self.config_path}")
        config = OmegaConf.load(self.config_path)

        # éªŒè¯é…ç½®ç»“æ„
        if not validate_config_structure(config):
            raise ValueError("Invalid configuration structure")

        logger.info("âœ“ Configuration loaded and validated")
        return config

    def _setup_environment(self):
        """
        è®¾ç½®å®éªŒç¯å¢ƒ

        åŒ…æ‹¬æ—¥å¿—ã€éšæœºç§å­ã€è¾“å‡ºç›®å½•ç­‰åŸºç¡€è®¾æ–½ã€‚
        """
        # è®¾ç½®æ—¥å¿—
        log_level = getattr(logging, self.config.get("log_level", "INFO").upper())
        setup_logging(level=log_level)

        # è®¾ç½®éšæœºç§å­
        if "seed" in self.config:
            pl.seed_everything(self.config.seed, workers=True)
            logger.info(f"Set random seed to {self.config.seed}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self._create_output_dirs()

        # ä¿å­˜é…ç½®æ–‡ä»¶åˆ°å®éªŒç›®å½•
        self._save_config()

    def _create_output_dirs(self):
        """åˆ›å»ºå®éªŒéœ€è¦çš„è¾“å‡ºç›®å½•"""
        if "outputs" in self.config:
            for dir_name, dir_path in self.config.outputs.items():
                Path(dir_path).mkdir(parents=True, exist_ok=True)
                logger.debug(f"Created directory: {dir_path}")

    def _save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶åˆ°å®éªŒç›®å½•ï¼ˆç¡®ä¿å¯é‡ç°æ€§ï¼‰"""
        if "outputs" in self.config and "log_dir" in self.config.outputs:
            config_save_path = Path(self.config.outputs.log_dir) / "config.yaml"
            config_save_path.parent.mkdir(parents=True, exist_ok=True)

            with open(config_save_path, "w") as f:
                OmegaConf.save(self.config, f)

            logger.info(f"Config saved to: {config_save_path}")

    def run(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®éªŒçš„ä¸»æ–¹æ³•

        è¿™æ˜¯æ•´ä¸ªæ¡†æ¶çš„æ ¸å¿ƒã€‚å®ƒæ ¹æ®ä»»åŠ¡ç±»å‹è°ƒç”¨ç›¸åº”çš„æ‰§è¡Œæ–¹æ³•ã€‚
        æ³¨æ„è¿™é‡Œçš„ä»£ç æœ‰å¤šä¹ˆç®€æ´ - æ‰€æœ‰çš„å¤æ‚æ€§éƒ½è¢«é…ç½®æ–‡ä»¶å¸æ”¶äº†ã€‚

        Returns:
            å®éªŒç»“æœå­—å…¸
        """
        logger.info(f"ğŸš€ Starting {self.task} task")
        self._print_experiment_info()

        # æ ¹æ®ä»»åŠ¡ç±»å‹åˆ†å‘åˆ°ä¸åŒçš„æ‰§è¡Œæ–¹æ³•
        task_methods = {
            "train": self._run_training,
            "test": self._run_testing,
            "predict": self._run_prediction,
            "validate": self._run_validation,
        }

        if self.task not in task_methods:
            raise ValueError(f"Unknown task: {self.task}. Available: {list(task_methods.keys())}")

        return task_methods[self.task]()

    def _run_training(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œè®­ç»ƒä»»åŠ¡

        è¿™æ˜¯æ•´ä¸ªé‡æ„çš„æ ¸å¿ƒæˆæœå±•ç¤ºã€‚çœ‹çœ‹è¿™ä¸ªæ–¹æ³•æœ‰å¤šä¹ˆç®€æ´ï¼š
        - ä»é…ç½®åˆ›å»ºæ¨¡å‹ï¼šä¸€è¡Œä»£ç 
        - ä»é…ç½®åˆ›å»ºæ•°æ®ï¼šä¸€è¡Œä»£ç 
        - ä»é…ç½®åˆ›å»ºè®­ç»ƒå™¨ï¼šä¸€è¡Œä»£ç 
        - å¼€å§‹è®­ç»ƒï¼šä¸€è¡Œä»£ç 

        è¿™å°±æ˜¯"é…ç½®é©±åŠ¨"è®¾è®¡çš„å¨åŠ›ï¼
        """
        logger.info("Initializing training components...")

        # ğŸ¯ æ ¸å¿ƒæ”¹è¿›ï¼šç”¨é…ç½®åˆ›å»ºæ‰€æœ‰ç»„ä»¶
        # ä¸å†éœ€è¦å¤æ‚çš„å·¥å‚ç±»æˆ–if-elseåˆ¤æ–­
        model = instantiate_from_config(self.config.model)
        data_module = instantiate_from_config(self.config.data)

        # å¤„ç†traineré…ç½®ï¼ˆå¯èƒ½åŒ…å«callbackså’Œloggersï¼‰
        trainer_config = self.config.trainer.copy()

        # åˆ›å»ºcallbacksï¼ˆå¦‚æœé…ç½®ä¸­æœ‰çš„è¯ï¼‰
        if "callbacks" in self.config:
            callbacks = []
            for callback_name, callback_config in self.config.callbacks.items():
                callback = instantiate_from_config(callback_config)
                callbacks.append(callback)
                logger.info(f"Added callback: {callback_name}")
            trainer_config.params.callbacks = callbacks

        # åˆ›å»ºloggersï¼ˆå¦‚æœé…ç½®ä¸­æœ‰çš„è¯ï¼‰
        if "loggers" in self.config:
            loggers = []
            for logger_name, logger_config in self.config.loggers.items():
                log_obj = instantiate_from_config(logger_config)
                loggers.append(log_obj)
                logger.info(f"Added logger: {logger_name}")
            trainer_config.params.logger = loggers

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = instantiate_from_config(trainer_config)

        logger.info("ğŸ“ Starting training...")

        # å¼€å§‹è®­ç»ƒ - å°±æ˜¯è¿™ä¹ˆç®€å•ï¼
        trainer.fit(model, data_module)

        # è¿”å›è®­ç»ƒç»“æœ
        return {
            "status": "completed",
            "best_model_path": trainer.checkpoint_callback.best_model_path if trainer.checkpoint_callback else None,
            "trainer": trainer,
        }

    def _run_testing(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œæµ‹è¯•ä»»åŠ¡

        æµ‹è¯•ä»»åŠ¡çš„æ ¸å¿ƒç›®æ ‡æ˜¯è¯„ä¼°å·²è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒåŠ è½½ä¿å­˜çš„
        æ£€æŸ¥ç‚¹ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œæ¨¡å‹ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Šã€‚

        Returns:
            åŒ…å«æµ‹è¯•ç»“æœå’Œç›¸å…³æ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        logger.info("ğŸ§ª Initializing testing task...")

        # éªŒè¯å¿…éœ€çš„é…ç½®
        if "checkpoint_path" not in self.config:
            raise ValueError("Testing requires 'checkpoint_path' in config")

        checkpoint_path = self.config.checkpoint_path
        if not Path(checkpoint_path).exists():
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

        logger.info(f"Loading checkpoint: {checkpoint_path}")

        # åˆ›å»ºç»„ä»¶
        logger.info("Creating model and data module...")
        model = instantiate_from_config(self.config.model)
        data_module = instantiate_from_config(self.config.data)

        # ä¸ºæµ‹è¯•ä»»åŠ¡è°ƒæ•´traineré…ç½®
        trainer_config = self.config.trainer.copy()
        trainer_config.params.update(
            {
                "logger": False,  # æµ‹è¯•æ—¶ä¸éœ€è¦æ—¥å¿—è®°å½•
                "enable_checkpointing": False,  # æµ‹è¯•æ—¶ä¸ä¿å­˜æ£€æŸ¥ç‚¹
                "enable_progress_bar": True,  # æ˜¾ç¤ºæµ‹è¯•è¿›åº¦
            }
        )

        trainer = instantiate_from_config(trainer_config)

        # è¿è¡Œæµ‹è¯•
        logger.info("ğŸ¯ Running model testing...")
        test_results = trainer.test(model, data_module, ckpt_path=checkpoint_path)

        # ä¿å­˜æµ‹è¯•ç»“æœ
        results_file = self._save_test_results(test_results)

        logger.info("âœ… Testing completed successfully!")
        return {
            "status": "completed",
            "test_results": test_results,
            "results_file": results_file,
            "checkpoint_used": checkpoint_path,
        }

    def _run_prediction(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ¨ç†ä»»åŠ¡

        æ¨ç†ä»»åŠ¡ç”¨äºåœ¨æ–°æ•°æ®ä¸Šç”Ÿæˆé¢„æµ‹ç»“æœã€‚å®ƒç‰¹åˆ«é€‚ç”¨äºï¼š
        1. ç”Ÿæˆç«èµ›æäº¤æ–‡ä»¶
        2. å¯¹æ–°çš„é¥æ„Ÿå›¾åƒè¿›è¡Œæ»‘å¡æ£€æµ‹
        3. æ‰¹é‡å¤„ç†å¤§é‡å›¾åƒ

        Returns:
            åŒ…å«é¢„æµ‹ç»“æœå’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        logger.info("ğŸ”® Initializing prediction task...")

        # éªŒè¯å¿…éœ€çš„é…ç½®
        if "checkpoint_path" not in self.config:
            raise ValueError("Prediction requires 'checkpoint_path' in config")

        checkpoint_path = self.config.checkpoint_path
        if not Path(checkpoint_path).exists():
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

        logger.info(f"Loading model from: {checkpoint_path}")

        # åˆ›å»ºç»„ä»¶
        model = instantiate_from_config(self.config.model)
        data_module = instantiate_from_config(self.config.data)

        # ä¸ºæ¨ç†ä»»åŠ¡é…ç½®trainer
        trainer_config = self.config.trainer.copy()
        trainer_config.params.update(
            {
                "logger": False,
                "enable_checkpointing": False,
                "enable_progress_bar": True,
            }
        )
        trainer = instantiate_from_config(trainer_config)

        # è¿è¡Œé¢„æµ‹
        logger.info("ğŸ¯ Generating predictions...")
        predictions = trainer.predict(model, data_module, ckpt_path=checkpoint_path)

        # å¤„ç†å’Œä¿å­˜é¢„æµ‹ç»“æœ
        processed_predictions = self._process_predictions(predictions)
        output_files = self._save_predictions(processed_predictions)

        logger.info("âœ… Prediction completed successfully!")
        return {
            "status": "completed",
            "predictions": processed_predictions,
            "output_files": output_files,
            "checkpoint_used": checkpoint_path,
            "num_samples": len(processed_predictions) if processed_predictions else 0,
        }

    def _run_validation(self) -> Dict[str, Any]:
        """
        æ‰§è¡ŒéªŒè¯ä»»åŠ¡

        éªŒè¯ä»»åŠ¡ç”¨äºåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œé€šå¸¸ç”¨äºï¼š
        1. æ¨¡å‹å¼€å‘è¿‡ç¨‹ä¸­çš„å¿«é€Ÿæ€§èƒ½æ£€æŸ¥
        2. è¶…å‚æ•°è°ƒä¼˜
        3. æ¨¡å‹é€‰æ‹©å’Œæ¯”è¾ƒ

        Returns:
            åŒ…å«éªŒè¯ç»“æœçš„å­—å…¸
        """
        logger.info("ğŸ” Initializing validation task...")

        # åˆ›å»ºç»„ä»¶
        model = instantiate_from_config(self.config.model)
        data_module = instantiate_from_config(self.config.data)

        # é…ç½®trainerï¼ˆéªŒè¯ä»»åŠ¡é€šå¸¸æ¯”è¾ƒè½»é‡ï¼‰
        trainer_config = self.config.trainer.copy()
        trainer_config.params.update(
            {
                "logger": False,
                "enable_checkpointing": False,
                "enable_progress_bar": True,
            }
        )
        trainer = instantiate_from_config(trainer_config)

        # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†æ£€æŸ¥ç‚¹
        checkpoint_path = self.config.get("checkpoint_path")
        if checkpoint_path:
            logger.info(f"Using checkpoint: {checkpoint_path}")
            if not Path(checkpoint_path).exists():
                raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

        # è¿è¡ŒéªŒè¯
        logger.info("ğŸ¯ Running validation...")
        val_results = trainer.validate(model, data_module, ckpt_path=checkpoint_path)

        # ä¿å­˜éªŒè¯ç»“æœ
        results_file = self._save_validation_results(val_results)

        logger.info("âœ… Validation completed successfully!")
        return {
            "status": "completed",
            "validation_results": val_results,
            "results_file": results_file,
            "checkpoint_used": checkpoint_path,
        }

    def _save_test_results(self, test_results: List[Dict]) -> Path:
        """
        ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶

        Args:
            test_results: Lightning trainer.test()çš„è¿”å›ç»“æœ

        Returns:
            ä¿å­˜çš„ç»“æœæ–‡ä»¶è·¯å¾„
        """
        import json
        from datetime import datetime

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config.outputs.get("predictions_dir", "outputs/test_results"))
        output_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"test_results_{self.config.experiment_name}_{timestamp}.json"
        results_file = output_dir / filename

        # ä¿å­˜ç»“æœ
        with open(results_file, "w") as f:
            json.dump(
                {
                    "experiment_name": self.config.experiment_name,
                    "timestamp": timestamp,
                    "checkpoint_path": self.config.get("checkpoint_path"),
                    "test_results": test_results,
                    "config_summary": {
                        "model_type": self.config.model.target.split(".")[-1],
                        "data_config": self.config.data.target.split(".")[-1],
                    },
                },
                f,
                indent=2,
            )

        logger.info(f"Test results saved to: {results_file}")
        return results_file

    def _save_validation_results(self, val_results: List[Dict]) -> Path:
        """
        ä¿å­˜éªŒè¯ç»“æœåˆ°æ–‡ä»¶

        Args:
            val_results: Lightning trainer.validate()çš„è¿”å›ç»“æœ

        Returns:
            ä¿å­˜çš„ç»“æœæ–‡ä»¶è·¯å¾„
        """
        import json
        from datetime import datetime

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config.outputs.get("predictions_dir", "outputs/validation_results"))
        output_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"validation_results_{self.config.experiment_name}_{timestamp}.json"
        results_file = output_dir / filename

        # ä¿å­˜ç»“æœ
        with open(results_file, "w") as f:
            json.dump(
                {
                    "experiment_name": self.config.experiment_name,
                    "timestamp": timestamp,
                    "checkpoint_path": self.config.get("checkpoint_path"),
                    "validation_results": val_results,
                    "config_summary": {
                        "model_type": self.config.model.target.split(".")[-1],
                        "data_config": self.config.data.target.split(".")[-1],
                    },
                },
                f,
                indent=2,
            )

        logger.info(f"Validation results saved to: {results_file}")
        return results_file

    def _process_predictions(self, raw_predictions: List) -> List[Dict]:
        """
        å¤„ç†åŸå§‹é¢„æµ‹ç»“æœ

        å°†PyTorchå¼ é‡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼ï¼Œå¹¶æ·»åŠ å¿…è¦çš„å…ƒæ•°æ®ã€‚

        Args:
            raw_predictions: trainer.predict()çš„åŸå§‹è¿”å›ç»“æœ

        Returns:
            å¤„ç†åçš„é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        import torch
        import numpy as np

        processed = []

        for batch_idx, batch_predictions in enumerate(raw_predictions):
            # å¦‚æœé¢„æµ‹ç»“æœæ˜¯å¼ é‡ï¼Œè½¬æ¢ä¸ºnumpyæ•°ç»„
            if isinstance(batch_predictions, torch.Tensor):
                predictions_np = batch_predictions.cpu().numpy()
            else:
                predictions_np = batch_predictions

            # å¤„ç†æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹
            for sample_idx, prediction in enumerate(predictions_np):
                processed.append(
                    {
                        "batch_idx": batch_idx,
                        "sample_idx": sample_idx,
                        "prediction": float(prediction) if np.isscalar(prediction) else prediction.tolist(),
                        "probability": (
                            float(torch.sigmoid(torch.tensor(prediction)).item()) if np.isscalar(prediction) else None
                        ),
                    }
                )

        logger.info(f"Processed {len(processed)} predictions")
        return processed

    def _save_predictions(self, predictions: List[Dict]) -> Dict[str, Path]:
        """
        ä¿å­˜é¢„æµ‹ç»“æœåˆ°å¤šç§æ ¼å¼çš„æ–‡ä»¶

        Args:
            predictions: å¤„ç†åçš„é¢„æµ‹ç»“æœ

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        import json
        import pandas as pd
        from datetime import datetime

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config.outputs.get("predictions_dir", "outputs/predictions"))
        output_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"predictions_{self.config.experiment_name}_{timestamp}"

        output_files = {}

        # ä¿å­˜JSONæ ¼å¼ï¼ˆå®Œæ•´ä¿¡æ¯ï¼‰
        json_file = output_dir / f"{base_filename}.json"
        with open(json_file, "w") as f:
            json.dump(
                {
                    "experiment_name": self.config.experiment_name,
                    "timestamp": timestamp,
                    "checkpoint_path": self.config.get("checkpoint_path"),
                    "num_predictions": len(predictions),
                    "predictions": predictions,
                },
                f,
                indent=2,
            )
        output_files["json"] = json_file

        # ä¿å­˜CSVæ ¼å¼ï¼ˆä¾¿äºåˆ†æï¼‰
        if predictions:
            df_data = []
            for pred in predictions:
                df_data.append(
                    {
                        "sample_id": f"sample_{pred['batch_idx']}_{pred['sample_idx']}",
                        "prediction": pred["prediction"],
                        "probability": pred.get("probability", None),
                    }
                )

            df = pd.DataFrame(df_data)
            csv_file = output_dir / f"{base_filename}.csv"
            df.to_csv(csv_file, index=False)
            output_files["csv"] = csv_file

        logger.info(f"Predictions saved to: {list(output_files.values())}")
        return output_files

    def _print_experiment_info(self):
        """æ‰“å°å®éªŒä¿¡æ¯"""
        print("\n" + "=" * 80)
        print(f"ğŸš€ MM-LandslideNet Experiment: {self.config.get('experiment_name', 'Unnamed')}")
        print("=" * 80)
        print(f"ğŸ“ Task: {self.task}")
        print(f"ğŸ“ Config: {self.config_path}")
        print(f"ğŸ• Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        if "model" in self.config:
            print(f"ğŸ§  Model: {self.config.model.target.split('.')[-1]}")
        if "data" in self.config:
            print(f"ğŸ“Š Data: {self.config.data.get('params', {}).get('train_data_dir', 'N/A')}")

        print("=" * 80 + "\n")


def create_parser() -> argparse.ArgumentParser:
    """
    åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨

    æ–°ç‰ˆæœ¬çš„å‘½ä»¤è¡Œæ¥å£æ›´åŠ ç®€æ´ï¼Œé‡ç‚¹çªå‡ºé…ç½®æ–‡ä»¶çš„ä½œç”¨ã€‚
    """
    parser = argparse.ArgumentParser(
        description="MM-LandslideNet: Configuration-Driven Deep Learning Framework",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # è®­ç»ƒæ¨¡å‹
  python main.py train configs/experiment/optical_baseline.yaml
  
  # æµ‹è¯•æ¨¡å‹  
  python main.py test configs/experiment/optical_baseline.yaml
  
  # è¿è¡Œæ¨ç†
  python main.py predict configs/experiment/optical_baseline.yaml
  
  # éªŒè¯æ¨¡å‹
  python main.py validate configs/experiment/optical_baseline.yaml

Configuration-First Design:
  This framework follows the "configuration-first" principle inspired by 
  latent-diffusion. All model architectures, training strategies, and data 
  processing pipelines are defined in YAML configuration files, making the
  framework extremely flexible and maintainable.
        """,
    )

    # ä¸»è¦å‚æ•°
    parser.add_argument("task", choices=["train", "test", "predict", "validate"], help="Task to execute")

    parser.add_argument("config", type=str, help="Path to configuration file")

    # å¯é€‰å‚æ•°
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose logging")

    parser.add_argument("--checkpoint", type=str, help="Path to model checkpoint (overrides config)")

    parser.add_argument(
        "--override",
        type=str,
        nargs="*",
        help="Override config values (e.g., --override training.max_epochs=100 data.batch_size=32)",
    )

    return parser


def apply_overrides(config: DictConfig, overrides: list) -> DictConfig:
    """
    åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–

    å…è®¸ç”¨æˆ·åœ¨å‘½ä»¤è¡Œä¸­è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„ç‰¹å®šå€¼ã€‚
    è¿™åœ¨è°ƒè¯•å’Œå¿«é€Ÿå®éªŒæ—¶éå¸¸æœ‰ç”¨ã€‚

    Args:
        config: åŸå§‹é…ç½®
        overrides: è¦†ç›–åˆ—è¡¨ï¼Œæ ¼å¼ä¸º ["key=value", "another.key=value"]

    Returns:
        ä¿®æ”¹åçš„é…ç½®
    """
    if not overrides:
        return config

    logger.info(f"Applying {len(overrides)} config overrides...")

    for override in overrides:
        try:
            key, value = override.split("=", 1)

            # å°è¯•è‡ªåŠ¨ç±»å‹è½¬æ¢
            try:
                # å¤„ç†æ•°å­—
                if value.isdigit():
                    value = int(value)
                elif value.replace(".", "").isdigit() and value.count(".") == 1:
                    value = float(value)
                # å¤„ç†å¸ƒå°”å€¼
                elif value.lower() in ["true", "false"]:
                    value = value.lower() == "true"
                # å¤„ç†åˆ—è¡¨ï¼ˆç®€å•æƒ…å†µï¼‰
                elif value.startswith("[") and value.endswith("]"):
                    value = eval(value)  # æ³¨æ„ï¼šç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥ç”¨æ›´å®‰å…¨çš„è§£ææ–¹æ³•
            except:
                pass  # ä¿æŒä¸ºå­—ç¬¦ä¸²

            OmegaConf.set(config, key, value)
            logger.info(f"  {key} = {value}")

        except Exception as e:
            logger.warning(f"Failed to apply override '{override}': {e}")

    return config


def main():
    """
    ä¸»å‡½æ•°

    è¿™æ˜¯æ•´ä¸ªç¨‹åºçš„å…¥å£ç‚¹ã€‚ä¸æ‚¨åŸæ¥çš„main.pyç›¸æ¯”ï¼Œ
    æ–°ç‰ˆæœ¬çš„é€»è¾‘æå…¶ç®€æ´ï¼š
    1. è§£æå‘½ä»¤è¡Œå‚æ•°
    2. åˆ›å»ºå®éªŒè¿è¡Œå™¨
    3. è¿è¡Œå®éªŒ
    4. æŠ¥å‘Šç»“æœ

    æ‰€æœ‰çš„å¤æ‚æ€§éƒ½è¢«é…ç½®æ–‡ä»¶å’Œå®ä¾‹åŒ–å·¥å…·å¸æ”¶äº†ã€‚
    """
    parser = create_parser()
    args = parser.parse_args()

    try:
        # åˆ›å»ºå®éªŒè¿è¡Œå™¨
        runner = ExperimentRunner(args.config, args.task)

        # åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if args.override:
            runner.config = apply_overrides(runner.config, args.override)

        # è¦†ç›–checkpointè·¯å¾„ï¼ˆå¦‚æœåœ¨å‘½ä»¤è¡Œä¸­æŒ‡å®šï¼‰
        if args.checkpoint:
            runner.config.checkpoint_path = args.checkpoint
            logger.info(f"Using checkpoint: {args.checkpoint}")

        # è¿è¡Œå®éªŒ
        results = runner.run()

        # æŠ¥å‘Šç»“æœ
        print(f"\nğŸ‰ Task '{args.task}' completed successfully!")
        if results.get("best_model_path"):
            print(f"ğŸ“ Best model saved to: {results['best_model_path']}")

        return 0

    except KeyboardInterrupt:
        logger.info("Experiment interrupted by user")
        return 1

    except Exception as e:
        logger.error(f"Experiment failed: {e}")
        if args.verbose:
            import traceback

            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
