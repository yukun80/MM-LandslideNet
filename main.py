import os
import sys
import argparse
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
import warnings
from datetime import datetime
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import torch
import pytorch_lightning as pl
from omegaconf import DictConfig, OmegaConf

# å¯¼å…¥æˆ‘ä»¬çš„æ ¸å¿ƒå·¥å…·
from lightning_landslide.src.utils.instantiate import instantiate_from_config, validate_config_structure
from lightning_landslide.src.utils.logging_utils import setup_logging, get_project_logger
from lightning_landslide.src.training.simple_kfold_trainer import SimpleKFoldTrainer

# å¯¼å…¥ä¸»åŠ¨å­¦ä¹ æ¨¡å—
from lightning_landslide.src.active_learning.active_steps import ActiveLearningStepManager

logger = get_project_logger(__name__)


class ExperimentRunner:
    """
    20250728-æ–°å¢åŠŸèƒ½ï¼š
    1. æ”¯æŒkfoldä»»åŠ¡ç±»å‹
    2. ä¿æŒç°æœ‰æ¶æ„çš„ç®€æ´æ€§
    3. å§”æ‰˜å¤æ‚é€»è¾‘ç»™ä¸“é—¨çš„KFoldTrainer

    20250729-æ–°å¢åŠŸèƒ½ï¼š
    1. active_train: ä¸»åŠ¨å­¦ä¹ +ä¼ªæ ‡ç­¾è®­ç»ƒ
    2. å®Œå…¨å‘åå…¼å®¹ç°æœ‰åŠŸèƒ½
    3. æ™ºèƒ½é…ç½®éªŒè¯å’Œé”™è¯¯å¤„ç†

    20250731-æ–°å¢æ”¯æŒï¼š
    1. uncertainty_estimation: ä¸ç¡®å®šæ€§ä¼°è®¡
    2. sample_selection: æ ·æœ¬é€‰æ‹©
    3. retrain: æ¨¡å‹é‡è®­ç»ƒ
    4. ä¿æŒæ‰€æœ‰ç°æœ‰åŠŸèƒ½ä¸å˜
    """

    def __init__(self, config_path: str, task: str = "train", **kwargs):
        """
        åˆå§‹åŒ–å®éªŒè¿è¡Œå™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            task: ä»»åŠ¡ç±»å‹
            **kwargs: é¢å¤–çš„ä»»åŠ¡å‚æ•°
        """
        setup_logging(level=logging.INFO)
        self.config_path = Path(config_path)
        self.task = task
        self.task_kwargs = kwargs
        self.config = self._load_config()
        self._setup_environment()

    def _load_config(self) -> DictConfig:
        """åŠ è½½å’ŒéªŒè¯é…ç½®æ–‡ä»¶"""
        if not self.config_path.exists():
            raise FileNotFoundError(f"Config file not found: {self.config_path}")

        logger.info(f"Loading config from: {self.config_path}")
        config = OmegaConf.load(self.config_path)

        # åŸºç¡€éªŒè¯
        if not validate_config_structure(config):
            raise ValueError("Invalid configuration structure")

        # ä¸»åŠ¨å­¦ä¹ ä»»åŠ¡éœ€è¦éªŒè¯ç›¸å…³é…ç½®
        if self.task in ["active_train", "uncertainty_estimation", "sample_selection", "retrain"]:
            self._validate_active_learning_config(config)

        logger.info("âœ“ Configuration loaded and validated")
        return config

    def _validate_active_learning_config(self, config: DictConfig):
        """éªŒè¯ä¸»åŠ¨å­¦ä¹ é…ç½®"""
        if "active_pseudo_learning" not in config:
            logger.warning("Missing 'active_pseudo_learning' section, using defaults")
            config.active_pseudo_learning = {}

        logger.info("âœ“ Active learning configuration validated")

    def _setup_environment(self):
        """è®¾ç½®å®éªŒç¯å¢ƒ"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self._create_output_dirs()

        # è®¾ç½®æ—¥å¿—
        log_level = getattr(logging, self.config.get("log_level", "INFO").upper())
        log_file = None

        if "outputs" in self.config and "log_dir" in self.config.outputs:
            log_file = Path(self.config.outputs.log_dir) / f"{self.config.experiment_name}.log"

        setup_logging(level=log_level, log_file=log_file)

        # PyTorchè®¾ç½®
        if "seed" in self.config:
            pl.seed_everything(self.config.seed, workers=True)

        # GPUè®¾ç½®
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = self.config.get("cudnn_benchmark", True)

    def _create_output_dirs(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        if "outputs" not in self.config:
            # åˆ›å»ºé»˜è®¤è¾“å‡ºé…ç½®
            experiment_name = self.config.get("experiment_name", f"exp_{int(datetime.now().timestamp())}")
            exp_dir = Path("lightning_landslide/exp") / experiment_name

            self.config.outputs = {
                "experiment_dir": str(exp_dir),
                "log_dir": str(exp_dir / "logs"),
                "model_dir": str(exp_dir / "models"),
                "checkpoint_dir": str(exp_dir / "checkpoints"),
            }

        # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•
        for dir_key, dir_path in self.config.outputs.items():
            Path(dir_path).mkdir(parents=True, exist_ok=True)

    def run(self) -> Dict[str, Any]:
        """è¿è¡Œå®éªŒ"""
        self._print_experiment_info()

        start_time = datetime.now()

        try:
            # æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œä¸åŒé€»è¾‘
            if self.task == "train":
                results = self._run_training()
            elif self.task == "kfold":
                results = self._run_kfold_training()
            elif self.task == "uncertainty_estimation":
                results = self._run_uncertainty_estimation()
            elif self.task == "sample_selection":
                results = self._run_sample_selection()
            elif self.task == "retrain":
                results = self._run_retraining()
            elif self.task == "predict":
                results = self._run_prediction()
            else:
                raise ValueError(f"Unknown task: {self.task}")

            # è®¡ç®—è¿è¡Œæ—¶é—´
            end_time = datetime.now()
            results["execution_time"] = str(end_time - start_time)
            results["task"] = self.task

            logger.info(f"âœ… {self.task.upper()} completed successfully")
            logger.info(f"â±ï¸ Total time: {results['execution_time']}")

            return results

        except Exception as e:
            logger.error(f"âŒ {self.task.upper()} failed: {str(e)}")
            raise

    def _run_training(self) -> Dict[str, Any]:
        """è¿è¡ŒåŸºç¡€è®­ç»ƒï¼ˆæ­¥éª¤1ï¼‰"""
        logger.info("ğŸš€ Running baseline training...")

        # å®ä¾‹åŒ–ç»„ä»¶
        model = instantiate_from_config(self.config.model)
        datamodule = instantiate_from_config(self.config.data)
        trainer = instantiate_from_config(self.config.trainer)

        # è®¾ç½®å›è°ƒ
        if "callbacks" in self.config:
            callbacks = []
            for cb_name, cb_config in self.config.callbacks.items():
                # ğŸ”§ åŠ¨æ€æ›¿æ¢ModelCheckpointä¸­çš„è·¯å¾„
                if cb_name == "model_checkpoint" and "dirpath" in cb_config.params:
                    # æ›¿æ¢å®éªŒåç§°å˜é‡
                    dirpath = cb_config.params.dirpath
                    if "${experiment_name}" in dirpath:
                        experiment_name = self.config.get("experiment_name", "default_exp")
                        cb_config.params.dirpath = dirpath.replace("${experiment_name}", experiment_name)

                callbacks.append(instantiate_from_config(cb_config))
            trainer.callbacks = callbacks

        # ğŸ”§ åŠ¨æ€è®¾ç½®Loggerè·¯å¾„
        if "logger" in self.config:
            logger_config = self.config.logger.copy()  # å¤åˆ¶é…ç½®é¿å…ä¿®æ”¹åŸå§‹é…ç½®

            # æ›¿æ¢å®éªŒåç§°å˜é‡
            if "name" in logger_config.params and "${experiment_name}" in str(logger_config.params.name):
                experiment_name = self.config.get("experiment_name", "default_exp")
                logger_config.params.name = logger_config.params.name.replace("${experiment_name}", experiment_name)

            trainer.logger = instantiate_from_config(logger_config)

        # è®­ç»ƒæ¨¡å‹
        trainer.fit(model, datamodule)
        """
            fit æ˜¯ pytorch_lightning.Trainer ç±»çš„æ ¸å¿ƒæ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•ä¼šè‡ªåŠ¨æ‰§è¡Œæ•´ä¸ªè®­ç»ƒæµç¨‹ï¼š

            æ•°æ®å‡†å¤‡ï¼šè°ƒç”¨ data_module.prepare_data() å’Œ data_module.setup()
            åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼šè·å– train/val dataloaders
            è®­ç»ƒå¾ªç¯ï¼š

            è°ƒç”¨ model.training_step() å¤„ç†æ¯ä¸ªbatch
            è®¡ç®—losså’Œæ¢¯åº¦
            æ‰§è¡Œä¼˜åŒ–å™¨æ›´æ–°

            éªŒè¯å¾ªç¯ï¼š

            è°ƒç”¨ model.validation_step()
            è®¡ç®—éªŒè¯æŒ‡æ ‡

            å›è°ƒæ‰§è¡Œï¼šè¿è¡Œcheckpointingã€early stoppingç­‰
            æ—¥å¿—è®°å½•ï¼šè‡ªåŠ¨è®°å½•æ‰€æœ‰æŒ‡æ ‡
            # Lightningå†…éƒ¨ä¼šè°ƒç”¨
            â”œâ”€â”€ data_module.prepare_data()           # æ•°æ®å‡†å¤‡
            â”œâ”€â”€ data_module.setup('fit')             # æ•°æ®é›†è®¾ç½®  
            â”œâ”€â”€ data_module.train_dataloader()       # è·å–è®­ç»ƒæ•°æ®åŠ è½½å™¨
            â”œâ”€â”€ data_module.val_dataloader()         # è·å–éªŒè¯æ•°æ®åŠ è½½å™¨
            â”œâ”€â”€ model.configure_optimizers()         # é…ç½®ä¼˜åŒ–å™¨
            â””â”€â”€ è®­ç»ƒå¾ªç¯:
                â”œâ”€â”€ model.training_step(batch, idx)  # æ¯ä¸ªè®­ç»ƒbatch
                â”œâ”€â”€ optimizer.step()                 # å‚æ•°æ›´æ–°
                â”œâ”€â”€ model.validation_step(batch, idx) # æ¯ä¸ªéªŒè¯batch
                â”œâ”€â”€ callbacks.on_epoch_end()         # å›è°ƒå‡½æ•°
                â””â”€â”€ logger.log_metrics()             # è®°å½•æŒ‡æ ‡
        """

        logger.info("ğŸ† Training completed successfully!")
        logger.info("ğŸ“Š Model performance should be evaluated based on validation metrics only")
        logger.info("ğŸ¯ Use the validation F1 score as the primary performance indicator")

        # ä»éªŒè¯æŒ‡æ ‡ä¸­æå–æœ€ç»ˆæ€§èƒ½
        best_val_metrics = {}
        if hasattr(trainer, "callback_metrics"):
            for key, value in trainer.callback_metrics.items():
                if "val_" in key:
                    best_val_metrics[key] = float(value) if hasattr(value, "item") else value

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = Path(self.config.outputs.experiment_dir) / "checkpoints" / "final_model.ckpt"
        final_model_path.parent.mkdir(parents=True, exist_ok=True)
        trainer.save_checkpoint(str(final_model_path))

        return {
            "best_checkpoint": (
                trainer.checkpoint_callback.best_model_path if hasattr(trainer, "checkpoint_callback") else None
            ),
            "final_model": str(final_model_path),
            "validation_metrics": best_val_metrics,  # ğŸ”§ ä½¿ç”¨éªŒè¯æŒ‡æ ‡è€Œä¸æ˜¯æµ‹è¯•æŒ‡æ ‡
            "training_completed": True,
        }

    def _run_kfold_training(self) -> Dict[str, Any]:
        """è¿è¡ŒKæŠ˜äº¤å‰éªŒè¯è®­ç»ƒ"""
        logger.info("ğŸ”„ Running K-fold cross-validation...")

        kfold_trainer = SimpleKFoldTrainer(
            config=dict(self.config),
            experiment_name=self.config.experiment_name,
            output_dir=self.config.outputs.experiment_dir,
        )

        return kfold_trainer.run_kfold_training()

    # =============================================================================
    # åˆ†æ­¥ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ï¼ˆæ­¥éª¤2-5ï¼‰
    # =============================================================================

    def _run_uncertainty_estimation(self) -> Dict[str, Any]:
        """è¿è¡Œä¸ç¡®å®šæ€§ä¼°è®¡ï¼ˆæ­¥éª¤2ï¼‰"""
        logger.info("ğŸ” Running uncertainty estimation step...")

        state_path = self.task_kwargs.get("state_path")
        return ActiveLearningStepManager.run_uncertainty_estimation(config=dict(self.config), state_path=state_path)

    def _run_sample_selection(self) -> Dict[str, Any]:
        """è¿è¡Œæ ·æœ¬é€‰æ‹©ï¼ˆæ­¥éª¤3ï¼‰"""
        logger.info("ğŸ¯ Running sample selection step...")

        state_path = self.task_kwargs.get("state_path")
        return ActiveLearningStepManager.run_sample_selection(config=dict(self.config), state_path=state_path)

    def _run_retraining(self) -> Dict[str, Any]:
        """è¿è¡Œæ¨¡å‹é‡è®­ç»ƒï¼ˆæ­¥éª¤5ï¼‰"""
        logger.info("ğŸ”„ Running model retraining step...")

        state_path = self.task_kwargs.get("state_path")
        annotation_file = self.task_kwargs.get("annotation_file")

        return ActiveLearningStepManager.run_retraining(
            config=dict(self.config), state_path=state_path, annotation_file=annotation_file
        )

    # =============================================================================
    # é¢„æµ‹
    # =============================================================================

    def _run_prediction(self) -> Dict[str, Any]:
        """è¿è¡Œé¢„æµ‹ï¼ˆä¸“é—¨ç”¨äºKaggleæäº¤ï¼‰"""
        logger.info("ğŸ”® Running prediction for Kaggle submission...")

        # åŠ è½½æœ€ä½³æ¨¡å‹
        checkpoint_path = self.config.get("checkpoint_path")
        if not checkpoint_path:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€ä½³æ£€æŸ¥ç‚¹
            exp_dir = Path(self.config.outputs.experiment_dir)
            checkpoint_dir = exp_dir / "checkpoints"

            # æŸ¥æ‰¾æœ€ä½³F1æ£€æŸ¥ç‚¹
            best_checkpoints = list(checkpoint_dir.glob("best-epoch=*-val_f1=*.ckpt"))
            if best_checkpoints:
                checkpoint_path = str(sorted(best_checkpoints)[-1])  # å–æœ€æ–°çš„
            else:
                raise FileNotFoundError("No trained model found. Please run training first.")

        logger.info(f"ğŸ“¥ Loading model from: {checkpoint_path}")

        # å®ä¾‹åŒ–ç»„ä»¶
        model = instantiate_from_config(self.config.model)
        datamodule = instantiate_from_config(self.config.data)
        trainer = instantiate_from_config(self.config.trainer)

        # åŠ è½½æ£€æŸ¥ç‚¹
        model = model.load_from_checkpoint(checkpoint_path)
        model.eval()

        # è®¾ç½®æ•°æ®ï¼ˆåªéœ€è¦æµ‹è¯•é›†ï¼‰
        datamodule.setup("predict")

        # è¿›è¡Œé¢„æµ‹ï¼ˆä¸è®¡ç®—ä»»ä½•æŒ‡æ ‡ï¼‰
        predictions = trainer.predict(model, datamodule.predict_dataloader())

        # å¤„ç†é¢„æµ‹ç»“æœ
        all_probs = []
        all_preds = []

        for batch_preds in predictions:
            probs = batch_preds["probabilities"].cpu().numpy()
            preds = batch_preds["predictions"].cpu().numpy()
            all_probs.extend(probs)
            all_preds.extend(preds)

        # è·å–æµ‹è¯•æ ·æœ¬ID
        test_dataset = datamodule.test_dataset
        sample_ids = [test_dataset.data_index.iloc[i]["ID"] for i in range(len(test_dataset))]

        # åˆ›å»ºæäº¤æ–‡ä»¶
        submission_df = pd.DataFrame(
            {"ID": sample_ids, "label": [int(pred) for pred in all_preds]}  # Kaggleé€šå¸¸è¦æ±‚æ•´æ•°æ ‡ç­¾
        )

        # ä¿å­˜æäº¤æ–‡ä»¶
        submission_path = exp_dir / "kaggle_submission.csv"
        submission_df.to_csv(submission_path, index=False)

        # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœï¼ˆåŒ…å«æ¦‚ç‡ï¼‰
        detailed_results = pd.DataFrame({"ID": sample_ids, "probability": all_probs, "prediction": all_preds})

        detailed_path = exp_dir / "detailed_predictions.csv"
        detailed_results.to_csv(detailed_path, index=False)

        logger.info(f"âœ… Prediction completed!")
        logger.info(f"ğŸ“„ Kaggle submission saved to: {submission_path}")
        logger.info(f"ğŸ“Š Detailed results saved to: {detailed_path}")
        logger.info(f"ğŸ¯ Predicted {len(sample_ids)} samples")

        # é¢„æµ‹ç»Ÿè®¡
        positive_ratio = sum(all_preds) / len(all_preds)
        logger.info(f"ğŸ“ˆ Positive prediction ratio: {positive_ratio:.3f}")

        return {
            "submission_path": str(submission_path),
            "detailed_path": str(detailed_path),
            "num_predictions": len(sample_ids),
            "positive_ratio": positive_ratio,
            "checkpoint_used": checkpoint_path,
        }

    def _print_experiment_info(self):
        """æ‰“å°å®éªŒä¿¡æ¯"""
        print("\n" + "=" * 80)
        print(f"ğŸš€ MM-LANDSLIDE NET - {self.task.upper()}")
        print("=" * 80)
        print(f"ğŸ“… TIME: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¯ TASK: {self.task}")
        print(f"ğŸ“ CONFIG: {self.config_path}")

        if "experiment_name" in self.config:
            print(f"ğŸ”¬ EXPERIMENT: {self.config.experiment_name}")

        if "model" in self.config:
            model_name = str(self.config.model.target).split(".")[-1]
            print(f"ğŸ¤– MODEL: {model_name}")

        if "data" in self.config:
            data_dir = self.config.data.get("params", {}).get("train_data_dir", "N/A")
            print(f"ğŸ“Š DATA: {data_dir}")

        # ä¸»åŠ¨å­¦ä¹ ç‰¹å®šä¿¡æ¯
        if self.task in ["active_train", "uncertainty_estimation", "sample_selection", "retrain"]:
            if "active_pseudo_learning" in self.config:
                apl_config = self.config.active_pseudo_learning
                print(f"ğŸ¯ MAX ITERATIONS: {apl_config.get('max_iterations', 5)}")
                print(f"ğŸ“ ANNOTATION BUDGET: {apl_config.get('annotation_budget', 50)}")

        print("=" * 80 + "\n")


def create_parser() -> argparse.ArgumentParser:
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="MM-LandslideNet: Stepwise Active Learning Framework",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # æ­¥éª¤1ï¼šåŸºç¡€è®­ç»ƒ
  python main.py train lightning_landslide/configs/optical_baseline.yaml
  
  # KæŠ˜äº¤å‰éªŒè¯
  python main.py kfold lightning_landslide/configs/optical_baseline_5-fold.yaml
  
  # === åˆ†æ­¥ä¸»åŠ¨å­¦ä¹  ===
  # æ­¥éª¤2ï¼šä¸ç¡®å®šæ€§ä¼°è®¡
  python main.py uncertainty_estimation lightning_landslide/configs/optical_baseline_active_steps.yaml
  
  # æ­¥éª¤3ï¼šæ ·æœ¬é€‰æ‹©
  python main.py sample_selection lightning_landslide/configs/optical_baseline_active_steps.yaml
  
  # æ­¥éª¤5ï¼šæ¨¡å‹é‡è®­ç»ƒ
  python main.py retrain lightning_landslide/configs/optical_baseline_active_steps.yaml \
--annotation_file lightning_landslide/exp/optical_swin_tiny_0731_active_steps/active_learning/annotation_results_iter_0.json
        """,
    )

    parser.add_argument(
        "task",
        choices=[
            "train",  # åŸºç¡€è®­ç»ƒ
            "kfold",  # KæŠ˜äº¤å‰éªŒè¯
            "uncertainty_estimation",  # æ­¥éª¤2ï¼šä¸ç¡®å®šæ€§ä¼°è®¡
            "sample_selection",  # æ­¥éª¤3ï¼šæ ·æœ¬é€‰æ‹©
            "retrain",  # æ­¥éª¤5ï¼šæ¨¡å‹é‡è®­ç»ƒ
        ],
        help="Task to execute",
    )

    parser.add_argument("config", type=str, help="Path to configuration file")

    # é€šç”¨å‚æ•°
    parser.add_argument("--experiment_name", type=str, help="Override experiment name")
    parser.add_argument("--checkpoint_path", type=str, help="Checkpoint path for prediction")

    # KæŠ˜ç‰¹å®šå‚æ•°
    parser.add_argument("--n_splits", type=int, help="Number of folds for K-fold CV")

    # ä¸»åŠ¨å­¦ä¹ ç‰¹å®šå‚æ•°
    parser.add_argument("--max_iterations", type=int, help="Maximum active learning iterations")
    parser.add_argument("--annotation_budget", type=int, help="Annotation budget per iteration")

    # åˆ†æ­¥ä¸»åŠ¨å­¦ä¹ å‚æ•°
    parser.add_argument("--state_path", type=str, help="Path to active learning state file")
    parser.add_argument("--annotation_file", type=str, help="Path to annotation results file")

    return parser


def main():
    """
    ä¸»å‡½æ•° - ç¨‹åºå…¥å£

    å¢å¼ºç‰ˆæœ¬æ”¯æŒæ›´å¤šä»»åŠ¡ç±»å‹ï¼ŒåŒæ—¶ä¿æŒå‘åå…¼å®¹æ€§ã€‚
    """
    parser = create_parser()
    args = parser.parse_args()

    # å‡†å¤‡ä»»åŠ¡å‚æ•°
    task_kwargs = {k: v for k, v in vars(args).items() if v is not None and k not in ["task", "config"]}

    try:
        # åˆ›å»ºå¹¶è¿è¡Œå®éªŒ
        runner = ExperimentRunner(config_path=args.config, task=args.task, **task_kwargs)

        results = runner.run()

        # æ‰“å°ç»“æœæ‘˜è¦
        print("\n" + "=" * 80)
        print(f"âœ… {args.task.upper()} COMPLETED SUCCESSFULLY")
        print("=" * 80)

        if "execution_time" in results:
            print(f"â±ï¸ Execution time: {results['execution_time']}")

        # æ‰“å°ä»»åŠ¡ç‰¹å®šç»“æœ
        if args.task == "train":
            print(f"ğŸ’¾ Best checkpoint: {results.get('best_checkpoint', 'N/A')}")
            if "test_results" in results:
                test_f1 = results["test_results"].get("test_f1", "N/A")
                print(f"ğŸ“ˆ Test F1 Score: {test_f1}")
        elif args.task == "uncertainty_estimation":
            print(f"ğŸ“Š Estimated uncertainty for {results.get('num_samples', 0)} samples")
            print(f"ğŸ“ Results saved to: {results.get('results_path', 'N/A')}")
        elif args.task == "sample_selection":
            print(f"ğŸ¯ Selected {results.get('num_selected', 0)} samples for annotation")
            print(f"ğŸ“ Annotation request: {results.get('annotation_file', 'N/A')}")
        elif args.task == "retrain":
            print(f"ğŸ“Š Added {results.get('num_annotations', 0)} human annotations")
            print(f"ğŸ·ï¸ Generated {results.get('num_pseudo_labels', 0)} pseudo labels")
            print(f"ğŸ’¾ New checkpoint: {results.get('new_checkpoint', 'N/A')}")

        print("=" * 80 + "\n")

        return 0

    except Exception as e:
        print(f"\nâŒ Task failed: {str(e)}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
