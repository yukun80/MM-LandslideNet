import os
import sys
import argparse
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
import warnings
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# æŠ‘åˆ¶ä¸å¿…è¦çš„è­¦å‘Š
# warnings.filterwarnings("ignore", ".*does not have many workers.*")
# warnings.filterwarnings("ignore", ".*The dataloader.*")

import torch
import pytorch_lightning as pl
from omegaconf import DictConfig, OmegaConf

# å¯¼å…¥æˆ‘ä»¬çš„æ ¸å¿ƒå·¥å…·
from lightning_landslide.src.utils.instantiate import instantiate_from_config, validate_config_structure
from lightning_landslide.src.utils.logging_utils import setup_logging, get_project_logger
from lightning_landslide.src.training.simple_kfold_trainer import SimpleKFoldTrainer
from lightning_landslide.src.active_learning.human_guided_active_learning import create_human_guided_active_learning

logger = get_project_logger(__name__)


class ExperimentRunner:
    """
    20250728-æ–°å¢åŠŸèƒ½ï¼š
    1. æ”¯æŒkfoldä»»åŠ¡ç±»å‹
    2. ä¿æŒç°æœ‰æ¶æ„çš„ç®€æ´æ€§
    3. å§”æ‰˜å¤æ‚é€»è¾‘ç»™ä¸“é—¨çš„KFoldTrainer

    20250729-æ–°å¢åŠŸèƒ½ï¼š
    1. active_train: ä¸»åŠ¨å­¦ä¹ +ä¼ªæ ‡ç­¾è®­ç»ƒ
    2. active_kfold: KæŠ˜+ä¸»åŠ¨å­¦ä¹ èåˆ
    3. å®Œå…¨å‘åå…¼å®¹ç°æœ‰åŠŸèƒ½
    4. æ™ºèƒ½é…ç½®éªŒè¯å’Œé”™è¯¯å¤„ç†
    """

    def __init__(self, config_path: str, task: str = "train", **kwargs):
        """
        åˆå§‹åŒ–å®éªŒè¿è¡Œå™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            task: ä»»åŠ¡ç±»å‹ (train/predict/kfold/active_train/active_kfold)
            **kwargs: é¢å¤–çš„ä»»åŠ¡å‚æ•°
        """
        setup_logging(level=logging.INFO)
        self.config_path = Path(config_path)
        self.task = task
        self.task_kwargs = kwargs
        self.config = self._load_config()
        self._setup_environment()

    def _load_config(self) -> DictConfig:
        """åŠ è½½å’ŒéªŒè¯é…ç½®æ–‡ä»¶"""
        if not self.config_path.exists():
            raise FileNotFoundError(f"Config file not found: {self.config_path}")

        logger.info(f"Loading config from: {self.config_path}")
        config = OmegaConf.load(self.config_path)

        # åŸºç¡€éªŒè¯
        if not validate_config_structure(config):
            raise ValueError("Invalid configuration structure")

        # ä¸»åŠ¨å­¦ä¹ ç‰¹å®šéªŒè¯
        if self.task in ["active_train", "active_kfold"]:
            self._validate_active_learning_config(config)

        logger.info("âœ“ Configuration loaded and validated")
        return config

    def _validate_active_learning_config(self, config: DictConfig):
        """éªŒè¯ä¸»åŠ¨å­¦ä¹ é…ç½®"""
        if "active_pseudo_learning" not in config:
            raise ValueError("Missing 'active_pseudo_learning' section for active learning tasks")

        active_config = config.active_pseudo_learning
        required_sections = ["uncertainty_estimation", "pseudo_labeling", "active_learning"]

        for section in required_sections:
            if section not in active_config:
                logger.warning(f"Missing '{section}' in active_pseudo_learning config, using defaults")

        logger.info("âœ“ Active learning configuration validated")

    def _setup_environment(self):
        """è®¾ç½®å®éªŒç¯å¢ƒ"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self._create_output_dirs()

        # è®¾ç½®æ—¥å¿—ï¼Œgetattrçš„ä½œç”¨æ˜¯è·å–configä¸­çš„log_levelï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨INFO
        log_level = getattr(logging, self.config.get("log_level", "INFO").upper())
        log_file = None

        if "outputs" in self.config and "log_dir" in self.config.outputs:
            log_file = Path(self.config.outputs.log_dir) / f"{self.config.experiment_name}.log"

        setup_logging(
            level=log_level,
            log_file=str(log_file) if log_file else None,
            use_colors=True,
        )

        # è®¾ç½®éšæœºç§å­ï¼Œseed_everythingçš„ä½œç”¨æ˜¯è®¾ç½®éšæœºç§å­ï¼Œå¹¶è®¾ç½®torch.manual_seedå’Œtorch.cuda.manual_seedï¼Œ
        # workersä¸ºTrueæ—¶ï¼Œä¼šè®¾ç½®torch.utils.data.DataLoaderçš„num_workersä¸º1
        if "seed" in self.config:
            pl.seed_everything(self.config.seed, workers=True)

        # ä¿å­˜é…ç½®æ–‡ä»¶åˆ°å®éªŒç›®å½•
        self._save_config()

    def _create_output_dirs(self):
        """æ ¹æ®experiment_nameåŠ¨æ€åˆ›å»ºå®éªŒè¾“å‡ºç›®å½•"""
        base_dir = Path(self.config.outputs.base_output_dir)
        experiment_path = base_dir / self.config.experiment_name
        logger.info(f"æ‰€æœ‰å®éªŒè¾“å‡ºå°†ä¿å­˜åˆ°: {experiment_path}")

        # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„å­ç›®å½•
        dirs_to_create = ["checkpoints", "logs", "predictions", "models", "visualizations", "data_versions"]

        # ä¸»åŠ¨å­¦ä¹ ç‰¹å®šç›®å½•
        if self.task in ["active_train", "active_kfold"]:
            dirs_to_create.extend(
                ["active_learning", "pseudo_labels", "uncertainty_analysis", "iteration_results", "annotations"]
            )

        for dir_name in dirs_to_create:
            (experiment_path / dir_name).mkdir(parents=True, exist_ok=True)

        # æ›´æ–°é…ç½®ä¸­çš„è·¯å¾„
        self.config.outputs = OmegaConf.create(
            {
                "base_output_dir": str(base_dir),
                "experiment_dir": str(experiment_path),
                "checkpoint_dir": str(experiment_path / "checkpoints"),
                "log_dir": str(experiment_path / "logs"),
                "prediction_dir": str(experiment_path / "predictions"),
                "model_dir": str(experiment_path / "models"),
                "visualization_dir": str(experiment_path / "visualizations"),
            }
        )

    def _save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶åˆ°å®éªŒç›®å½•"""
        config_save_path = Path(self.config.outputs.experiment_dir) / "config.yaml"

        # æ·»åŠ è¿è¡Œæ—¶ä¿¡æ¯
        runtime_info = {
            "runtime": {
                "task": self.task,
                "start_time": datetime.now().isoformat(),
                "config_path": str(self.config_path),
                "command_line_args": self.task_kwargs,
                "pytorch_version": str(torch.__version__),
                "pytorch_lightning_version": str(pl.__version__),
            }
        }

        # åˆå¹¶é…ç½®
        enhanced_config = OmegaConf.merge(self.config, runtime_info)

        # ä¿å­˜
        with open(config_save_path, "w") as f:
            OmegaConf.save(enhanced_config, f)

        logger.info(f"ğŸ“„ Configuration saved to: {config_save_path}")

    def run(self) -> Dict[str, Any]:
        """è¿è¡Œå®éªŒçš„ä¸»å…¥å£"""
        logger.info(f"ğŸš€ Starting task: {self.task}")
        self._print_experiment_banner()

        try:
            if self.task == "train":
                return self._run_standard_training()
            elif self.task == "predict":
                return self._run_prediction()
            elif self.task == "kfold":
                return self._run_kfold_training()
            elif self.task == "active_train":
                return self._run_active_training()
            elif self.task == "active_kfold":
                return self._run_active_kfold_training()
            else:
                raise ValueError(f"Unknown task: {self.task}")

        except Exception as e:
            logger.error(f"âŒ Task '{self.task}' failed: {str(e)}")
            raise

    def _run_standard_training(self) -> Dict[str, Any]:
        """è¿è¡Œæ ‡å‡†è®­ç»ƒ"""
        logger.info("ğŸ¯ Running standard training...")

        # åˆ›å»ºç»„ä»¶
        model = instantiate_from_config(self.config.model)
        datamodule = instantiate_from_config(self.config.data)
        trainer = self._create_standard_trainer()

        # å¼€å§‹è®­ç»ƒ
        logger.info("ğŸš€ Starting training...")
        trainer.fit(model, datamodule)
        """
            fit æ˜¯ pytorch_lightning.Trainer ç±»çš„æ ¸å¿ƒæ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•ä¼šè‡ªåŠ¨æ‰§è¡Œæ•´ä¸ªè®­ç»ƒæµç¨‹ï¼š

            æ•°æ®å‡†å¤‡ï¼šè°ƒç”¨ data_module.prepare_data() å’Œ data_module.setup()
            åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼šè·å– train/val dataloaders
            è®­ç»ƒå¾ªç¯ï¼š

            è°ƒç”¨ model.training_step() å¤„ç†æ¯ä¸ªbatch
            è®¡ç®—losså’Œæ¢¯åº¦
            æ‰§è¡Œä¼˜åŒ–å™¨æ›´æ–°

            éªŒè¯å¾ªç¯ï¼š

            è°ƒç”¨ model.validation_step()
            è®¡ç®—éªŒè¯æŒ‡æ ‡

            å›è°ƒæ‰§è¡Œï¼šè¿è¡Œcheckpointingã€early stoppingç­‰
            æ—¥å¿—è®°å½•ï¼šè‡ªåŠ¨è®°å½•æ‰€æœ‰æŒ‡æ ‡
            # Lightningå†…éƒ¨ä¼šè°ƒç”¨
            â”œâ”€â”€ data_module.prepare_data()           # æ•°æ®å‡†å¤‡
            â”œâ”€â”€ data_module.setup('fit')             # æ•°æ®é›†è®¾ç½®  
            â”œâ”€â”€ data_module.train_dataloader()       # è·å–è®­ç»ƒæ•°æ®åŠ è½½å™¨
            â”œâ”€â”€ data_module.val_dataloader()         # è·å–éªŒè¯æ•°æ®åŠ è½½å™¨
            â”œâ”€â”€ model.configure_optimizers()         # é…ç½®ä¼˜åŒ–å™¨
            â””â”€â”€ è®­ç»ƒå¾ªç¯:
                â”œâ”€â”€ model.training_step(batch, idx)  # æ¯ä¸ªè®­ç»ƒbatch
                â”œâ”€â”€ optimizer.step()                 # å‚æ•°æ›´æ–°
                â”œâ”€â”€ model.validation_step(batch, idx) # æ¯ä¸ªéªŒè¯batch
                â”œâ”€â”€ callbacks.on_epoch_end()         # å›è°ƒå‡½æ•°
                â””â”€â”€ logger.log_metrics()             # è®°å½•æŒ‡æ ‡
        """

        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        test_results = trainer.test(model, datamodule, verbose=False)

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = Path(self.config.outputs.model_dir) / "final_model.ckpt"
        trainer.save_checkpoint(str(final_model_path))

        return {
            "best_checkpoint": trainer.checkpoint_callback.best_model_path,
            "final_model": str(final_model_path),
            "test_results": test_results[0] if test_results else {},
            "training_completed": True,
        }

    def _run_kfold_training(self) -> Dict[str, Any]:
        """è¿è¡ŒKæŠ˜äº¤å‰éªŒè¯è®­ç»ƒ"""
        logger.info("ğŸ”„ Running K-fold cross-validation...")

        # ä½¿ç”¨ç°æœ‰çš„SimpleKFoldTrainer
        kfold_trainer = SimpleKFoldTrainer(
            config=dict(self.config),
            experiment_name=self.config.experiment_name,
            output_dir=self.config.outputs.experiment_dir,
        )

        return kfold_trainer.run_kfold_training()

    def _run_active_training(self) -> Dict[str, Any]:
        """è¿è¡Œä¸»åŠ¨å­¦ä¹ è®­ç»ƒ - æ”¯æŒäººå·¥æŒ‡å¯¼"""
        logger.info("ğŸ¯ğŸ·ï¸ Running Active Learning + Pseudo Labeling...")

        # æ£€æŸ¥æ ‡æ³¨æ¨¡å¼
        annotation_mode = self.config.get("active_pseudo_learning", {}).get("annotation_mode", "simulated")

        if annotation_mode == "human":
            logger.info("ğŸ‘¤ Using HUMAN-GUIDED active learning")
            # ä½¿ç”¨äººå·¥æŒ‡å¯¼å®ç°
            trainer = create_human_guided_active_learning(
                config=dict(self.config),
                experiment_name=self.config.experiment_name,
                output_dir=self.config.outputs.experiment_dir,
            )
        else:
            # ç»“æŸç¨‹åº
            raise ValueError("Human-guided active learning is not implemented yet")

        # è¿è¡Œä¸»åŠ¨å­¦ä¹ æµç¨‹
        results = trainer.run()

        return {
            "active_learning_results": results,
            "annotation_mode": annotation_mode,
            "training_completed": True,
        }

    def _run_active_kfold_training(self) -> Dict[str, Any]:
        """è¿è¡Œä¸»åŠ¨å­¦ä¹ +KæŠ˜äº¤å‰éªŒè¯èåˆè®­ç»ƒ"""
        logger.info("ğŸ”„ğŸ¯ğŸ·ï¸ Running Active Learning + K-fold Cross-validation...")

        # è¿™æ˜¯ä¸€ä¸ªæ›´å¤æ‚çš„ç»„åˆç­–ç•¥
        # æˆ‘ä»¬å°†åœ¨æ¯ä¸ªfoldä¸­éƒ½åº”ç”¨ä¸»åŠ¨å­¦ä¹ 
        from lightning_landslide.src.training.active_kfold_trainer import ActiveKFoldTrainer

        active_kfold_trainer = ActiveKFoldTrainer(
            config=dict(self.config),
            experiment_name=self.config.experiment_name,
            output_dir=self.config.outputs.experiment_dir,
        )

        return active_kfold_trainer.run()

    def _run_prediction(self) -> Dict[str, Any]:
        """è¿è¡Œé¢„æµ‹ä»»åŠ¡"""
        logger.info("ğŸ”® Running prediction...")

        # åŠ è½½æ¨¡å‹
        checkpoint_path = self.config.get("checkpoint_path")
        if not checkpoint_path:
            raise ValueError("checkpoint_path is required for prediction task")

        model = instantiate_from_config(self.config.model)
        model = model.load_from_checkpoint(checkpoint_path)

        # åˆ›å»ºæ•°æ®æ¨¡å—
        datamodule = instantiate_from_config(self.config.data)

        # åˆ›å»ºé¢„æµ‹å™¨
        trainer = self._create_standard_trainer()

        # è¿›è¡Œé¢„æµ‹
        predictions = trainer.predict(model, datamodule)

        # ä¿å­˜é¢„æµ‹ç»“æœ
        prediction_path = Path(self.config.outputs.prediction_dir) / "predictions.csv"
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„é¢„æµ‹æ ¼å¼æ¥ä¿å­˜

        return {
            "prediction_path": str(prediction_path),
            "num_predictions": len(predictions) if predictions else 0,
            "prediction_completed": True,
        }

    def _create_standard_trainer(self) -> pl.Trainer:
        """åˆ›å»ºæ ‡å‡†PyTorch Lightningè®­ç»ƒå™¨"""
        from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
        from pytorch_lightning.loggers import TensorBoardLogger

        # åŸºç¡€è®­ç»ƒå™¨é…ç½®
        trainer_config = dict(self.config.trainer.params)

        # è®¾ç½®å›è°ƒ
        callbacks = []

        # æ¨¡å‹æ£€æŸ¥ç‚¹å›è°ƒ
        checkpoint_callback = ModelCheckpoint(
            dirpath=self.config.outputs.checkpoint_dir,
            filename="{epoch}-{val_f1:.4f}",
            monitor="val_f1",
            mode="max",
            save_top_k=3,
            save_last=True,
        )
        callbacks.append(checkpoint_callback)

        # æ—©åœå›è°ƒ
        early_stopping = EarlyStopping(
            monitor="val_f1",
            patience=15,
            mode="max",
            verbose=True,
        )
        callbacks.append(early_stopping)

        # æ—¥å¿—è®°å½•å™¨
        tb_logger = TensorBoardLogger(
            save_dir=self.config.outputs.log_dir,
            name="training",
            version="",
        )

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = pl.Trainer(**trainer_config)
        trainer.callbacks = callbacks
        trainer.logger = tb_logger

        return trainer

    def _print_experiment_banner(self):
        """æ‰“å°å®éªŒä¿¡æ¯æ¨ªå¹…"""
        print("\n" + "=" * 80)
        print(f"ğŸ§ª EXPERIMENT: {self.config.experiment_name}")
        print(f"ğŸ“‹ TASK: {self.task.upper()}")
        print(f"â° START TIME: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“ OUTPUT DIR: {self.config.outputs.experiment_dir}")

        if "model" in self.config:
            model_name = self.config.model.get("target", "Unknown").split(".")[-1]
            print(f"ğŸ¤– MODEL: {model_name}")

        if "data" in self.config:
            data_dir = self.config.data.get("params", {}).get("train_data_dir", "N/A")
            print(f"ğŸ“Š DATA: {data_dir}")

        # ä¸»åŠ¨å­¦ä¹ ç‰¹å®šä¿¡æ¯
        if self.task in ["active_train", "active_kfold"] and "active_pseudo_learning" in self.config:
            apl_config = self.config.active_pseudo_learning
            print(f"ğŸ¯ MAX ITERATIONS: {apl_config.get('max_iterations', 5)}")
            print(f"ğŸ·ï¸ PSEUDO THRESHOLD: {apl_config.get('pseudo_labeling', {}).get('confidence_threshold', 0.9)}")
            print(f"ğŸ“ ANNOTATION BUDGET: {apl_config.get('annotation_budget', 50)}")

        print("=" * 80 + "\n")


def create_parser() -> argparse.ArgumentParser:
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="MM-LandslideNet: Enhanced Deep Learning Framework with Active Learning",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # æ ‡å‡†è®­ç»ƒ
  python main.py train lightning_landslide/configs/optical_baseline.yaml
  
  # KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒ  
  python main.py kfold lightning_landslide/configs/optical_baseline_5-fold.yaml
  
  # ä¸»åŠ¨å­¦ä¹ +ä¼ªæ ‡ç­¾è®­ç»ƒ
  python main.py active_train lightning_landslide/configs/optical_baseline_active.yaml
  
  # ä¸»åŠ¨å­¦ä¹ +KæŠ˜äº¤å‰éªŒè¯
  python main.py active_kfold configs/optical_baseline_active_kfold.yaml
  
  # é¢„æµ‹
  python main.py predict configs/predict_config.yaml --checkpoint_path path/to/model.ckpt
        """,
    )

    parser.add_argument(
        "task",
        choices=["train", "predict", "kfold", "active_train", "active_kfold"],
        help="Task to execute",
    )

    parser.add_argument("config", type=str, help="Path to configuration file")

    # é€šç”¨å‚æ•°
    parser.add_argument("--experiment_name", type=str, help="Override experiment name")
    parser.add_argument("--checkpoint_path", type=str, help="Checkpoint path for prediction")

    # KæŠ˜ç‰¹å®šå‚æ•°
    parser.add_argument("--n_splits", type=int, help="Number of folds for K-fold CV")

    # ä¸»åŠ¨å­¦ä¹ ç‰¹å®šå‚æ•°
    parser.add_argument("--max_iterations", type=int, help="Maximum active learning iterations")
    parser.add_argument("--annotation_budget", type=int, help="Annotation budget per iteration")
    parser.add_argument("--pseudo_threshold", type=float, help="Pseudo label confidence threshold")

    return parser


def main():
    """
    ä¸»å‡½æ•° - ç¨‹åºå…¥å£

    å¢å¼ºç‰ˆæœ¬æ”¯æŒæ›´å¤šä»»åŠ¡ç±»å‹ï¼ŒåŒæ—¶ä¿æŒå‘åå…¼å®¹æ€§ã€‚
    """
    parser = create_parser()
    args = parser.parse_args()

    # å‡†å¤‡ä»»åŠ¡å‚æ•°
    task_kwargs = {}
    if args.experiment_name:
        task_kwargs["experiment_name"] = args.experiment_name
    if args.checkpoint_path:
        task_kwargs["checkpoint_path"] = args.checkpoint_path
    if args.n_splits:
        task_kwargs["n_splits"] = args.n_splits
    if args.max_iterations:
        task_kwargs["max_iterations"] = args.max_iterations
    if args.annotation_budget:
        task_kwargs["annotation_budget"] = args.annotation_budget
    if args.pseudo_threshold:
        task_kwargs["pseudo_threshold"] = args.pseudo_threshold

    # åˆ›å»ºå¹¶è¿è¡Œå®éªŒ
    try:
        runner = ExperimentRunner(args.config, args.task, **task_kwargs)
        results = runner.run()

        # æŠ¥å‘Šç»“æœ
        print(f"\nğŸ‰ Task '{args.task}' completed successfully!")

        if args.task == "kfold":
            if "mean_cv_score" in results:
                print(f"ğŸ“ˆ Mean CV Score: {results['mean_cv_score']:.4f} Â± {results['std_cv_score']:.4f}")

        elif args.task in ["active_train", "active_kfold"]:
            if "best_performance" in results:
                print(f"ğŸ† Best Performance: {results['best_performance']:.4f}")
            if "total_iterations" in results:
                print(f"ğŸ”„ Total Iterations: {results['total_iterations']}")

        elif args.task == "train":
            if results.get("best_checkpoint"):
                print(f"ğŸ“ Best model: {results['best_checkpoint']}")

        elif args.task == "predict":
            if results.get("prediction_path"):
                print(f"ğŸ“„ Predictions saved: {results['prediction_path']}")

    except Exception as e:
        print(f"\nâŒ Task failed: {str(e)}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
