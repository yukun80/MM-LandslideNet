import os
import sys
import argparse
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
import warnings
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# æŠ‘åˆ¶ä¸å¿…è¦çš„è­¦å‘Š
# warnings.filterwarnings("ignore", ".*does not have many workers.*")
# warnings.filterwarnings("ignore", ".*The dataloader.*")

import torch
import pytorch_lightning as pl
from omegaconf import DictConfig, OmegaConf

# å¯¼å…¥æˆ‘ä»¬çš„æ ¸å¿ƒå·¥å…·
from lightning_landslide.src.utils.instantiate import instantiate_from_config, validate_config_structure
from lightning_landslide.src.utils.logging_utils import setup_logging, get_project_logger
from lightning_landslide.src.training.simple_kfold_trainer import SimpleKFoldTrainer

logger = get_project_logger(__name__)


class ExperimentRunner:
    """
    å¢å¼ºçš„å®éªŒè¿è¡Œå™¨ - æ”¯æŒKæŠ˜äº¤å‰éªŒè¯

    æ–°å¢åŠŸèƒ½ï¼š
    1. æ”¯æŒkfoldä»»åŠ¡ç±»å‹
    2. ä¿æŒç°æœ‰æ¶æ„çš„ç®€æ´æ€§
    3. å§”æ‰˜å¤æ‚é€»è¾‘ç»™ä¸“é—¨çš„KFoldTrainer
    """

    def __init__(self, config_path: str, task: str = "train", **kwargs):
        """
        åˆå§‹åŒ–å®éªŒè¿è¡Œå™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            task: è¦æ‰§è¡Œçš„ä»»åŠ¡ç±»å‹ï¼ˆtrain/predict/kfoldï¼‰
            **kwargs: é¢å¤–çš„ä»»åŠ¡å‚æ•°
        """
        setup_logging(level=logging.INFO)
        self.config_path = Path(config_path)
        self.task = task
        self.task_kwargs = kwargs  # å­˜å‚¨é¢å¤–çš„ä»»åŠ¡å‚æ•°
        self.config = self._load_config()
        self._setup_environment()

    def _load_config(self) -> DictConfig:
        """åŠ è½½å’ŒéªŒè¯é…ç½®æ–‡ä»¶"""
        if not self.config_path.exists():
            raise FileNotFoundError(f"Config file not found: {self.config_path}")

        logger.info(f"Loading config from: {self.config_path}")
        config = OmegaConf.load(self.config_path)

        # éªŒè¯é…ç½®ç»“æ„
        if not validate_config_structure(config):
            raise ValueError("Invalid configuration structure")

        logger.info("âœ“ Configuration loaded and validated")
        return config

    def _setup_environment(self):
        """è®¾ç½®å®éªŒç¯å¢ƒ"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self._create_output_dirs()

        # è®¾ç½®æ—¥å¿—ï¼Œgetattrçš„ä½œç”¨æ˜¯è·å–configä¸­çš„log_levelï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨INFO
        log_level = getattr(logging, self.config.get("log_level", "INFO").upper())
        log_file = None

        if "outputs" in self.config and "log_dir" in self.config.outputs:
            log_file = Path(self.config.outputs.log_dir) / f"{self.config.experiment_name}.log"

        setup_logging(
            level=log_level,
            log_file=str(log_file) if log_file else None,
            use_colors=True,
        )

        # è®¾ç½®éšæœºç§å­ï¼Œseed_everythingçš„ä½œç”¨æ˜¯è®¾ç½®éšæœºç§å­ï¼Œå¹¶è®¾ç½®torch.manual_seedå’Œtorch.cuda.manual_seedï¼Œ
        # workersä¸ºTrueæ—¶ï¼Œä¼šè®¾ç½®torch.utils.data.DataLoaderçš„num_workersä¸º1
        if "seed" in self.config:
            pl.seed_everything(self.config.seed, workers=True)

        # ä¿å­˜é…ç½®æ–‡ä»¶åˆ°å®éªŒç›®å½•
        self._save_config()

    def _create_output_dirs(self):
        """æ ¹æ®experiment_nameåŠ¨æ€åˆ›å»ºå®éªŒè¾“å‡ºç›®å½•"""
        base_dir = Path(self.config.outputs.base_output_dir)
        experiment_path = base_dir / self.config.experiment_name
        logger.info(f"æ‰€æœ‰å®éªŒè¾“å‡ºå°†ä¿å­˜åˆ°: {experiment_path}")

        # 2. éå†æ‰€æœ‰å­ç›®å½•é…ç½®ï¼Œåˆ›å»ºç›®å½•å¹¶æ›´æ–°é…ç½®
        # .items()æ˜¯å­—å…¸çš„ä¸€ä¸ªæ–¹æ³•ï¼Œå®ƒä¼šæŠŠå­—å…¸é‡Œçš„æ¯ä¸€å¯¹â€œé”® (key)â€å’Œâ€œå€¼ (value)â€æ‹¿å‡ºæ¥ï¼Œç»„æˆä¸€ä¸ªä¸€ä¸ªçš„å…ƒç»„ (tuple)ã€‚
        for key, subdir_name in list(self.config.outputs.items()):
            if key.endswith("_subdir"):
                # æ„å»ºå®Œæ•´çš„ç›®å½•è·¯å¾„
                full_path = experiment_path / subdir_name
                full_path.mkdir(parents=True, exist_ok=True)

                # ç”Ÿæˆæ–°çš„é…ç½®é”® (ä¾‹å¦‚, 'checkpoint_subdir' -> 'checkpoint_dir')
                new_key = key.replace("_subdir", "_dir")

                # å°†åŠ¨æ€ç”Ÿæˆçš„å®Œæ•´è·¯å¾„æ›´æ–°å›é…ç½®å¯¹è±¡
                OmegaConf.update(self.config.outputs, new_key, str(full_path))

                logger.debug(f"åˆ›å»ºå¹¶é…ç½®ç›®å½•: {new_key} = {full_path}")

        # 3. æ¸…ç†æ—§çš„ subdir é…ç½®ï¼ˆå¯é€‰ï¼Œä½†ä¿æŒé…ç½®æ•´æ´ï¼‰
        for key in list(self.config.outputs.keys()):
            if key.endswith("_subdir"):
                del self.config.outputs[key]

    def _save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶åˆ°å®éªŒç›®å½•ï¼ˆç¡®ä¿å¯é‡ç°æ€§ï¼‰ï¼Œå¦‚æœoutputsåœ¨configä¸­ï¼Œåˆ™ä¿å­˜config.yamlåˆ°outputs.log_dirç›®å½•ä¸‹ï¼Œ
        å¦‚æœoutputs.log_dirä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºlog_dirç›®å½•"""
        if "outputs" in self.config and "log_dir" in self.config.outputs:
            timestamp = datetime.now().strftime(self.config.outputs.get("timestamp_format", "%Y%m%d_%H%M%S"))
            config_save_path = Path(self.config.outputs.log_dir) / f"config_{timestamp}.yaml"
            config_save_path.parent.mkdir(parents=True, exist_ok=True)

            with open(config_save_path, "w") as f:
                OmegaConf.save(self.config, f)

            logger.info(f"Config saved to: {config_save_path}")

    def run(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®éªŒçš„ä¸»æ–¹æ³• - æ‰©å±•æ”¯æŒKæŠ˜äº¤å‰éªŒè¯

        Returns:
            å®éªŒç»“æœå­—å…¸
        """
        logger.info(f"ğŸš€ Starting {self.task} task")
        self._print_experiment_info()

        # æ‰©å±•çš„ä»»åŠ¡æ–¹æ³•æ˜ å°„
        task_methods = {
            "train": self._run_training,
            "predict": self._run_prediction,
            "kfold": self._run_kfold,  # æ–°å¢KæŠ˜ä»»åŠ¡
        }

        if self.task not in task_methods:
            raise ValueError(f"Unknown task: {self.task}. Available: {list(task_methods.keys())}")

        return task_methods[self.task]()

    def _run_training(self) -> Dict[str, Any]:
        """æ‰§è¡Œæ ‡å‡†è®­ç»ƒä»»åŠ¡"""
        logger.info("Initializing training components...")

        # åˆ›å»ºæ¨¡å‹å’Œæ•°æ®æ¨¡å—
        model = instantiate_from_config(self.config.model)
        data_module = instantiate_from_config(self.config.data)

        # å¤„ç†traineré…ç½®
        trainer_config = self.config.trainer.copy()
        callbacks = self._create_callbacks()
        loggers = self._create_loggers()

        # åˆ›å»ºtrainer
        trainer = instantiate_from_config(trainer_config)

        if callbacks:
            trainer.callbacks = callbacks
        if loggers:
            trainer.logger = loggers[0] if len(loggers) == 1 else loggers

        # å¼€å§‹è®­ç»ƒ
        logger.info("ğŸš€ Starting training...")
        trainer.fit(model, data_module)
        """
            fit æ˜¯ pytorch_lightning.Trainer ç±»çš„æ ¸å¿ƒæ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•ä¼šè‡ªåŠ¨æ‰§è¡Œæ•´ä¸ªè®­ç»ƒæµç¨‹ï¼š

            æ•°æ®å‡†å¤‡ï¼šè°ƒç”¨ data_module.prepare_data() å’Œ data_module.setup()
            åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼šè·å– train/val dataloaders
            è®­ç»ƒå¾ªç¯ï¼š

            è°ƒç”¨ model.training_step() å¤„ç†æ¯ä¸ªbatch
            è®¡ç®—losså’Œæ¢¯åº¦
            æ‰§è¡Œä¼˜åŒ–å™¨æ›´æ–°

            éªŒè¯å¾ªç¯ï¼š

            è°ƒç”¨ model.validation_step()
            è®¡ç®—éªŒè¯æŒ‡æ ‡

            å›è°ƒæ‰§è¡Œï¼šè¿è¡Œcheckpointingã€early stoppingç­‰
            æ—¥å¿—è®°å½•ï¼šè‡ªåŠ¨è®°å½•æ‰€æœ‰æŒ‡æ ‡
            # Lightningå†…éƒ¨ä¼šè°ƒç”¨
            â”œâ”€â”€ data_module.prepare_data()           # æ•°æ®å‡†å¤‡
            â”œâ”€â”€ data_module.setup('fit')             # æ•°æ®é›†è®¾ç½®  
            â”œâ”€â”€ data_module.train_dataloader()       # è·å–è®­ç»ƒæ•°æ®åŠ è½½å™¨
            â”œâ”€â”€ data_module.val_dataloader()         # è·å–éªŒè¯æ•°æ®åŠ è½½å™¨
            â”œâ”€â”€ model.configure_optimizers()         # é…ç½®ä¼˜åŒ–å™¨
            â””â”€â”€ è®­ç»ƒå¾ªç¯:
                â”œâ”€â”€ model.training_step(batch, idx)  # æ¯ä¸ªè®­ç»ƒbatch
                â”œâ”€â”€ optimizer.step()                 # å‚æ•°æ›´æ–°
                â”œâ”€â”€ model.validation_step(batch, idx) # æ¯ä¸ªéªŒè¯batch
                â”œâ”€â”€ callbacks.on_epoch_end()         # å›è°ƒå‡½æ•°
                â””â”€â”€ logger.log_metrics()             # è®°å½•æŒ‡æ ‡
        """

        return {
            "status": "completed",
            "trainer": trainer,
            "model": model,
            "best_checkpoint": self._get_best_checkpoint_path(trainer),
        }

    def _run_kfold(self) -> Dict[str, Any]:
        """
        æ‰§è¡ŒKæŠ˜äº¤å‰éªŒè¯ä»»åŠ¡ - é‡æ„ç‰ˆæœ¬

        æˆ‘ä»¬ä¸å†éœ€è¦å¤æ‚çš„é…ç½®è§£æå’Œå‚æ•°ä¼ é€’ï¼Œ
        è€Œæ˜¯ç›´æ¥å°†å®Œæ•´é…ç½®ä¼ é€’ç»™ä¸“é—¨çš„KæŠ˜è®­ç»ƒå™¨ã€‚

        å°±åƒæŠŠæ•´ä¸ªèœè°±äº¤ç»™ä¸“ä¸šå¨å¸ˆï¼Œè€Œä¸æ˜¯é€ä¸ªè§£é‡Šæ¯ä¸ªæ­¥éª¤ã€‚
        """
        logger.info("ğŸ¯ Initializing K-Fold Cross Validation...")

        # éªŒè¯é…ç½®å®Œæ•´æ€§
        self._validate_kfold_config()

        # è·å–å®éªŒè¾“å‡ºè·¯å¾„ - ä¸åŸºç¡€è®­ç»ƒå®Œå…¨ä¸€è‡´çš„é€»è¾‘
        experiment_output_dir = self._get_experiment_output_dir()

        logger.info(f"ğŸ“ K-fold experiment will be saved to: {experiment_output_dir}")

        # åˆ›å»ºå¹¶è¿è¡ŒKæŠ˜è®­ç»ƒå™¨
        kfold_trainer = SimpleKFoldTrainer(
            config=self.config,
            experiment_name=self.config.get("experiment_name", "kfold_experiment"),
            output_dir=str(experiment_output_dir),  # ä½¿ç”¨ä¸åŸºç¡€è®­ç»ƒä¸€è‡´çš„è·¯å¾„
        )

        # æ‰§è¡ŒKæŠ˜è®­ç»ƒ
        results = kfold_trainer.run_kfold_training()

        # æ‰“å°ç»“æœæ‘˜è¦
        self._print_kfold_summary(results)

        return results

    def _get_experiment_output_dir(self) -> Path:
        """
        è·å–å®éªŒè¾“å‡ºç›®å½• - ä¸åŸºç¡€è®­ç»ƒä¿æŒå®Œå…¨ä¸€è‡´

        è¿™ä¸ªæ–¹æ³•å¤ç”¨äº†åŸºç¡€è®­ç»ƒä¸­_create_output_dirs()çš„æ ¸å¿ƒé€»è¾‘ï¼Œ
        ç¡®ä¿k-foldè®­ç»ƒçš„è¾“å‡ºè·¯å¾„ä¸æ ‡å‡†è®­ç»ƒå®Œå…¨ä¸€è‡´ã€‚

        è®¾è®¡æ€æƒ³ï¼š
        å°±åƒå·¥å‚ä¸­çš„æ ‡å‡†åŒ–ç”Ÿäº§æµç¨‹ï¼Œæ— è®ºç”Ÿäº§ä»€ä¹ˆäº§å“ï¼Œ
        éƒ½éµå¾ªç›¸åŒçš„è´¨é‡æ ‡å‡†å’Œå·¥åºè§„èŒƒã€‚

        Returns:
            å®éªŒè¾“å‡ºç›®å½•çš„å®Œæ•´è·¯å¾„
        """
        # 1. è·å–åŸºç¡€è¾“å‡ºç›®å½• - ä¸åŸºç¡€è®­ç»ƒç›¸åŒçš„é€»è¾‘
        base_dir = Path(self.config.outputs.base_output_dir)

        # 2. ä½¿ç”¨å®éªŒåç§°æ„å»ºå®Œæ•´è·¯å¾„ - ä¸åŸºç¡€è®­ç»ƒç›¸åŒçš„é€»è¾‘
        experiment_path = base_dir / self.config.experiment_name

        # 3. ç¡®ä¿ç›®å½•å­˜åœ¨
        experiment_path.mkdir(parents=True, exist_ok=True)

        logger.debug(f"ğŸ“‚ Experiment output directory: {experiment_path}")

        return experiment_path

    def _validate_kfold_config(self):
        """
        éªŒè¯KæŠ˜é…ç½®çš„å®Œæ•´æ€§

        è¿™ä¸ªæ–¹æ³•ç¡®ä¿ç”¨æˆ·æä¾›çš„é…ç½®åŒ…å«æ‰€æœ‰å¿…éœ€çš„éƒ¨åˆ†ã€‚
        å°±åƒæ£€æŸ¥èœè°±æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€çš„ææ–™å’Œæ­¥éª¤ã€‚
        """
        required_sections = ["model", "data", "trainer"]
        missing_sections = []

        for section in required_sections:
            if section not in self.config:
                missing_sections.append(section)

        if missing_sections:
            raise ValueError(
                f"K-fold training requires the following config sections: {missing_sections}. "
                f"Please ensure your config file includes all required sections."
            )

        # å¦‚æœæ²¡æœ‰kfoldé…ç½®ï¼Œæˆ‘ä»¬æ·»åŠ é»˜è®¤é…ç½®
        if "kfold" not in self.config:
            logger.info("ğŸ”§ No 'kfold' section found in config, using defaults")
            self.config["kfold"] = {
                "n_splits": 5,
                "stratified": True,
                "primary_metric": "f1",
                "save_oof_predictions": True,
                "save_fold_models": True,
            }

        logger.info("âœ… K-fold configuration validated")

    def _print_kfold_summary(self, results: Dict[str, Any]):
        """
        æ‰“å°KæŠ˜è®­ç»ƒç»“æœæ‘˜è¦

        è¿™ä¸ªæ–¹æ³•ä¸ºç”¨æˆ·æä¾›æ¸…æ™°ã€æ˜“è¯»çš„ç»“æœæ‘˜è¦ã€‚
        å°±åƒä¸ºä¸€åœºéŸ³ä¹ä¼šåˆ¶ä½œèŠ‚ç›®å•ï¼Œçªå‡ºæœ€é‡è¦çš„ä¿¡æ¯ã€‚
        """
        print("\n" + "=" * 80)
        print("ğŸ‰ K-FOLD CROSS VALIDATION COMPLETED")
        print("=" * 80)

        print(f"ğŸ“Š Experiment: {results['experiment_name']}")
        print(f"ğŸ¯ {results['n_splits']}-Fold Cross Validation")
        print(f"â±ï¸  Total Time: {results['total_time']:.2f}s")
        print(
            f"ğŸ“ˆ Primary Metric ({results['primary_metric']}): {results['mean_cv_score']:.4f} Â± {results['std_cv_score']:.4f}"
        )

        print(f"\nğŸ“‹ Individual Fold Results:")
        for i, score in enumerate(results["cv_scores"]):
            print(f"   Fold {i+1}: {score:.4f}")

        print(f"\nğŸ“ Results saved to: outputs/kfold_experiments/{results['experiment_name']}")
        print("=" * 80 + "\n")

    def _run_prediction(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ¨ç†ä»»åŠ¡

        æ¨ç†ä»»åŠ¡ç”¨äºåœ¨æ–°æ•°æ®ä¸Šç”Ÿæˆé¢„æµ‹ç»“æœã€‚å®ƒç‰¹åˆ«é€‚ç”¨äºï¼š
        1. ç”Ÿæˆç«èµ›æäº¤æ–‡ä»¶
        2. å¯¹æ–°çš„é¥æ„Ÿå›¾åƒè¿›è¡Œæ»‘å¡æ£€æµ‹
        3. æ‰¹é‡å¤„ç†å¤§é‡å›¾åƒ

        Returns:
            åŒ…å«é¢„æµ‹ç»“æœå’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        logger.info("ğŸ”® Initializing prediction task...")

        # éªŒè¯å¿…éœ€çš„é…ç½®
        if "checkpoint_path" not in self.config:
            raise ValueError("Prediction requires 'checkpoint_path' in config")

        checkpoint_path = self.config.checkpoint_path
        if not Path(checkpoint_path).exists():
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

        logger.info(f"Loading model from: {checkpoint_path}")

        # åˆ›å»ºç»„ä»¶
        model = instantiate_from_config(self.config.model)
        data_module = instantiate_from_config(self.config.data)

        # ä¸ºæ¨ç†ä»»åŠ¡é…ç½®trainer
        trainer_config = self.config.trainer.copy()
        trainer_config.params.update(
            {
                "logger": False,
                "enable_checkpointing": False,
                "enable_progress_bar": True,
            }
        )
        trainer = instantiate_from_config(trainer_config)

        # è¿è¡Œé¢„æµ‹
        logger.info("ğŸ¯ Generating predictions...")
        predictions = trainer.predict(model, data_module, ckpt_path=checkpoint_path)

        # å¤„ç†å’Œä¿å­˜é¢„æµ‹ç»“æœ
        processed_predictions = self._process_predictions(predictions)
        output_files = self._save_predictions(processed_predictions)

        logger.info("âœ… Prediction completed successfully!")
        return {
            "status": "completed",
            "predictions": processed_predictions,
            "output_files": output_files,
            "checkpoint_used": checkpoint_path,
            "num_samples": len(processed_predictions) if processed_predictions else 0,
        }

    def _process_predictions(self, raw_predictions: List) -> List[Dict]:
        """
        å¤„ç†åŸå§‹é¢„æµ‹ç»“æœ

        å°†PyTorchå¼ é‡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼ï¼Œå¹¶æ·»åŠ å¿…è¦çš„å…ƒæ•°æ®ã€‚

        Args:
            raw_predictions: trainer.predict()çš„åŸå§‹è¿”å›ç»“æœ

        Returns:
            å¤„ç†åçš„é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        import torch
        import numpy as np

        processed = []

        for batch_idx, batch_predictions in enumerate(raw_predictions):
            # å¦‚æœé¢„æµ‹ç»“æœæ˜¯å¼ é‡ï¼Œè½¬æ¢ä¸ºnumpyæ•°ç»„
            if isinstance(batch_predictions, torch.Tensor):
                predictions_np = batch_predictions.cpu().numpy()
            else:
                predictions_np = batch_predictions

            # å¤„ç†æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹
            for sample_idx, prediction in enumerate(predictions_np):
                processed.append(
                    {
                        "batch_idx": batch_idx,
                        "sample_idx": sample_idx,
                        "prediction": float(prediction) if np.isscalar(prediction) else prediction.tolist(),
                        "probability": (
                            float(torch.sigmoid(torch.tensor(prediction)).item()) if np.isscalar(prediction) else None
                        ),
                    }
                )

        logger.info(f"Processed {len(processed)} predictions")
        return processed

    def _save_predictions(self, predictions: List[Dict]) -> Dict[str, Path]:
        """
        ä¿å­˜é¢„æµ‹ç»“æœåˆ°å¤šç§æ ¼å¼çš„æ–‡ä»¶

        Args:
            predictions: å¤„ç†åçš„é¢„æµ‹ç»“æœ

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        import json
        import pandas as pd
        from datetime import datetime

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config.outputs.get("predictions_dir", "outputs/predictions"))
        output_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"predictions_{self.config.experiment_name}_{timestamp}"

        output_files = {}

        # ä¿å­˜JSONæ ¼å¼ï¼ˆå®Œæ•´ä¿¡æ¯ï¼‰
        json_file = output_dir / f"{base_filename}.json"
        with open(json_file, "w") as f:
            json.dump(
                {
                    "experiment_name": self.config.experiment_name,
                    "timestamp": timestamp,
                    "checkpoint_path": self.config.get("checkpoint_path"),
                    "num_predictions": len(predictions),
                    "predictions": predictions,
                },
                f,
                indent=2,
            )
        output_files["json"] = json_file

        # ä¿å­˜CSVæ ¼å¼ï¼ˆä¾¿äºåˆ†æï¼‰
        if predictions:
            df_data = []
            for pred in predictions:
                df_data.append(
                    {
                        "sample_id": f"sample_{pred['batch_idx']}_{pred['sample_idx']}",
                        "prediction": pred["prediction"],
                        "probability": pred.get("probability", None),
                    }
                )

            df = pd.DataFrame(df_data)
            csv_file = output_dir / f"{base_filename}.csv"
            df.to_csv(csv_file, index=False)
            output_files["csv"] = csv_file

        logger.info(f"Predictions saved to: {list(output_files.values())}")
        return output_files

    def _create_callbacks(self) -> List:
        """
        åˆ›å»ºcallbacks - ç‹¬ç«‹çš„æ–¹æ³•ï¼Œæ›´æ¸…æ™°çš„èŒè´£åˆ†ç¦»
        callbacksçš„ä½œç”¨ï¼š
        1. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®°å½•è®­ç»ƒæ—¥å¿—
        2. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¿å­˜æ¨¡å‹
        3. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹
        4. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¿å­˜éªŒè¯é›†ä¸Šçš„æœ€ä½³æ¨¡å‹
        5. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¿å­˜æµ‹è¯•é›†ä¸Šçš„æœ€ä½³æ¨¡å‹
        6. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¿å­˜è®­ç»ƒé›†ä¸Šçš„æœ€ä½³æ¨¡å‹
        """
        callbacks = []

        if "callbacks" not in self.config:
            return callbacks

        # è·å–åŠ¨æ€ç”Ÿæˆçš„è·¯å¾„
        dynamic_checkpoint_dir = self.config.outputs.get("checkpoint_dir")
        dynamic_log_dir = self.config.outputs.get("log_dir")

        logger.info("-" * 100)

        for callback_name, callback_config in self.config.callbacks.items():
            # æ·±æ‹·è´é…ç½®ï¼Œé¿å…ä¿®æ”¹åŸå§‹é…ç½®
            effective_config = callback_config.copy()

            # å¦‚æœæ˜¯ ModelCheckpoint ä¸”æ²¡æœ‰ dirpathï¼Œä½¿ç”¨åŠ¨æ€è·¯å¾„
            if callback_config.target == "pytorch_lightning.callbacks.ModelCheckpoint" and dynamic_checkpoint_dir:
                # ç›´æ¥è®¾ç½®åŠ¨æ€è·¯å¾„ï¼Œè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„é™æ€è·¯å¾„
                OmegaConf.update(effective_config, "params.dirpath", dynamic_checkpoint_dir)

            if callback_config.target == "lightning_landslide.src.utils.metrics.MetricsLogger" and dynamic_log_dir:
                OmegaConf.update(effective_config, "params.log_dir", dynamic_log_dir)

            # åˆ›å»ºcallback
            callback = instantiate_from_config(effective_config)
            callbacks.append(callback)
            logger.info(f"âœ“ Added callback: {callback_name} ({type(callback).__name__})")

        logger.info("-" * 100)
        return callbacks

    def _create_loggers(self) -> List:
        """
        åˆ›å»ºloggersï¼Œå¹¶ç¡®ä¿å®ƒä»¬ä½¿ç”¨åŠ¨æ€ç”Ÿæˆçš„è·¯å¾„ã€‚
        """
        loggers = []

        if "loggers" not in self.config:
            return loggers

        # è·å–æˆ‘ä»¬åŠ¨æ€åˆ›å»ºçš„æ—¥å¿—ç›®å½•
        dynamic_log_dir = self.config.outputs.get("log_dir")

        logger.info("-" * 100)

        for logger_name, logger_config in self.config.loggers.items():
            if logger_name == "tensorboard":
                effective_config = logger_config.copy()
                OmegaConf.update(effective_config, "params.save_dir", dynamic_log_dir)
                OmegaConf.update(effective_config, "params.name", "")
                OmegaConf.update(effective_config, "params.version", "")
                logger.info(f"Logger '{logger_name}' å°†ä½¿ç”¨åŠ¨æ€è·¯å¾„: {dynamic_log_dir}")

                # ä½¿ç”¨æ›´æ–°åçš„é…ç½®æ¥å®ä¾‹åŒ–
                lightning_logger = instantiate_from_config(effective_config)
                loggers.append(lightning_logger)
                logger.info(f"âœ“ Added logger: {logger_name} ({type(lightning_logger).__name__})")

            elif logger_name == "wandb":
                # WandBé…ç½®ä¿æŒä¸å˜
                lightning_logger = instantiate_from_config(logger_config)
                loggers.append(lightning_logger)
                logger.info(f"âœ“ Added logger: {logger_name} ({type(lightning_logger).__name__})")
        logger.info("-" * 100)
        return loggers

    def _get_best_checkpoint_path(self, trainer) -> Optional[str]:
        """è·å–æœ€ä½³æ£€æŸ¥ç‚¹è·¯å¾„"""
        for callback in trainer.callbacks:
            if isinstance(callback, pl.callbacks.ModelCheckpoint):
                return getattr(callback, "best_model_path", None)
        return None

    def _print_experiment_info(self):
        """æ‰“å°å®éªŒä¿¡æ¯"""
        print("\n" + "=" * 80)
        print(f"ğŸš€ MM-LandslideNet Experiment: {self.config.get('experiment_name', 'Unnamed')}")
        print("=" * 80)
        print(f"ğŸ“ Task: {self.task}")
        print(f"ğŸ“ Config: {self.config_path}")
        print(f"ğŸ• Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        if "model" in self.config:
            print(f"ğŸ§  Model: {self.config.model.target.split('.')[-1]}")
        if "data" in self.config:
            print(f"ğŸ“Š Data: {self.config.data.get('params', {}).get('train_data_dir', 'N/A')}")

        print("=" * 80 + "\n")


def create_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="MM-LandslideNet: Configuration-Driven Deep Learning Framework",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # æ ‡å‡†è®­ç»ƒ
  python main.py train configs/optical_baseline.yaml
  
  # KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒ  
  python main.py kfold lightning_landslide/configs/optical_baseline_5-fold.yaml
  
  # KæŠ˜è®­ç»ƒï¼Œè¦†ç›–æŠ˜æ•°
  python main.py kfold configs/optical_baseline_kfold.yaml --n_splits 10
  
  # KæŠ˜è®­ç»ƒï¼Œè‡ªå®šä¹‰å®éªŒåç§°
  python main.py kfold configs/optical_baseline_kfold.yaml --experiment_name my_kfold_experiment
        """,
    )

    parser.add_argument(
        "task",
        choices=["train", "predict", "kfold"],  # ç®€åŒ–ä»»åŠ¡é€‰æ‹©ï¼Œç§»é™¤å¤æ‚çš„kfold_predict
        help="Task to execute",
    )

    parser.add_argument("config", type=str, help="Path to configuration file")

    # KæŠ˜ç‰¹å®šå‚æ•°
    parser.add_argument("--n_splits", type=int, help="Number of folds for K-fold CV (overrides config)")
    parser.add_argument("--experiment_name", type=str, help="Override experiment name")

    return parser


def main():
    """
    ä¸»å‡½æ•°

    è¿™æ˜¯æ•´ä¸ªç¨‹åºçš„å…¥å£ç‚¹ã€‚ä¸æ‚¨åŸæ¥çš„main.pyç›¸æ¯”ï¼Œ
    æ–°ç‰ˆæœ¬çš„é€»è¾‘æå…¶ç®€æ´ï¼š
    1. è§£æå‘½ä»¤è¡Œå‚æ•°
    2. åˆ›å»ºå®éªŒè¿è¡Œå™¨
    3. è¿è¡Œå®éªŒ
    4. æŠ¥å‘Šç»“æœ

    æ‰€æœ‰çš„å¤æ‚æ€§éƒ½è¢«é…ç½®æ–‡ä»¶å’Œå®ä¾‹åŒ–å·¥å…·å¸æ”¶äº†ã€‚
    """
    parser = create_parser()  # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    args = parser.parse_args()  # è§£æå‘½ä»¤è¡Œå‚æ•°

    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = ExperimentRunner(args.config, args.task)

    # è¿è¡Œå®éªŒ
    results = runner.run()

    # æŠ¥å‘Šç»“æœ
    print(f"\nğŸ‰ Task '{args.task}' completed successfully!")

    if args.task == "kfold":
        if "mean_cv_score" in results:
            print(f"ğŸ“ˆ Mean CV Score: {results['mean_cv_score']:.4f} Â± {results['std_cv_score']:.4f}")
    elif results.get("best_checkpoint"):
        print(f"ğŸ“ Best model saved to: {results['best_checkpoint']}")


if __name__ == "__main__":
    main()
