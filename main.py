import os
import sys
import argparse
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
import warnings
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# æŠ‘åˆ¶ä¸å¿…è¦çš„è­¦å‘Š
# warnings.filterwarnings("ignore", ".*does not have many workers.*")
# warnings.filterwarnings("ignore", ".*The dataloader.*")

import torch
import pytorch_lightning as pl
from omegaconf import DictConfig, OmegaConf

# å¯¼å…¥æˆ‘ä»¬çš„æ ¸å¿ƒå·¥å…·
from lightning_landslide.src.utils.instantiate import instantiate_from_config, validate_config_structure
from lightning_landslide.src.utils.logging_utils import setup_logging, get_project_logger

logger = get_project_logger(__name__)


class ExperimentRunner:
    """
    å®éªŒè¿è¡Œå™¨

    ä¸æ‚¨åŸæ¥çš„TaskRunnerç›¸æ¯”ï¼Œè¿™ä¸ªç‰ˆæœ¬æ›´åŠ ä¸“æ³¨å’Œç®€åŒ–ï¼š
    - åªæœ‰ä¸€ä¸ªæ ¸å¿ƒèŒè´£ï¼šè¿è¡Œå®éªŒ
    - æ‰€æœ‰çš„å¤æ‚æ€§éƒ½å°è£…åœ¨é…ç½®æ–‡ä»¶ä¸­
    - ä»£ç é€»è¾‘å˜å¾—æå…¶ç®€æ´
    """

    def __init__(self, config_path: str, task: str = "train"):
        """
        åˆå§‹åŒ–å®éªŒè¿è¡Œå™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            task: è¦æ‰§è¡Œçš„ä»»åŠ¡ç±»å‹ï¼ˆtrain/predictç­‰ï¼‰
        """
        setup_logging(level=logging.INFO)
        self.config_path = Path(config_path)
        self.task = task
        self.config = self._load_config()
        self._setup_environment()

    def _load_config(self) -> DictConfig:
        """
        åŠ è½½å’ŒéªŒè¯é…ç½®æ–‡ä»¶
        """
        if not self.config_path.exists():
            raise FileNotFoundError(f"Config file not found: {self.config_path}")

        # æ‰“å°è·¯å¾„ï¼ŒåŠ è½½é…ç½®æ–‡ä»¶
        logger.info(f"Loading config from: {self.config_path}")
        config = OmegaConf.load(self.config_path)

        # éªŒè¯é…ç½®ç»“æ„ï¼Œç¡®ä¿é…ç½®æ–‡ä»¶çš„ç»“æ„æ˜¯æ­£ç¡®çš„
        if not validate_config_structure(config):
            raise ValueError("Invalid configuration structure")

        logger.info("âœ“ Configuration loaded and validated")
        return config

    def _setup_environment(self):
        """
        è®¾ç½®å®éªŒç¯å¢ƒ

        åŒ…æ‹¬æ—¥å¿—ã€éšæœºç§å­ã€è¾“å‡ºç›®å½•ç­‰åŸºç¡€è®¾æ–½ã€‚
        """
        # è®¾ç½®æ—¥å¿—ï¼Œgetattrçš„ä½œç”¨æ˜¯è·å–configä¸­çš„log_levelï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨INFO
        log_level = getattr(logging, self.config.get("log_level", "INFO").upper())
        setup_logging(level=log_level)

        # è®¾ç½®éšæœºç§å­ï¼Œseed_everythingçš„ä½œç”¨æ˜¯è®¾ç½®éšæœºç§å­ï¼Œå¹¶è®¾ç½®torch.manual_seedå’Œtorch.cuda.manual_seedï¼Œ
        # workersä¸ºTrueæ—¶ï¼Œä¼šè®¾ç½®torch.utils.data.DataLoaderçš„num_workersä¸º1
        if "seed" in self.config:
            pl.seed_everything(self.config.seed, workers=True)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self._create_output_dirs()

        # ä¿å­˜é…ç½®æ–‡ä»¶åˆ°å®éªŒç›®å½•
        self._save_config()

    def _create_output_dirs(self):
        """
        æ ¹æ® experiment_name åŠ¨æ€åˆ›å»ºå®éªŒè¾“å‡ºç›®å½•ã€‚
        """
        # 1. è·å–åŸºç¡€è·¯å¾„å’Œå®éªŒåç§°
        base_dir = Path(self.config.outputs.base_output_dir)
        experiment_path = base_dir / self.config.experiment_name
        logger.info(f"æ‰€æœ‰å®éªŒè¾“å‡ºå°†ä¿å­˜åˆ°: {experiment_path}")

        # 2. éå†æ‰€æœ‰å­ç›®å½•é…ç½®ï¼Œåˆ›å»ºç›®å½•å¹¶æ›´æ–°é…ç½®
        # .items()æ˜¯å­—å…¸çš„ä¸€ä¸ªæ–¹æ³•ï¼Œå®ƒä¼šæŠŠå­—å…¸é‡Œçš„æ¯ä¸€å¯¹â€œé”® (key)â€å’Œâ€œå€¼ (value)â€æ‹¿å‡ºæ¥ï¼Œç»„æˆä¸€ä¸ªä¸€ä¸ªçš„å…ƒç»„ (tuple)ã€‚
        for key, subdir_name in list(self.config.outputs.items()):
            if key.endswith("_subdir"):
                # æ„å»ºå®Œæ•´çš„ç›®å½•è·¯å¾„
                full_path = experiment_path / subdir_name
                full_path.mkdir(parents=True, exist_ok=True)

                # ç”Ÿæˆæ–°çš„é…ç½®é”® (ä¾‹å¦‚, 'checkpoint_subdir' -> 'checkpoint_dir')
                new_key = key.replace("_subdir", "_dir")

                # å°†åŠ¨æ€ç”Ÿæˆçš„å®Œæ•´è·¯å¾„æ›´æ–°å›é…ç½®å¯¹è±¡
                OmegaConf.update(self.config.outputs, new_key, str(full_path))

                logger.debug(f"åˆ›å»ºå¹¶é…ç½®ç›®å½•: {new_key} = {full_path}")

        # 3. æ¸…ç†æ—§çš„ subdir é…ç½®ï¼ˆå¯é€‰ï¼Œä½†ä¿æŒé…ç½®æ•´æ´ï¼‰
        for key in list(self.config.outputs.keys()):
            if key.endswith("_subdir"):
                del self.config.outputs[key]

    def _save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶åˆ°å®éªŒç›®å½•ï¼ˆç¡®ä¿å¯é‡ç°æ€§ï¼‰ï¼Œå¦‚æœoutputsåœ¨configä¸­ï¼Œåˆ™ä¿å­˜config.yamlåˆ°outputs.log_dirç›®å½•ä¸‹ï¼Œ
        å¦‚æœoutputs.log_dirä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºlog_dirç›®å½•"""
        if "outputs" in self.config and "log_dir" in self.config.outputs:
            from datetime import datetime

            timestamp = datetime.now().strftime(self.config.outputs.get("timestamp_format", "%Y%m%d_%H%M%S"))
            config_save_path = Path(self.config.outputs.log_dir) / f"config_{timestamp}.yaml"

            # parentæ˜¯log_dirçš„çˆ¶ç›®å½•ï¼Œå¦‚æœlog_dirä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºlog_dirç›®å½•
            config_save_path.parent.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜config.yamlæ–‡ä»¶
            with open(config_save_path, "w") as f:
                OmegaConf.save(self.config, f)

            logger.info(f"Config saved to: {config_save_path}")

    def run(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®éªŒçš„ä¸»æ–¹æ³•

        è¿™æ˜¯æ•´ä¸ªæ¡†æ¶çš„æ ¸å¿ƒã€‚å®ƒæ ¹æ®ä»»åŠ¡ç±»å‹è°ƒç”¨ç›¸åº”çš„æ‰§è¡Œæ–¹æ³•ã€‚
        æ³¨æ„è¿™é‡Œçš„ä»£ç æœ‰å¤šä¹ˆç®€æ´ - æ‰€æœ‰çš„å¤æ‚æ€§éƒ½è¢«é…ç½®æ–‡ä»¶å¸æ”¶äº†ã€‚

        Returns:
            å®éªŒç»“æœå­—å…¸
        """
        logger.info(f"ğŸš€ Starting {self.task} task")
        self._print_experiment_info()

        # æ ¹æ®ä»»åŠ¡ç±»å‹åˆ†å‘åˆ°ä¸åŒçš„æ‰§è¡Œæ–¹æ³•
        task_methods = {
            "train": self._run_training,
            "predict": self._run_prediction,
        }

        if self.task not in task_methods:
            raise ValueError(f"Unknown task: {self.task}. Available: {list(task_methods.keys())}")

        return task_methods[self.task]()

    def _run_training(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œè®­ç»ƒä»»åŠ¡ - å‚è€ƒlatent-diffusionçš„ä¼˜é›…è§£å†³æ–¹æ¡ˆ

        å…³é”®æ€è·¯ï¼š
        1. ä¸è¦ä¿®æ”¹trainer_configçš„params
        2. åœ¨instantiate_from_config(trainer_config)ä¹‹åå†è®¾ç½®callbackså’Œloggers
        3. è¿™æ ·é¿å…äº†instantiate.pyè§£æå¤æ‚å¯¹è±¡çš„é—®é¢˜
        """
        logger.info("Initializing training components...")

        # åˆ›å»ºæ¨¡å‹
        model = instantiate_from_config(self.config.model)
        # åˆ›å»ºæ•°æ®æ¨¡å—
        data_module = instantiate_from_config(self.config.data)

        # å¤„ç†traineré…ç½® - ä¿æŒåŸå§‹é…ç½®çš„çº¯å‡€æ€§
        trainer_config = self.config.trainer.copy()

        # å•ç‹¬å¤„ç†callbacks
        callbacks = self._create_callbacks()

        # å•ç‹¬å¤„ç†loggers
        loggers = self._create_loggers()

        # åˆ›å»ºtrainerï¼ˆä¸åŒ…å«callbackså’Œloggersï¼Œé¿å…instantiate.pyçš„è§£æé—®é¢˜ï¼‰
        trainer = instantiate_from_config(trainer_config)

        # åœ¨traineråˆ›å»ºå®Œæˆåï¼Œå†è®¾ç½®callbackså’Œloggers
        if callbacks:
            trainer.callbacks = callbacks

        if loggers:
            trainer.logger = loggers[0] if len(loggers) == 1 else loggers

        # å¼€å§‹è®­ç»ƒ
        logger.info("ğŸš€ Starting training...")
        trainer.fit(model, data_module)

        return {
            "status": "completed",
            "trainer": trainer,
            "model": model,
            "best_checkpoint": self._get_best_checkpoint_path(trainer),
        }

    def _create_callbacks(self) -> List:
        """
        åˆ›å»ºcallbacks - ç‹¬ç«‹çš„æ–¹æ³•ï¼Œæ›´æ¸…æ™°çš„èŒè´£åˆ†ç¦»
        callbacksçš„ä½œç”¨ï¼š
        1. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®°å½•è®­ç»ƒæ—¥å¿—
        2. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¿å­˜æ¨¡å‹
        3. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹
        4. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¿å­˜éªŒè¯é›†ä¸Šçš„æœ€ä½³æ¨¡å‹
        5. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¿å­˜æµ‹è¯•é›†ä¸Šçš„æœ€ä½³æ¨¡å‹
        6. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¿å­˜è®­ç»ƒé›†ä¸Šçš„æœ€ä½³æ¨¡å‹
        """
        callbacks = []

        if "callbacks" not in self.config:
            return callbacks

        for callback_name, callback_config in self.config.callbacks.items():
            try:
                callback = instantiate_from_config(callback_config)
                callbacks.append(callback)
                logger.info(f"âœ“ Added callback: {callback_name} ({type(callback).__name__})")
            except Exception as e:
                logger.error(f"âœ— Failed to create callback {callback_name}: {e}")
                raise

        return callbacks

    def _create_loggers(self) -> List:
        """
        åˆ›å»ºloggersï¼Œå¹¶ç¡®ä¿å®ƒä»¬ä½¿ç”¨åŠ¨æ€ç”Ÿæˆçš„è·¯å¾„ã€‚
        """
        loggers = []

        if "loggers" not in self.config:
            return loggers

        # è·å–æˆ‘ä»¬åŠ¨æ€åˆ›å»ºçš„æ—¥å¿—ç›®å½•
        dynamic_log_dir = self.config.outputs.get("log_dir")

        for logger_name, logger_config in self.config.loggers.items():
            effective_config = logger_config.copy()
            OmegaConf.update(effective_config, "params.save_dir", dynamic_log_dir)
            logger.info(f"Logger '{logger_name}' å°†ä½¿ç”¨åŠ¨æ€è·¯å¾„: {dynamic_log_dir}")

            # ä½¿ç”¨æ›´æ–°åçš„é…ç½®æ¥å®ä¾‹åŒ–
            lightning_logger = instantiate_from_config(effective_config)
            loggers.append(lightning_logger)
            logger.info(f"âœ“ Added logger: {logger_name} ({type(lightning_logger).__name__})")

        return loggers

    def _get_best_checkpoint_path(self, trainer) -> Optional[str]:
        """
        è·å–æœ€ä½³æ£€æŸ¥ç‚¹è·¯å¾„ - å·¥å…·æ–¹æ³•
        """
        for callback in trainer.callbacks:
            if isinstance(callback, pl.callbacks.ModelCheckpoint):
                return getattr(callback, "best_model_path", None)
        return None

    def _run_testing(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œæµ‹è¯•ä»»åŠ¡

        æµ‹è¯•ä»»åŠ¡çš„æ ¸å¿ƒç›®æ ‡æ˜¯è¯„ä¼°å·²è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒåŠ è½½ä¿å­˜çš„
        æ£€æŸ¥ç‚¹ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œæ¨¡å‹ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Šã€‚

        Returns:
            åŒ…å«æµ‹è¯•ç»“æœå’Œç›¸å…³æ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        logger.info("ğŸ§ª Initializing testing task...")

        # éªŒè¯å¿…éœ€çš„é…ç½®
        if "checkpoint_path" not in self.config:
            raise ValueError("Testing requires 'checkpoint_path' in config")

        checkpoint_path = self.config.checkpoint_path
        if not Path(checkpoint_path).exists():
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

        logger.info(f"Loading checkpoint: {checkpoint_path}")

        # åˆ›å»ºç»„ä»¶
        logger.info("Creating model and data module...")
        model = instantiate_from_config(self.config.model)
        data_module = instantiate_from_config(self.config.data)

        # ä¸ºæµ‹è¯•ä»»åŠ¡è°ƒæ•´traineré…ç½®
        trainer_config = self.config.trainer.copy()
        trainer_config.params.update(
            {
                "logger": False,  # æµ‹è¯•æ—¶ä¸éœ€è¦æ—¥å¿—è®°å½•
                "enable_checkpointing": False,  # æµ‹è¯•æ—¶ä¸ä¿å­˜æ£€æŸ¥ç‚¹
                "enable_progress_bar": True,  # æ˜¾ç¤ºæµ‹è¯•è¿›åº¦
            }
        )

        trainer = instantiate_from_config(trainer_config)

        # è¿è¡Œæµ‹è¯•
        logger.info("ğŸ¯ Running model testing...")
        test_results = trainer.test(model, data_module, ckpt_path=checkpoint_path)

        # ä¿å­˜æµ‹è¯•ç»“æœ
        results_file = self._save_test_results(test_results)

        logger.info("âœ… Testing completed successfully!")
        return {
            "status": "completed",
            "test_results": test_results,
            "results_file": results_file,
            "checkpoint_used": checkpoint_path,
        }

    def _run_prediction(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ¨ç†ä»»åŠ¡

        æ¨ç†ä»»åŠ¡ç”¨äºåœ¨æ–°æ•°æ®ä¸Šç”Ÿæˆé¢„æµ‹ç»“æœã€‚å®ƒç‰¹åˆ«é€‚ç”¨äºï¼š
        1. ç”Ÿæˆç«èµ›æäº¤æ–‡ä»¶
        2. å¯¹æ–°çš„é¥æ„Ÿå›¾åƒè¿›è¡Œæ»‘å¡æ£€æµ‹
        3. æ‰¹é‡å¤„ç†å¤§é‡å›¾åƒ

        Returns:
            åŒ…å«é¢„æµ‹ç»“æœå’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        logger.info("ğŸ”® Initializing prediction task...")

        # éªŒè¯å¿…éœ€çš„é…ç½®
        if "checkpoint_path" not in self.config:
            raise ValueError("Prediction requires 'checkpoint_path' in config")

        checkpoint_path = self.config.checkpoint_path
        if not Path(checkpoint_path).exists():
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

        logger.info(f"Loading model from: {checkpoint_path}")

        # åˆ›å»ºç»„ä»¶
        model = instantiate_from_config(self.config.model)
        data_module = instantiate_from_config(self.config.data)

        # ä¸ºæ¨ç†ä»»åŠ¡é…ç½®trainer
        trainer_config = self.config.trainer.copy()
        trainer_config.params.update(
            {
                "logger": False,
                "enable_checkpointing": False,
                "enable_progress_bar": True,
            }
        )
        trainer = instantiate_from_config(trainer_config)

        # è¿è¡Œé¢„æµ‹
        logger.info("ğŸ¯ Generating predictions...")
        predictions = trainer.predict(model, data_module, ckpt_path=checkpoint_path)

        # å¤„ç†å’Œä¿å­˜é¢„æµ‹ç»“æœ
        processed_predictions = self._process_predictions(predictions)
        output_files = self._save_predictions(processed_predictions)

        logger.info("âœ… Prediction completed successfully!")
        return {
            "status": "completed",
            "predictions": processed_predictions,
            "output_files": output_files,
            "checkpoint_used": checkpoint_path,
            "num_samples": len(processed_predictions) if processed_predictions else 0,
        }

    def _run_validation(self) -> Dict[str, Any]:
        """
        æ‰§è¡ŒéªŒè¯ä»»åŠ¡

        éªŒè¯ä»»åŠ¡ç”¨äºåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œé€šå¸¸ç”¨äºï¼š
        1. æ¨¡å‹å¼€å‘è¿‡ç¨‹ä¸­çš„å¿«é€Ÿæ€§èƒ½æ£€æŸ¥
        2. è¶…å‚æ•°è°ƒä¼˜
        3. æ¨¡å‹é€‰æ‹©å’Œæ¯”è¾ƒ

        Returns:
            åŒ…å«éªŒè¯ç»“æœçš„å­—å…¸
        """
        logger.info("ğŸ” Initializing validation task...")

        # åˆ›å»ºç»„ä»¶
        model = instantiate_from_config(self.config.model)
        data_module = instantiate_from_config(self.config.data)

        # é…ç½®trainerï¼ˆéªŒè¯ä»»åŠ¡é€šå¸¸æ¯”è¾ƒè½»é‡ï¼‰
        trainer_config = self.config.trainer.copy()
        trainer_config.params.update(
            {
                "logger": False,
                "enable_checkpointing": False,
                "enable_progress_bar": True,
            }
        )
        trainer = instantiate_from_config(trainer_config)

        # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†æ£€æŸ¥ç‚¹
        checkpoint_path = self.config.get("checkpoint_path")
        if checkpoint_path:
            logger.info(f"Using checkpoint: {checkpoint_path}")
            if not Path(checkpoint_path).exists():
                raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

        # è¿è¡ŒéªŒè¯
        logger.info("ğŸ¯ Running validation...")
        val_results = trainer.validate(model, data_module, ckpt_path=checkpoint_path)

        # ä¿å­˜éªŒè¯ç»“æœ
        results_file = self._save_validation_results(val_results)

        logger.info("âœ… Validation completed successfully!")
        return {
            "status": "completed",
            "validation_results": val_results,
            "results_file": results_file,
            "checkpoint_used": checkpoint_path,
        }

    def _save_test_results(self, test_results: List[Dict]) -> Path:
        """
        ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶

        Args:
            test_results: Lightning trainer.test()çš„è¿”å›ç»“æœ

        Returns:
            ä¿å­˜çš„ç»“æœæ–‡ä»¶è·¯å¾„
        """
        import json
        from datetime import datetime

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config.outputs.get("predictions_dir", "outputs/test_results"))
        output_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"test_results_{self.config.experiment_name}_{timestamp}.json"
        results_file = output_dir / filename

        # ä¿å­˜ç»“æœ
        with open(results_file, "w") as f:
            json.dump(
                {
                    "experiment_name": self.config.experiment_name,
                    "timestamp": timestamp,
                    "checkpoint_path": self.config.get("checkpoint_path"),
                    "test_results": test_results,
                    "config_summary": {
                        "model_type": self.config.model.target.split(".")[-1],
                        "data_config": self.config.data.target.split(".")[-1],
                    },
                },
                f,
                indent=2,
            )

        logger.info(f"Test results saved to: {results_file}")
        return results_file

    def _save_validation_results(self, val_results: List[Dict]) -> Path:
        """
        ä¿å­˜éªŒè¯ç»“æœåˆ°æ–‡ä»¶

        Args:
            val_results: Lightning trainer.validate()çš„è¿”å›ç»“æœ

        Returns:
            ä¿å­˜çš„ç»“æœæ–‡ä»¶è·¯å¾„
        """
        import json
        from datetime import datetime

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config.outputs.get("predictions_dir", "outputs/validation_results"))
        output_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"validation_results_{self.config.experiment_name}_{timestamp}.json"
        results_file = output_dir / filename

        # ä¿å­˜ç»“æœ
        with open(results_file, "w") as f:
            json.dump(
                {
                    "experiment_name": self.config.experiment_name,
                    "timestamp": timestamp,
                    "checkpoint_path": self.config.get("checkpoint_path"),
                    "validation_results": val_results,
                    "config_summary": {
                        "model_type": self.config.model.target.split(".")[-1],
                        "data_config": self.config.data.target.split(".")[-1],
                    },
                },
                f,
                indent=2,
            )

        logger.info(f"Validation results saved to: {results_file}")
        return results_file

    def _process_predictions(self, raw_predictions: List) -> List[Dict]:
        """
        å¤„ç†åŸå§‹é¢„æµ‹ç»“æœ

        å°†PyTorchå¼ é‡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼ï¼Œå¹¶æ·»åŠ å¿…è¦çš„å…ƒæ•°æ®ã€‚

        Args:
            raw_predictions: trainer.predict()çš„åŸå§‹è¿”å›ç»“æœ

        Returns:
            å¤„ç†åçš„é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        import torch
        import numpy as np

        processed = []

        for batch_idx, batch_predictions in enumerate(raw_predictions):
            # å¦‚æœé¢„æµ‹ç»“æœæ˜¯å¼ é‡ï¼Œè½¬æ¢ä¸ºnumpyæ•°ç»„
            if isinstance(batch_predictions, torch.Tensor):
                predictions_np = batch_predictions.cpu().numpy()
            else:
                predictions_np = batch_predictions

            # å¤„ç†æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹
            for sample_idx, prediction in enumerate(predictions_np):
                processed.append(
                    {
                        "batch_idx": batch_idx,
                        "sample_idx": sample_idx,
                        "prediction": float(prediction) if np.isscalar(prediction) else prediction.tolist(),
                        "probability": (
                            float(torch.sigmoid(torch.tensor(prediction)).item()) if np.isscalar(prediction) else None
                        ),
                    }
                )

        logger.info(f"Processed {len(processed)} predictions")
        return processed

    def _save_predictions(self, predictions: List[Dict]) -> Dict[str, Path]:
        """
        ä¿å­˜é¢„æµ‹ç»“æœåˆ°å¤šç§æ ¼å¼çš„æ–‡ä»¶

        Args:
            predictions: å¤„ç†åçš„é¢„æµ‹ç»“æœ

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        import json
        import pandas as pd
        from datetime import datetime

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config.outputs.get("predictions_dir", "outputs/predictions"))
        output_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"predictions_{self.config.experiment_name}_{timestamp}"

        output_files = {}

        # ä¿å­˜JSONæ ¼å¼ï¼ˆå®Œæ•´ä¿¡æ¯ï¼‰
        json_file = output_dir / f"{base_filename}.json"
        with open(json_file, "w") as f:
            json.dump(
                {
                    "experiment_name": self.config.experiment_name,
                    "timestamp": timestamp,
                    "checkpoint_path": self.config.get("checkpoint_path"),
                    "num_predictions": len(predictions),
                    "predictions": predictions,
                },
                f,
                indent=2,
            )
        output_files["json"] = json_file

        # ä¿å­˜CSVæ ¼å¼ï¼ˆä¾¿äºåˆ†æï¼‰
        if predictions:
            df_data = []
            for pred in predictions:
                df_data.append(
                    {
                        "sample_id": f"sample_{pred['batch_idx']}_{pred['sample_idx']}",
                        "prediction": pred["prediction"],
                        "probability": pred.get("probability", None),
                    }
                )

            df = pd.DataFrame(df_data)
            csv_file = output_dir / f"{base_filename}.csv"
            df.to_csv(csv_file, index=False)
            output_files["csv"] = csv_file

        logger.info(f"Predictions saved to: {list(output_files.values())}")
        return output_files

    def _print_experiment_info(self):
        """æ‰“å°å®éªŒä¿¡æ¯"""
        print("\n" + "=" * 80)
        print(f"ğŸš€ MM-LandslideNet Experiment: {self.config.get('experiment_name', 'Unnamed')}")
        print("=" * 80)
        print(f"ğŸ“ Task: {self.task}")
        print(f"ğŸ“ Config: {self.config_path}")
        print(f"ğŸ• Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        if "model" in self.config:
            print(f"ğŸ§  Model: {self.config.model.target.split('.')[-1]}")
        if "data" in self.config:
            print(f"ğŸ“Š Data: {self.config.data.get('params', {}).get('train_data_dir', 'N/A')}")

        print("=" * 80 + "\n")


def apply_overrides(config: DictConfig, overrides: list) -> DictConfig:
    """
    åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–

    å…è®¸ç”¨æˆ·åœ¨å‘½ä»¤è¡Œä¸­è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„ç‰¹å®šå€¼ã€‚
    è¿™åœ¨è°ƒè¯•å’Œå¿«é€Ÿå®éªŒæ—¶éå¸¸æœ‰ç”¨ã€‚

    Args:
        config: åŸå§‹é…ç½®
        overrides: è¦†ç›–åˆ—è¡¨ï¼Œæ ¼å¼ä¸º ["key=value", "another.key=value"]

    Returns:
        ä¿®æ”¹åçš„é…ç½®
    """
    if not overrides:
        return config

    logger.info(f"Applying {len(overrides)} config overrides...")

    for override in overrides:
        try:
            key, value = override.split("=", 1)

            # å°è¯•è‡ªåŠ¨ç±»å‹è½¬æ¢
            try:
                # å¤„ç†æ•°å­—
                if value.isdigit():
                    value = int(value)
                elif value.replace(".", "").isdigit() and value.count(".") == 1:
                    value = float(value)
                # å¤„ç†å¸ƒå°”å€¼
                elif value.lower() in ["true", "false"]:
                    value = value.lower() == "true"
                # å¤„ç†åˆ—è¡¨ï¼ˆç®€å•æƒ…å†µï¼‰
                elif value.startswith("[") and value.endswith("]"):
                    value = eval(value)  # æ³¨æ„ï¼šç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥ç”¨æ›´å®‰å…¨çš„è§£ææ–¹æ³•
            except:
                pass  # ä¿æŒä¸ºå­—ç¬¦ä¸²

            OmegaConf.update(config, key, value)
            logger.info(f"  {key} = {value}")

        except Exception as e:
            logger.warning(f"Failed to apply override '{override}': {e}")

    return config


def create_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="MM-LandslideNet: Configuration-Driven Deep Learning Framework",
        formatter_class=argparse.RawDescriptionHelpFormatter,  # å†™çš„æ ¼å¼æ˜¯ä»€ä¹ˆæ ·å°±æŒ‰ä»€ä¹ˆæ ·æ˜¾ç¤ºã€‚
        epilog="""
            Examples:
            # è®­ç»ƒæ¨¡å‹
            python main.py train lightning_landslide/configs/experiment/optical_baseline.yaml
            
            # è¿è¡Œæ¨ç†
            python main.py predict lightning_landslide/configs/experiment/optical_baseline.yaml
            
            # éªŒè¯æ¨¡å‹
            python main.py validate lightning_landslide/configs/experiment/optical_baseline.yaml

            """,
    )

    # ä¸»è¦å‚æ•°
    parser.add_argument(
        "task",
        choices=["train", "predict", "validate"],
        help="Task to execute",
    )

    parser.add_argument(
        "config",
        type=str,
        help="Path to configuration file",
    )

    return parser


def main():
    """
    ä¸»å‡½æ•°

    è¿™æ˜¯æ•´ä¸ªç¨‹åºçš„å…¥å£ç‚¹ã€‚ä¸æ‚¨åŸæ¥çš„main.pyç›¸æ¯”ï¼Œ
    æ–°ç‰ˆæœ¬çš„é€»è¾‘æå…¶ç®€æ´ï¼š
    1. è§£æå‘½ä»¤è¡Œå‚æ•°
    2. åˆ›å»ºå®éªŒè¿è¡Œå™¨
    3. è¿è¡Œå®éªŒ
    4. æŠ¥å‘Šç»“æœ

    æ‰€æœ‰çš„å¤æ‚æ€§éƒ½è¢«é…ç½®æ–‡ä»¶å’Œå®ä¾‹åŒ–å·¥å…·å¸æ”¶äº†ã€‚
    """
    parser = create_parser()  # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    args = parser.parse_args()  # è§£æå‘½ä»¤è¡Œå‚æ•°

    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = ExperimentRunner(args.config, args.task)

    # è¿è¡Œå®éªŒ
    results = runner.run()

    # æŠ¥å‘Šç»“æœ
    print(f"\nğŸ‰ Task '{args.task}' completed successfully!")
    if results.get("best_model_path"):
        print(f"ğŸ“ Best model saved to: {results['best_model_path']}")


if __name__ == "__main__":
    main()
