import os
import sys
import argparse
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
import warnings
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# æŠ‘åˆ¶ä¸å¿…è¦çš„è­¦å‘Š
# warnings.filterwarnings("ignore", ".*does not have many workers.*")
# warnings.filterwarnings("ignore", ".*The dataloader.*")

import torch
import pytorch_lightning as pl
from omegaconf import DictConfig, OmegaConf

# å¯¼å…¥æˆ‘ä»¬çš„æ ¸å¿ƒå·¥å…·
from lightning_landslide.src.utils.instantiate import instantiate_from_config, validate_config_structure
from lightning_landslide.src.utils.logging_utils import setup_logging, get_project_logger
from lightning_landslide.src.training.kfold_trainer import KFoldTrainer

logger = get_project_logger(__name__)


class ExperimentRunner:
    """
    å¢å¼ºçš„å®éªŒè¿è¡Œå™¨ - æ”¯æŒKæŠ˜äº¤å‰éªŒè¯

    æ–°å¢åŠŸèƒ½ï¼š
    1. æ”¯æŒkfoldä»»åŠ¡ç±»å‹
    2. ä¿æŒç°æœ‰æ¶æ„çš„ç®€æ´æ€§
    3. å§”æ‰˜å¤æ‚é€»è¾‘ç»™ä¸“é—¨çš„KFoldTrainer
    """

    def __init__(self, config_path: str, task: str = "train", **kwargs):
        """
        åˆå§‹åŒ–å®éªŒè¿è¡Œå™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            task: è¦æ‰§è¡Œçš„ä»»åŠ¡ç±»å‹ï¼ˆtrain/predict/kfoldï¼‰
            **kwargs: é¢å¤–çš„ä»»åŠ¡å‚æ•°
        """
        setup_logging(level=logging.INFO)
        self.config_path = Path(config_path)
        self.task = task
        self.task_kwargs = kwargs  # å­˜å‚¨é¢å¤–çš„ä»»åŠ¡å‚æ•°
        self.config = self._load_config()
        self._setup_environment()

    def _load_config(self) -> DictConfig:
        """åŠ è½½å’ŒéªŒè¯é…ç½®æ–‡ä»¶"""
        if not self.config_path.exists():
            raise FileNotFoundError(f"Config file not found: {self.config_path}")

        logger.info(f"Loading config from: {self.config_path}")
        config = OmegaConf.load(self.config_path)

        # éªŒè¯é…ç½®ç»“æ„
        if not validate_config_structure(config):
            raise ValueError("Invalid configuration structure")

        logger.info("âœ“ Configuration loaded and validated")
        return config

    def _setup_environment(self):
        """è®¾ç½®å®éªŒç¯å¢ƒ"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self._create_output_dirs()

        # è®¾ç½®æ—¥å¿—ï¼Œgetattrçš„ä½œç”¨æ˜¯è·å–configä¸­çš„log_levelï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨INFO
        log_level = getattr(logging, self.config.get("log_level", "INFO").upper())
        log_file = None

        if "outputs" in self.config and "log_dir" in self.config.outputs:
            log_file = Path(self.config.outputs.log_dir) / f"{self.config.experiment_name}.log"

        setup_logging(
            level=log_level,
            log_file=str(log_file) if log_file else None,
            use_colors=True,
        )

        # è®¾ç½®éšæœºç§å­ï¼Œseed_everythingçš„ä½œç”¨æ˜¯è®¾ç½®éšæœºç§å­ï¼Œå¹¶è®¾ç½®torch.manual_seedå’Œtorch.cuda.manual_seedï¼Œ
        # workersä¸ºTrueæ—¶ï¼Œä¼šè®¾ç½®torch.utils.data.DataLoaderçš„num_workersä¸º1
        if "seed" in self.config:
            pl.seed_everything(self.config.seed, workers=True)

        # ä¿å­˜é…ç½®æ–‡ä»¶åˆ°å®éªŒç›®å½•
        self._save_config()

    def _create_output_dirs(self):
        """æ ¹æ®experiment_nameåŠ¨æ€åˆ›å»ºå®éªŒè¾“å‡ºç›®å½•"""
        base_dir = Path(self.config.outputs.base_output_dir)
        experiment_path = base_dir / self.config.experiment_name
        logger.info(f"æ‰€æœ‰å®éªŒè¾“å‡ºå°†ä¿å­˜åˆ°: {experiment_path}")

        # 2. éå†æ‰€æœ‰å­ç›®å½•é…ç½®ï¼Œåˆ›å»ºç›®å½•å¹¶æ›´æ–°é…ç½®
        # .items()æ˜¯å­—å…¸çš„ä¸€ä¸ªæ–¹æ³•ï¼Œå®ƒä¼šæŠŠå­—å…¸é‡Œçš„æ¯ä¸€å¯¹â€œé”® (key)â€å’Œâ€œå€¼ (value)â€æ‹¿å‡ºæ¥ï¼Œç»„æˆä¸€ä¸ªä¸€ä¸ªçš„å…ƒç»„ (tuple)ã€‚
        for key, subdir_name in list(self.config.outputs.items()):
            if key.endswith("_subdir"):
                # æ„å»ºå®Œæ•´çš„ç›®å½•è·¯å¾„
                full_path = experiment_path / subdir_name
                full_path.mkdir(parents=True, exist_ok=True)

                # ç”Ÿæˆæ–°çš„é…ç½®é”® (ä¾‹å¦‚, 'checkpoint_subdir' -> 'checkpoint_dir')
                new_key = key.replace("_subdir", "_dir")

                # å°†åŠ¨æ€ç”Ÿæˆçš„å®Œæ•´è·¯å¾„æ›´æ–°å›é…ç½®å¯¹è±¡
                OmegaConf.update(self.config.outputs, new_key, str(full_path))

                logger.debug(f"åˆ›å»ºå¹¶é…ç½®ç›®å½•: {new_key} = {full_path}")

        # 3. æ¸…ç†æ—§çš„ subdir é…ç½®ï¼ˆå¯é€‰ï¼Œä½†ä¿æŒé…ç½®æ•´æ´ï¼‰
        for key in list(self.config.outputs.keys()):
            if key.endswith("_subdir"):
                del self.config.outputs[key]

    def _save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶åˆ°å®éªŒç›®å½•ï¼ˆç¡®ä¿å¯é‡ç°æ€§ï¼‰ï¼Œå¦‚æœoutputsåœ¨configä¸­ï¼Œåˆ™ä¿å­˜config.yamlåˆ°outputs.log_dirç›®å½•ä¸‹ï¼Œ
        å¦‚æœoutputs.log_dirä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºlog_dirç›®å½•"""
        if "outputs" in self.config and "log_dir" in self.config.outputs:
            timestamp = datetime.now().strftime(self.config.outputs.get("timestamp_format", "%Y%m%d_%H%M%S"))
            config_save_path = Path(self.config.outputs.log_dir) / f"config_{timestamp}.yaml"
            config_save_path.parent.mkdir(parents=True, exist_ok=True)

            with open(config_save_path, "w") as f:
                OmegaConf.save(self.config, f)

            logger.info(f"Config saved to: {config_save_path}")

    def run(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®éªŒçš„ä¸»æ–¹æ³• - æ‰©å±•æ”¯æŒKæŠ˜äº¤å‰éªŒè¯

        Returns:
            å®éªŒç»“æœå­—å…¸
        """
        logger.info(f"ğŸš€ Starting {self.task} task")
        self._print_experiment_info()

        # æ‰©å±•çš„ä»»åŠ¡æ–¹æ³•æ˜ å°„
        task_methods = {
            "train": self._run_training,
            "predict": self._run_prediction,
            "kfold": self._run_kfold,  # æ–°å¢KæŠ˜ä»»åŠ¡
            "kfold_predict": self._run_kfold_predict,  # æ–°å¢KæŠ˜é¢„æµ‹ä»»åŠ¡
        }

        if self.task not in task_methods:
            raise ValueError(f"Unknown task: {self.task}. Available: {list(task_methods.keys())}")

        return task_methods[self.task]()

    def _run_training(self) -> Dict[str, Any]:
        """æ‰§è¡Œæ ‡å‡†è®­ç»ƒä»»åŠ¡"""
        logger.info("Initializing training components...")

        # åˆ›å»ºæ¨¡å‹å’Œæ•°æ®æ¨¡å—
        model = instantiate_from_config(self.config.model)
        data_module = instantiate_from_config(self.config.data)

        # å¤„ç†traineré…ç½®
        trainer_config = self.config.trainer.copy()
        callbacks = self._create_callbacks()
        loggers = self._create_loggers()

        # åˆ›å»ºtrainer
        trainer = instantiate_from_config(trainer_config)

        if callbacks:
            trainer.callbacks = callbacks
        if loggers:
            trainer.logger = loggers[0] if len(loggers) == 1 else loggers

        # å¼€å§‹è®­ç»ƒ
        logger.info("ğŸš€ Starting training...")
        trainer.fit(model, data_module)
        """
            fit æ˜¯ pytorch_lightning.Trainer ç±»çš„æ ¸å¿ƒæ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•ä¼šè‡ªåŠ¨æ‰§è¡Œæ•´ä¸ªè®­ç»ƒæµç¨‹ï¼š

            æ•°æ®å‡†å¤‡ï¼šè°ƒç”¨ data_module.prepare_data() å’Œ data_module.setup()
            åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼šè·å– train/val dataloaders
            è®­ç»ƒå¾ªç¯ï¼š

            è°ƒç”¨ model.training_step() å¤„ç†æ¯ä¸ªbatch
            è®¡ç®—losså’Œæ¢¯åº¦
            æ‰§è¡Œä¼˜åŒ–å™¨æ›´æ–°

            éªŒè¯å¾ªç¯ï¼š

            è°ƒç”¨ model.validation_step()
            è®¡ç®—éªŒè¯æŒ‡æ ‡

            å›è°ƒæ‰§è¡Œï¼šè¿è¡Œcheckpointingã€early stoppingç­‰
            æ—¥å¿—è®°å½•ï¼šè‡ªåŠ¨è®°å½•æ‰€æœ‰æŒ‡æ ‡
            # Lightningå†…éƒ¨ä¼šè°ƒç”¨
            â”œâ”€â”€ data_module.prepare_data()           # æ•°æ®å‡†å¤‡
            â”œâ”€â”€ data_module.setup('fit')             # æ•°æ®é›†è®¾ç½®  
            â”œâ”€â”€ data_module.train_dataloader()       # è·å–è®­ç»ƒæ•°æ®åŠ è½½å™¨
            â”œâ”€â”€ data_module.val_dataloader()         # è·å–éªŒè¯æ•°æ®åŠ è½½å™¨
            â”œâ”€â”€ model.configure_optimizers()         # é…ç½®ä¼˜åŒ–å™¨
            â””â”€â”€ è®­ç»ƒå¾ªç¯:
                â”œâ”€â”€ model.training_step(batch, idx)  # æ¯ä¸ªè®­ç»ƒbatch
                â”œâ”€â”€ optimizer.step()                 # å‚æ•°æ›´æ–°
                â”œâ”€â”€ model.validation_step(batch, idx) # æ¯ä¸ªéªŒè¯batch
                â”œâ”€â”€ callbacks.on_epoch_end()         # å›è°ƒå‡½æ•°
                â””â”€â”€ logger.log_metrics()             # è®°å½•æŒ‡æ ‡
        """

        return {
            "status": "completed",
            "trainer": trainer,
            "model": model,
            "best_checkpoint": self._get_best_checkpoint_path(trainer),
        }

    def _run_kfold(self) -> Dict[str, Any]:
        """
        æ‰§è¡ŒKæŠ˜äº¤å‰éªŒè¯ä»»åŠ¡

        è¿™ä¸ªæ–¹æ³•å§”æ‰˜ç»™ä¸“é—¨çš„KFoldTrainerï¼Œä¿æŒmain.pyçš„ç®€æ´æ€§
        """
        logger.info("ğŸ¯ Initializing K-Fold Cross Validation...")

        # æ£€æŸ¥æ˜¯å¦æœ‰KæŠ˜é…ç½®
        if "kfold" not in self.config:
            raise ValueError("K-fold task requires 'kfold' configuration section")

        # æå–KæŠ˜é…ç½®
        kfold_config = self.config.kfold

        # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
        if "n_splits" in self.task_kwargs:
            kfold_config.n_splits = self.task_kwargs["n_splits"]
        if "experiment_name" in self.task_kwargs:
            kfold_config.experiment_name = self.task_kwargs["experiment_name"]

        # åˆ›å»ºKFoldTrainer
        trainer = KFoldTrainer(
            model_config=OmegaConf.to_container(self.config.model, resolve=True),
            data_config=OmegaConf.to_container(self.config.data.params, resolve=True),
            trainer_config=OmegaConf.to_container(self.config.trainer.params, resolve=True),
            # KæŠ˜é…ç½®
            n_splits=kfold_config.get("n_splits", 5),
            stratified=kfold_config.get("stratified", True),
            # è¾“å‡ºé…ç½®
            output_dir=kfold_config.get("output_dir", "outputs/kfold_experiments"),
            experiment_name=kfold_config.get("experiment_name", self.config.get("experiment_name", "kfold_experiment")),
            # æ€§èƒ½é…ç½®
            primary_metric=kfold_config.get("primary_metric", "f1"),
            early_stopping_patience=kfold_config.get("early_stopping_patience", 15),
            # å…¶ä»–é…ç½®
            seed=self.config.get("seed", 3407),
            save_predictions=kfold_config.get("save_predictions", True),
            save_models=kfold_config.get("save_models", True),
            generate_oof=kfold_config.get("generate_oof", True),
        )

        # è¿è¡ŒKæŠ˜è®­ç»ƒ
        logger.info(f"ğŸ”„ Starting {kfold_config.get('n_splits', 5)}-fold cross validation...")
        results = trainer.train_kfold()

        # æ‰“å°ç»“æœæ‘˜è¦
        self._print_kfold_summary(results)

        return results

    def _run_kfold_predict(self) -> Dict[str, Any]:
        """
        æ‰§è¡ŒKæŠ˜é¢„æµ‹ä»»åŠ¡ï¼ˆä»å·²è®­ç»ƒçš„KæŠ˜æ¨¡å‹ç”Ÿæˆé¢„æµ‹ï¼‰
        """
        logger.info("ğŸ”® Running K-Fold prediction...")

        # æ£€æŸ¥å¿…éœ€çš„é…ç½®
        if "resume_from" not in self.task_kwargs:
            raise ValueError("K-fold prediction requires --resume_from argument")

        experiment_dir = self.task_kwargs["resume_from"]
        if not Path(experiment_dir).exists():
            raise FileNotFoundError(f"Experiment directory not found: {experiment_dir}")

        # è¿™é‡Œå¯ä»¥å®ç°ä»ç°æœ‰æ¨¡å‹ç”Ÿæˆé¢„æµ‹çš„é€»è¾‘
        # æˆ–è€…è°ƒç”¨KFoldTrainerçš„ç›¸å…³æ–¹æ³•

        logger.info("âœ… K-Fold prediction completed")
        return {"status": "prediction_completed", "experiment_dir": experiment_dir}

    def _run_prediction(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ¨ç†ä»»åŠ¡

        æ¨ç†ä»»åŠ¡ç”¨äºåœ¨æ–°æ•°æ®ä¸Šç”Ÿæˆé¢„æµ‹ç»“æœã€‚å®ƒç‰¹åˆ«é€‚ç”¨äºï¼š
        1. ç”Ÿæˆç«èµ›æäº¤æ–‡ä»¶
        2. å¯¹æ–°çš„é¥æ„Ÿå›¾åƒè¿›è¡Œæ»‘å¡æ£€æµ‹
        3. æ‰¹é‡å¤„ç†å¤§é‡å›¾åƒ

        Returns:
            åŒ…å«é¢„æµ‹ç»“æœå’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        logger.info("ğŸ”® Initializing prediction task...")

        # éªŒè¯å¿…éœ€çš„é…ç½®
        if "checkpoint_path" not in self.config:
            raise ValueError("Prediction requires 'checkpoint_path' in config")

        checkpoint_path = self.config.checkpoint_path
        if not Path(checkpoint_path).exists():
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

        logger.info(f"Loading model from: {checkpoint_path}")

        # åˆ›å»ºç»„ä»¶
        model = instantiate_from_config(self.config.model)
        data_module = instantiate_from_config(self.config.data)

        # ä¸ºæ¨ç†ä»»åŠ¡é…ç½®trainer
        trainer_config = self.config.trainer.copy()
        trainer_config.params.update(
            {
                "logger": False,
                "enable_checkpointing": False,
                "enable_progress_bar": True,
            }
        )
        trainer = instantiate_from_config(trainer_config)

        # è¿è¡Œé¢„æµ‹
        logger.info("ğŸ¯ Generating predictions...")
        predictions = trainer.predict(model, data_module, ckpt_path=checkpoint_path)

        # å¤„ç†å’Œä¿å­˜é¢„æµ‹ç»“æœ
        processed_predictions = self._process_predictions(predictions)
        output_files = self._save_predictions(processed_predictions)

        logger.info("âœ… Prediction completed successfully!")
        return {
            "status": "completed",
            "predictions": processed_predictions,
            "output_files": output_files,
            "checkpoint_used": checkpoint_path,
            "num_samples": len(processed_predictions) if processed_predictions else 0,
        }

    def _print_kfold_summary(self, results: Dict[str, Any]) -> None:
        """æ‰“å°KæŠ˜ç»“æœæ‘˜è¦"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ‰ K-FOLD CROSS VALIDATION COMPLETED!")
        logger.info("=" * 60)
        logger.info(f"Experiment: {results['experiment_name']}")
        logger.info(f"Number of Folds: {results['n_splits']}")
        logger.info(f"Mean CV Score: {results['mean_cv_score']:.4f} Â± {results['std_cv_score']:.4f}")
        logger.info(f"Training Time: {results['training_time']:.2f}s")

        if results.get("oof_metrics"):
            oof = results["oof_metrics"]
            logger.info(f"OOF Metrics:")
            logger.info(f"  F1 Score: {oof.get('f1_score', 0):.4f}")
            logger.info(f"  AUC Score: {oof.get('auc_score', 0):.4f}")
            logger.info(f"  Accuracy: {oof.get('accuracy', 0):.4f}")

        # æ‰“å°æ¯æŠ˜ç»“æœ
        logger.info("Individual Fold Results:")
        for i, fold_result in enumerate(results["fold_results"]):
            score = fold_result["val_metrics"].get("f1", 0)
            logger.info(f"  Fold {i+1}: {score:.4f}")

        logger.info("=" * 60)

    def _process_predictions(self, raw_predictions: List) -> List[Dict]:
        """
        å¤„ç†åŸå§‹é¢„æµ‹ç»“æœ

        å°†PyTorchå¼ é‡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼ï¼Œå¹¶æ·»åŠ å¿…è¦çš„å…ƒæ•°æ®ã€‚

        Args:
            raw_predictions: trainer.predict()çš„åŸå§‹è¿”å›ç»“æœ

        Returns:
            å¤„ç†åçš„é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        import torch
        import numpy as np

        processed = []

        for batch_idx, batch_predictions in enumerate(raw_predictions):
            # å¦‚æœé¢„æµ‹ç»“æœæ˜¯å¼ é‡ï¼Œè½¬æ¢ä¸ºnumpyæ•°ç»„
            if isinstance(batch_predictions, torch.Tensor):
                predictions_np = batch_predictions.cpu().numpy()
            else:
                predictions_np = batch_predictions

            # å¤„ç†æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹
            for sample_idx, prediction in enumerate(predictions_np):
                processed.append(
                    {
                        "batch_idx": batch_idx,
                        "sample_idx": sample_idx,
                        "prediction": float(prediction) if np.isscalar(prediction) else prediction.tolist(),
                        "probability": (
                            float(torch.sigmoid(torch.tensor(prediction)).item()) if np.isscalar(prediction) else None
                        ),
                    }
                )

        logger.info(f"Processed {len(processed)} predictions")
        return processed

    def _save_predictions(self, predictions: List[Dict]) -> Dict[str, Path]:
        """
        ä¿å­˜é¢„æµ‹ç»“æœåˆ°å¤šç§æ ¼å¼çš„æ–‡ä»¶

        Args:
            predictions: å¤„ç†åçš„é¢„æµ‹ç»“æœ

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        import json
        import pandas as pd
        from datetime import datetime

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config.outputs.get("predictions_dir", "outputs/predictions"))
        output_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"predictions_{self.config.experiment_name}_{timestamp}"

        output_files = {}

        # ä¿å­˜JSONæ ¼å¼ï¼ˆå®Œæ•´ä¿¡æ¯ï¼‰
        json_file = output_dir / f"{base_filename}.json"
        with open(json_file, "w") as f:
            json.dump(
                {
                    "experiment_name": self.config.experiment_name,
                    "timestamp": timestamp,
                    "checkpoint_path": self.config.get("checkpoint_path"),
                    "num_predictions": len(predictions),
                    "predictions": predictions,
                },
                f,
                indent=2,
            )
        output_files["json"] = json_file

        # ä¿å­˜CSVæ ¼å¼ï¼ˆä¾¿äºåˆ†æï¼‰
        if predictions:
            df_data = []
            for pred in predictions:
                df_data.append(
                    {
                        "sample_id": f"sample_{pred['batch_idx']}_{pred['sample_idx']}",
                        "prediction": pred["prediction"],
                        "probability": pred.get("probability", None),
                    }
                )

            df = pd.DataFrame(df_data)
            csv_file = output_dir / f"{base_filename}.csv"
            df.to_csv(csv_file, index=False)
            output_files["csv"] = csv_file

        logger.info(f"Predictions saved to: {list(output_files.values())}")
        return output_files

    def _create_callbacks(self) -> List:
        """
        åˆ›å»ºcallbacks - ç‹¬ç«‹çš„æ–¹æ³•ï¼Œæ›´æ¸…æ™°çš„èŒè´£åˆ†ç¦»
        callbacksçš„ä½œç”¨ï¼š
        1. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®°å½•è®­ç»ƒæ—¥å¿—
        2. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¿å­˜æ¨¡å‹
        3. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹
        4. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¿å­˜éªŒè¯é›†ä¸Šçš„æœ€ä½³æ¨¡å‹
        5. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¿å­˜æµ‹è¯•é›†ä¸Šçš„æœ€ä½³æ¨¡å‹
        6. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¿å­˜è®­ç»ƒé›†ä¸Šçš„æœ€ä½³æ¨¡å‹
        """
        callbacks = []

        if "callbacks" not in self.config:
            return callbacks

        # è·å–åŠ¨æ€ç”Ÿæˆçš„è·¯å¾„
        dynamic_checkpoint_dir = self.config.outputs.get("checkpoint_dir")
        dynamic_log_dir = self.config.outputs.get("log_dir")

        logger.info("-" * 100)

        for callback_name, callback_config in self.config.callbacks.items():
            # æ·±æ‹·è´é…ç½®ï¼Œé¿å…ä¿®æ”¹åŸå§‹é…ç½®
            effective_config = callback_config.copy()

            # å¦‚æœæ˜¯ ModelCheckpoint ä¸”æ²¡æœ‰ dirpathï¼Œä½¿ç”¨åŠ¨æ€è·¯å¾„
            if callback_config.target == "pytorch_lightning.callbacks.ModelCheckpoint" and dynamic_checkpoint_dir:
                # ç›´æ¥è®¾ç½®åŠ¨æ€è·¯å¾„ï¼Œè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„é™æ€è·¯å¾„
                OmegaConf.update(effective_config, "params.dirpath", dynamic_checkpoint_dir)

            if callback_config.target == "lightning_landslide.src.utils.metrics.MetricsLogger" and dynamic_log_dir:
                OmegaConf.update(effective_config, "params.log_dir", dynamic_log_dir)

            # åˆ›å»ºcallback
            callback = instantiate_from_config(effective_config)
            callbacks.append(callback)
            logger.info(f"âœ“ Added callback: {callback_name} ({type(callback).__name__})")

        logger.info("-" * 100)
        return callbacks

    def _create_loggers(self) -> List:
        """
        åˆ›å»ºloggersï¼Œå¹¶ç¡®ä¿å®ƒä»¬ä½¿ç”¨åŠ¨æ€ç”Ÿæˆçš„è·¯å¾„ã€‚
        """
        loggers = []

        if "loggers" not in self.config:
            return loggers

        # è·å–æˆ‘ä»¬åŠ¨æ€åˆ›å»ºçš„æ—¥å¿—ç›®å½•
        dynamic_log_dir = self.config.outputs.get("log_dir")

        logger.info("-" * 100)

        for logger_name, logger_config in self.config.loggers.items():
            if logger_name == "tensorboard":
                effective_config = logger_config.copy()
                OmegaConf.update(effective_config, "params.save_dir", dynamic_log_dir)
                OmegaConf.update(effective_config, "params.name", "")
                OmegaConf.update(effective_config, "params.version", "")
                logger.info(f"Logger '{logger_name}' å°†ä½¿ç”¨åŠ¨æ€è·¯å¾„: {dynamic_log_dir}")

                # ä½¿ç”¨æ›´æ–°åçš„é…ç½®æ¥å®ä¾‹åŒ–
                lightning_logger = instantiate_from_config(effective_config)
                loggers.append(lightning_logger)
                logger.info(f"âœ“ Added logger: {logger_name} ({type(lightning_logger).__name__})")

            elif logger_name == "wandb":
                # WandBé…ç½®ä¿æŒä¸å˜
                lightning_logger = instantiate_from_config(logger_config)
                loggers.append(lightning_logger)
                logger.info(f"âœ“ Added logger: {logger_name} ({type(lightning_logger).__name__})")
        logger.info("-" * 100)
        return loggers

    def _get_best_checkpoint_path(self, trainer) -> Optional[str]:
        """è·å–æœ€ä½³æ£€æŸ¥ç‚¹è·¯å¾„"""
        for callback in trainer.callbacks:
            if isinstance(callback, pl.callbacks.ModelCheckpoint):
                return getattr(callback, "best_model_path", None)
        return None

    def _print_experiment_info(self):
        """æ‰“å°å®éªŒä¿¡æ¯"""
        print("\n" + "=" * 80)
        print(f"ğŸš€ MM-LandslideNet Experiment: {self.config.get('experiment_name', 'Unnamed')}")
        print("=" * 80)
        print(f"ğŸ“ Task: {self.task}")
        print(f"ğŸ“ Config: {self.config_path}")
        print(f"ğŸ• Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        if "model" in self.config:
            print(f"ğŸ§  Model: {self.config.model.target.split('.')[-1]}")
        if "data" in self.config:
            print(f"ğŸ“Š Data: {self.config.data.get('params', {}).get('train_data_dir', 'N/A')}")

        print("=" * 80 + "\n")


def create_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="MM-LandslideNet: Configuration-Driven Deep Learning Framework",
        formatter_class=argparse.RawDescriptionHelpFormatter,  # å†™çš„æ ¼å¼æ˜¯ä»€ä¹ˆæ ·å°±æŒ‰ä»€ä¹ˆæ ·æ˜¾ç¤ºã€‚
        epilog="""
Examples:
  # æ ‡å‡†è®­ç»ƒ
  python main.py train configs/optical_baseline.yaml
  
  # KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒ  
  python main.py kfold configs/optical_baseline_kfold.yaml
  
  # KæŠ˜è®­ç»ƒï¼Œè¦†ç›–æŠ˜æ•°
  python main.py kfold configs/optical_baseline_kfold.yaml --n_splits 10
  
  # æ ‡å‡†æ¨ç†
  python main.py predict configs/optical_baseline.yaml
  
  # KæŠ˜é¢„æµ‹ï¼ˆä»å·²è®­ç»ƒçš„æ¨¡å‹ï¼‰
  python main.py kfold_predict configs/optical_baseline_kfold.yaml --resume_from outputs/kfold_experiments/my_experiment
  
  # éªŒè¯æ¨¡å‹
  python main.py validate configs/optical_baseline.yaml
        """,
    )

    # ä¸»è¦å‚æ•°
    parser.add_argument(
        "task",
        choices=["train", "predict", "kfold", "kfold_predict"],
        help="Task to execute",
    )

    parser.add_argument("config", type=str, help="Path to configuration file")

    # KæŠ˜ç‰¹å®šå‚æ•°
    parser.add_argument("--n_splits", type=int, help="Number of folds for K-fold CV (overrides config)")

    parser.add_argument("--experiment_name", type=str, help="Override experiment name")

    parser.add_argument("--resume_from", type=str, help="Resume from existing experiment directory (for kfold_predict)")

    # è°ƒè¯•å‚æ•°
    parser.add_argument("--debug", action="store_true", help="Enable debug mode")

    return parser


def main():
    """
    ä¸»å‡½æ•°

    è¿™æ˜¯æ•´ä¸ªç¨‹åºçš„å…¥å£ç‚¹ã€‚ä¸æ‚¨åŸæ¥çš„main.pyç›¸æ¯”ï¼Œ
    æ–°ç‰ˆæœ¬çš„é€»è¾‘æå…¶ç®€æ´ï¼š
    1. è§£æå‘½ä»¤è¡Œå‚æ•°
    2. åˆ›å»ºå®éªŒè¿è¡Œå™¨
    3. è¿è¡Œå®éªŒ
    4. æŠ¥å‘Šç»“æœ

    æ‰€æœ‰çš„å¤æ‚æ€§éƒ½è¢«é…ç½®æ–‡ä»¶å’Œå®ä¾‹åŒ–å·¥å…·å¸æ”¶äº†ã€‚
    """
    parser = create_parser()  # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    args = parser.parse_args()  # è§£æå‘½ä»¤è¡Œå‚æ•°

    # å‡†å¤‡ä»»åŠ¡å‚æ•°
    task_kwargs = {}
    if args.n_splits is not None:
        task_kwargs["n_splits"] = args.n_splits
    if args.experiment_name is not None:
        task_kwargs["experiment_name"] = args.experiment_name
    if args.resume_from is not None:
        task_kwargs["resume_from"] = args.resume_from
    if args.debug:
        task_kwargs["debug"] = True

    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = ExperimentRunner(args.config, args.task, **task_kwargs)

    # è¿è¡Œå®éªŒ
    results = runner.run()

    # æŠ¥å‘Šç»“æœ
    print(f"\nğŸ‰ Task '{args.task}' completed successfully!")

    if args.task == "kfold":
        if "mean_cv_score" in results:
            print(f"ğŸ“ˆ Mean CV Score: {results['mean_cv_score']:.4f} Â± {results['std_cv_score']:.4f}")
    elif results.get("best_checkpoint"):
        print(f"ğŸ“ Best model saved to: {results['best_checkpoint']}")


if __name__ == "__main__":
    main()
